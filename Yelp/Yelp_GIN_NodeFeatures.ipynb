{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "43aec689",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.utils as utils\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "from torch_geometric.nn import GINConv\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch_geometric as tg\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy import sparse as sp\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8122e7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculatePosEncodings_rswe(edge_index, num_nodes):\n",
    "    edge_index = edge_index.t().tolist()\n",
    "    edges = [(src, dst) for src, dst in edge_index]\n",
    "    rows, cols = zip(*edges)\n",
    "    data = np.ones(len(rows))\n",
    "    A = csr_matrix((data, (rows, cols)), shape=(num_nodes, num_nodes))\n",
    "\n",
    "    in_degrees_dict = {node: 0 for node in range(num_nodes)}\n",
    "    # Calculate the in-degrees for each node\n",
    "    for edge in edges:\n",
    "        _, dst = edge\n",
    "        in_degrees_dict[dst] += 1\n",
    "\n",
    "    in_degrees = np.array([in_degrees_dict[i] for i in range(len(in_degrees_dict))], dtype=float)\n",
    "    in_degrees = in_degrees.clip(1)  # Clip to ensure no division by zero\n",
    "    in_degrees = np.power(in_degrees, -1)  # Take the element-wise inverse square root\n",
    "\n",
    "    Dinv = sp.diags(in_degrees, dtype=float)\n",
    "\n",
    "    RW = A * Dinv  \n",
    "    M = RW\n",
    "    \n",
    "    # das ist wieder ein Hyperparameter; sollte >1 sein weil eins immer 0 ist irgendwie!\n",
    "    pos_enc_dim = 5\n",
    "\n",
    "    nb_pos_enc = pos_enc_dim\n",
    "    PE = [torch.from_numpy(M.diagonal()).float()]\n",
    "    M_power = M\n",
    "    for _ in range(nb_pos_enc-1):\n",
    "        M_power = M_power * M\n",
    "        PE.append(torch.from_numpy(M_power.diagonal()).float())\n",
    "    PE = torch.stack(PE,dim=-1)\n",
    "\n",
    "    #ERGEBNIS\n",
    "    RESULT_POS_ENCODING = PE \n",
    "    return RESULT_POS_ENCODING\n",
    "\n",
    "def calculatePosEncodings(edge_index, num_nodes):\n",
    "    print(\"checkpoint1\")\n",
    "    edge_index = edge_index.t().tolist()\n",
    "    edges = [(src, dst) for src, dst in edge_index]\n",
    "\n",
    "    # Create the adjacency matrix in CSR format -> das wird dann fÃ¼r die encodings benutzt!\n",
    "    rows, cols = zip(*edges)\n",
    "    data = np.ones(len(rows))\n",
    "    A = csr_matrix((data, (rows, cols)), shape=(num_nodes, num_nodes))\n",
    "\n",
    "    ''' this code computes the in_degrees matrix from the edge list. it can later be adapted to compute the in-degrees matrix from the adjacency matrix (however, then, we should\n",
    "    do some tests with small sample graphs to ensure everytheing is correct\n",
    "    '''\n",
    "    in_degrees_dict = {node: 0 for node in range(num_nodes)}\n",
    "    # Calculate the in-degrees for each node\n",
    "    for edge in edges:\n",
    "        print(\"checkpoint2\")\n",
    "        _, dst = edge\n",
    "        in_degrees_dict[dst] += 1\n",
    "\n",
    "    in_degrees = np.array([in_degrees_dict[i] for i in range(len(in_degrees_dict))], dtype=float)\n",
    "    in_degrees = in_degrees.clip(1)  # Clip to ensure no division by zero\n",
    "    in_degrees = np.power(in_degrees, -0.5)  # Take the element-wise inverse square root\n",
    "    print(\"checkpoint3\")\n",
    "    # Create the sparse diagonal matrix N\n",
    "    N = sp.diags(in_degrees, dtype=float)\n",
    "\n",
    "    L = sp.eye(num_nodes) - N * A * N\n",
    "\n",
    "    #calc eigvals and eigVecs, equivalent to the original code\n",
    "    EigVal, EigVec = np.linalg.eig(L.toarray())\n",
    "    idx = EigVal.argsort() # increasing order\n",
    "    EigVal, EigVec = EigVal[idx], np.real(EigVec[:,idx])\n",
    "\n",
    "    #pos_enc_dim = hyperparameter!\n",
    "    pos_enc_dim = 5\n",
    "    RESULT_POS_ENCODING = torch.from_numpy(EigVec[:,1:pos_enc_dim+1]).float() \n",
    "    return RESULT_POS_ENCODING\n",
    "\n",
    "def calculateLoss(task_loss, batch, num_nodes, positional_encoding):\n",
    "    #HYPERPARAMETERS\n",
    "    device = \"cpu\"\n",
    "    pos_enc_dim = 1\n",
    "    alpha_loss: 1e-3\n",
    "    lambda_loss: 100  # ist auch 100\n",
    "\n",
    "    #edge_index im korrekten Format definieren\n",
    "    edge_index = batch.edge_index.t().tolist()\n",
    "    edge_index = [(src, dst) for src, dst in edge_index]\n",
    "\n",
    "    # Loss B: Laplacian Eigenvector Loss --------------------------------------------\n",
    "    n = num_nodes\n",
    "\n",
    "    # Laplacian \n",
    "    rows, cols = zip(*edge_index)\n",
    "    data = np.ones(len(rows))\n",
    "    A = csr_matrix((data, (rows, cols)), shape=(num_nodes, num_nodes))\n",
    "\n",
    "    ''' this code computes the in_degrees matrix from the edge list. it can later be adapted to compute the in-degrees matrix from the adjacency matrix (however, then, we should\n",
    "    do some tests with small sample graphs to ensure everything is correct'''\n",
    "\n",
    "    in_degrees_dict = {node: 0 for node in range(num_nodes)}\n",
    "    # Calculate the in-degrees for each node\n",
    "    for edge in edge_index:\n",
    "        _, dst = edge\n",
    "        in_degrees_dict[dst] += 1\n",
    "\n",
    "    in_degrees = np.array([in_degrees_dict[i] for i in range(len(in_degrees_dict))], dtype=float)\n",
    "    in_degrees = in_degrees.clip(1)  # Clip to ensure no division by zero\n",
    "    in_degrees = np.power(in_degrees, -0.5)  # Take the element-wise inverse square root\n",
    "\n",
    "    # Create the sparse diagonal matrix N\n",
    "    N = sp.diags(in_degrees, dtype=float)\n",
    "    L = sp.eye(num_nodes) - N * A * N\n",
    "\n",
    "    p = positional_encoding\n",
    "    pT = torch.transpose(p, 1, 0)\n",
    "    loss_b_1 = torch.trace(torch.mm(torch.mm(pT, torch.Tensor(L.todense()).to(device)), p))\n",
    "\n",
    "    '''  TODO: loss_b_2 \n",
    "    '''\n",
    "\n",
    "    loss_b = loss_b_1\n",
    "\n",
    "    #TODO: parameter tunen!\n",
    "    loss = task_loss + 1e-3* loss_b\n",
    "    return loss\n",
    "\n",
    "\n",
    "def precision(predictions, targets, threshold):\n",
    "    # Apply a threshold to the predictions\n",
    "    binary_predictions = (predictions >= threshold).astype(int)\n",
    "    binary_targets = (targets >= threshold).astype(int)\n",
    "\n",
    "    # Calculate the true positive (TP) and false positive (FP) counts\n",
    "    TP = np.sum((binary_predictions == 1) & (binary_targets == 1))\n",
    "    FP = np.sum((binary_predictions == 1) & (binary_targets == 0))\n",
    "\n",
    "    print(\"Negative: \")\n",
    "    print(np.sum(binary_targets == 1))\n",
    "    print(\"Positive: \")\n",
    "    print(np.sum(binary_targets == 0))\n",
    "    # Calculate precision\n",
    "    precision_value = TP / (TP + FP)\n",
    "    return precision_value\n",
    "\n",
    "def recall(predictions, targets, threshold):\n",
    "    # Apply a threshold to the predictions\n",
    "    binary_predictions = (predictions >= threshold).astype(int)\n",
    "    binary_targets = (targets >= threshold).astype(int)\n",
    "    # Calculate the true positive (TP) and false negative (FN) counts\n",
    "    TP = np.sum((binary_predictions == 1) & (binary_targets == 1))\n",
    "    FN = np.sum((binary_predictions == 0) & (binary_targets == 1))\n",
    "    # Calculate recall\n",
    "    recall_value = TP / (TP + FN)\n",
    "    return recall_value\n",
    "\n",
    "def precission_recall_at_k (predictions, targets, threshold, k):\n",
    "    # Combine ratings and predictions into tuples for sorting\n",
    "    combined = list(zip(targets, predictions))\n",
    "\n",
    "    # Sort the combined list in descending order of predictions\n",
    "    combined.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Extract top k sorted items and calculate precision and recall\n",
    "    top_k_items = combined[:k]\n",
    "    true_positives = sum(1 for rating, _ in top_k_items if rating >= threshold)\n",
    "    false_positives = k - true_positives\n",
    "    relevant_items = sum(1 for rating in targets if rating >= threshold)\n",
    "    false_negatives = relevant_items - true_positives\n",
    "\n",
    "    precision_at_k = true_positives / (true_positives + false_positives)\n",
    "    recall_at_k = true_positives / (true_positives + false_negatives)\n",
    "    normalized_recall_at_k = recall_at_k / (k / relevant_items)\n",
    "\n",
    "\n",
    "    return precision_at_k, recall_at_k, normalized_recall_at_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4d4d17e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done business\n",
      "done reviews\n",
      "done users\n"
     ]
    }
   ],
   "source": [
    "# Reading in all the data\n",
    "\n",
    "business_ids=[]\n",
    "business_average_stars=[]\n",
    "business_review_count=[]\n",
    "\n",
    "try:\n",
    "    with open('yelp_dataset/yelp_academic_dataset_business.json', 'r', encoding='utf-8') as json_file:\n",
    "        for line in json_file:\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "                business_ids.append(data[\"business_id\"])\n",
    "                business_average_stars.append(data[\"stars\"])\n",
    "                business_review_count.append(data[\"review_count\"])\n",
    "            except json.decoder.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON: {e}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"File not found.\")\n",
    "print(\"done business\")\n",
    "\n",
    "review_ids=[]\n",
    "review_business_ids=[]\n",
    "review_user_ids=[]\n",
    "review_stars=[]\n",
    "\n",
    "try:\n",
    "    with open('yelp_dataset/yelp_academic_dataset_review.json', 'r', encoding='utf-8') as json_file:\n",
    "        for i,line in enumerate(json_file):\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "                review_ids.append(data[\"review_id\"])\n",
    "                review_business_ids.append(data[\"business_id\"])\n",
    "                review_user_ids.append(data[\"user_id\"])\n",
    "                review_stars.append(data[\"stars\"])\n",
    "            except json.decoder.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON: {e}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"File not found.\")\n",
    "print(\"done reviews\")\n",
    "\n",
    "user_ids=[]\n",
    "\n",
    "try:\n",
    "    with open('yelp_dataset/yelp_academic_dataset_user.json', 'r', encoding='utf-8') as json_file:\n",
    "        for line in json_file:\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "                user_ids.append(data[\"user_id\"])\n",
    "            except json.decoder.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON: {e}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"File not found.\")\n",
    "print(\"done users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "618b183e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1987897\n",
      "150346\n",
      "6990280\n",
      "---------------------------------------------\n",
      "20000\n",
      "10000\n",
      "6990280\n"
     ]
    }
   ],
   "source": [
    "# As there is too much data use a subset by manipulating the business and user sets\n",
    "\n",
    "print(len(user_ids))\n",
    "print(len(business_ids))\n",
    "print(len(review_stars))\n",
    "\n",
    "businesses = 10000\n",
    "users = 20000\n",
    "\n",
    "business_average_stars = business_average_stars[:businesses]\n",
    "business_review_count = business_review_count[:businesses]\n",
    "business_ids = business_ids[:businesses]\n",
    "\n",
    "user_ids = user_ids[:users]\n",
    "\n",
    "print(\"---------------------------------------------\")\n",
    "print(len(user_ids))\n",
    "print(len(business_ids))\n",
    "print(len(review_stars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eed791a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47703\n"
     ]
    }
   ],
   "source": [
    "# User and business to index mapping in order.\n",
    "# Review lists are adjusted to only include users and businesses from the subset lists.\n",
    "\n",
    "user_to_index = {user_id: index for index, user_id in enumerate(set(user_ids))}\n",
    "business_to_index = {business_id: index for index, business_id in enumerate(set(business_ids))}\n",
    "\n",
    "\n",
    "user_index_col = [user_to_index[user_id] for user_id in user_ids]\n",
    "business_index_col = [business_to_index[business_id] for business_id in business_ids]\n",
    "\n",
    "skip_indexes = []\n",
    "current_index = -1\n",
    "\n",
    "for user_id in review_user_ids:\n",
    "    try:\n",
    "        current_index = current_index + 1\n",
    "        tmp = user_to_index[user_id]\n",
    "    except KeyError:\n",
    "        skip_indexes.append(current_index)\n",
    "\n",
    "current_index = -1\n",
    "for business_id in review_business_ids:\n",
    "    try:\n",
    "        current_index = current_index + 1\n",
    "        tmp = business_to_index[business_id]\n",
    "    except KeyError:\n",
    "        skip_indexes.append(current_index)    \n",
    "\n",
    "skip_indexes = set(skip_indexes)\n",
    "print(6990280 - len(skip_indexes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "75d50b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22403\n",
      "30000\n",
      "Number of Users in index col 16327\n",
      "Number of businesses in index col 6076\n",
      "Total reviews left 47703\n",
      "22403\n"
     ]
    }
   ],
   "source": [
    "current_index = -1\n",
    "\n",
    "review_user_index_col = []\n",
    "for user_id in review_user_ids:\n",
    "    try:\n",
    "        current_index = current_index + 1\n",
    "        if current_index in skip_indexes:\n",
    "            pass\n",
    "        else:\n",
    "            user_index = user_to_index[user_id]\n",
    "            review_user_index_col.append(businesses+user_index)\n",
    "    except KeyError:\n",
    "        print(\"Huh?\")        \n",
    "        \n",
    "current_index = -1        \n",
    "review_business_index_col = []\n",
    "for business_id in review_business_ids:\n",
    "    try:\n",
    "        current_index = current_index + 1\n",
    "        if current_index in skip_indexes:\n",
    "            pass\n",
    "        else:\n",
    "            business_index = business_to_index[business_id]\n",
    "            review_business_index_col.append(business_index)\n",
    "    except KeyError:\n",
    "        print(\"Huh?\")\n",
    "    \n",
    "current_index = -1\n",
    "adjusted_review_stars = []\n",
    "for star in review_stars:\n",
    "        current_index = current_index + 1\n",
    "        if current_index in skip_indexes:\n",
    "            pass\n",
    "        else:\n",
    "            adjusted_review_stars.append(star)\n",
    "\n",
    "num_nodes = len(set(review_user_index_col)) + len(set(review_business_index_col)) \n",
    "print(num_nodes)\n",
    "print(len(user_to_index)+len(business_to_index))\n",
    "\n",
    "\n",
    "\n",
    "print(\"Number of Users in index col \" + str(len(set(review_user_index_col))))\n",
    "print(\"Number of businesses in index col \" + str(len(set(review_business_index_col))))\n",
    "print(\"Total reviews left \" + str(len(adjusted_review_stars)))\n",
    "print(num_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a1f84a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4554\n",
      "tensor([[5.0000e+00, 1.5371e-03],\n",
      "        [3.0000e+00, 3.2938e-03],\n",
      "        [3.5000e+00, 4.8309e-03],\n",
      "        ...,\n",
      "        [0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00]])\n"
     ]
    }
   ],
   "source": [
    "# Feature list including average stars and the review_count/max_review_count of a business.\n",
    "# Also a dummy padding is added for the users.\n",
    "\n",
    "adjusted_features = []\n",
    "\n",
    "for starts in business_average_stars:\n",
    "    adjusted_features.append([starts])\n",
    "    \n",
    "highest_review_count = 0\n",
    "for review in business_review_count:\n",
    "    if(review > highest_review_count):\n",
    "        highest_review_count = review\n",
    "print(highest_review_count)\n",
    "        \n",
    "for i in range(len(business_review_count)):\n",
    "    adjusted_features[i].append(business_review_count[i]/highest_review_count)\n",
    "\n",
    "adjusted_features_tensor = torch.tensor(adjusted_features)\n",
    "\n",
    "num_features = len(adjusted_features[0])\n",
    "\n",
    "num_users = len(set(user_ids))\n",
    "user_genre_features = torch.zeros(num_users,num_features)\n",
    "\n",
    "features = torch.cat((adjusted_features_tensor, user_genre_features), dim=0)\n",
    "\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0a582bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[28249, 19032, 27839,  ..., 16685, 17429, 20506],\n",
      "        [ 4471,  1455,  7659,  ...,   852,  2203,  2275]])\n"
     ]
    }
   ],
   "source": [
    "rating_tensor = torch.tensor(adjusted_review_stars, dtype=torch.float)\n",
    "\n",
    "edge_index = torch.tensor([review_user_index_col, review_business_index_col], dtype=torch.long)\n",
    "\n",
    "positional_encodings = calculatePosEncodings_rswe(edge_index, 30000)\n",
    "\n",
    "#TODO: num nodes 30.000 oder num_nodes?\n",
    "data = Data(edge_index=edge_index, x=features, y=rating_tensor, positional_encodings=positional_encodings)\n",
    "\n",
    "print(edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f393a925",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GINModel_nopos(nn.Module):\n",
    "        def __init__(self, hidden_channels):\n",
    "            super(GINModel_nopos, self).__init__()\n",
    "            # number of in layers = number of node features + number of positional embedding dimensions\n",
    "            num_features = 2\n",
    "            self.conv1 = GINConv(nn.Sequential(\n",
    "                nn.Linear(num_features, hidden_channels),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_channels, hidden_channels),\n",
    "            ))\n",
    "            self.conv2 = GINConv(nn.Sequential(\n",
    "                nn.Linear(hidden_channels, hidden_channels),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_channels, hidden_channels) #TODO: diesen Parameter mal tunen! \n",
    "            ))\n",
    "\n",
    "            self.conv3 = GINConv(nn.Sequential(\n",
    "                nn.Linear(hidden_channels, hidden_channels),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_channels, 1) #TODO: diesen Parameter mal tunen! \n",
    "            ))\n",
    "\n",
    "        def forward(self, x, edge_index, pos_embeddings):\n",
    "            x = x.view(-1, x.size(2))\n",
    "\n",
    "            #fh from the paper\n",
    "            x = self.conv1(x, edge_index)\n",
    "            #x = F.relu(x)\n",
    "            #x = self.conv2(x, edge_index)\n",
    "          #  x = F.relu(x)\n",
    "          #  x = self.conv3(x, edge_index)\n",
    "\n",
    "            # Predict movie ratings (edge features) using a linear layer\n",
    "            user_embed = x[edge_index[0]]\n",
    "            movie_embed = x[edge_index[1]]\n",
    "            \n",
    "        # ratings = torch.sum(user_embed * movie_embed, dim=1)\n",
    "            ratings = torch.sum(movie_embed, dim=1)\n",
    "            return ratings\n",
    "\n",
    "\n",
    "class GINModel_var1(nn.Module):\n",
    "        def __init__(self, hidden_channels):\n",
    "            super(GINModel_var1, self).__init__()\n",
    "            # number of in layers = number of node features + number of positional embedding dimensions\n",
    "            num_features = 7\n",
    "            self.conv1 = GINConv(nn.Sequential(\n",
    "                nn.Linear(num_features, hidden_channels),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_channels, hidden_channels),\n",
    "            ))\n",
    "            self.conv2 = GINConv(nn.Sequential(\n",
    "                nn.Linear(hidden_channels, hidden_channels),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_channels, hidden_channels)  \n",
    "            ))\n",
    "\n",
    "            self.conv3 = GINConv(nn.Sequential(\n",
    "                nn.Linear(hidden_channels, hidden_channels),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_channels, hidden_channels) \n",
    "            ))\n",
    "\n",
    "            #this is for learning of the positional encodings, which is seperate!!!\n",
    "            self.conv1_pos = GINConv(nn.Sequential(\n",
    "                nn.Linear(5, hidden_channels),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_channels, hidden_channels),\n",
    "            ))\n",
    "            self.conv2_pos = GINConv(nn.Sequential(\n",
    "                nn.Linear(hidden_channels, hidden_channels),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_channels, hidden_channels), \n",
    "            ))\n",
    "            self.conv3_pos = GINConv(nn.Sequential(\n",
    "                nn.Linear(hidden_channels, hidden_channels),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_channels, hidden_channels) \n",
    "            ))\n",
    "            self.linear = nn.Linear(hidden_channels, 1)\n",
    "\n",
    "\n",
    "\n",
    "        def forward(self, x, edge_index, pos_embeddings):\n",
    "            x = x.view(-1, x.size(2))\n",
    "            pos_embeddings = pos_embeddings.view(-1, pos_embeddings.size(2))\n",
    "            x = torch.cat([x, pos_embeddings], dim=1)\n",
    "\n",
    "            #fh from the paper\n",
    "            x = self.conv1(x, edge_index)\n",
    "           # x = F.relu(x)\n",
    "           \n",
    "           # x = self.conv2(x, edge_index)\n",
    "           # x = F.relu(x)\n",
    "           # x = self.conv3(x, edge_index)\n",
    "\n",
    "            #Now the learning of positional embeddings. So this is fp from the paper\n",
    "            pos_embeddings = self.conv1_pos(pos_embeddings, edge_index)\n",
    "           # pos_embeddings = F.relu(pos_embeddings)\n",
    "           # pos_embeddings = self.conv2_pos(pos_embeddings, edge_index)\n",
    "           # pos_embeddings = F.relu(pos_embeddings)\n",
    "           # pos_embeddings = self.conv3_pos(pos_embeddings, edge_index)\n",
    "\n",
    "            final_output = self.linear(torch.cat([x, pos_embeddings]))\n",
    "            movie_embed = final_output[edge_index[1]]\n",
    "            ratings = torch.sum(movie_embed, dim=1)\n",
    "\n",
    "            return ratings\n",
    "\n",
    "'''____________________________________________________________________________________'''\n",
    "\n",
    "class GINModel_var2(nn.Module):\n",
    "        def __init__(self, hidden_channels):\n",
    "            super(GINModel_var2, self).__init__()\n",
    "            # number of in layers = number of node features + number of positional embedding dimensions\n",
    "            num_features = 2\n",
    "            self.conv1 = GINConv(nn.Sequential(\n",
    "                nn.Linear(num_features, hidden_channels),\n",
    "               # nn.ReLU(),\n",
    "               # nn.Linear(hidden_channels, hidden_channels),\n",
    "            ))\n",
    "            self.conv2 = GINConv(nn.Sequential(\n",
    "                nn.Linear(hidden_channels, hidden_channels),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_channels, hidden_channels)  \n",
    "            ))\n",
    "            self.conv2_var2 = GINConv (nn.Sequential(\n",
    "                nn.Linear(hidden_channels*2, hidden_channels*2),\n",
    "           #     nn.ReLU(),\n",
    "            #    nn.Linear(hidden_channels, hidden_channels)  \n",
    "            ))\n",
    "\n",
    "            self.conv3 = GINConv(nn.Sequential(\n",
    "                nn.Linear(hidden_channels, hidden_channels),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_channels, hidden_channels) \n",
    "            ))\n",
    "\n",
    "            #this is for learning of the positional encodings, which is seperate!!!\n",
    "            self.conv1_pos = GINConv(nn.Sequential(\n",
    "                nn.Linear(5, hidden_channels),\n",
    "             #   nn.ReLU(),\n",
    "              #  nn.Linear(hidden_channels, hidden_channels),\n",
    "            ))\n",
    "            self.conv2_pos = GINConv(nn.Sequential(\n",
    "                nn.Linear(hidden_channels, hidden_channels),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_channels, hidden_channels), \n",
    "            ))\n",
    "            self.conv3_pos = GINConv(nn.Sequential(\n",
    "                nn.Linear(hidden_channels, hidden_channels),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_channels, hidden_channels) \n",
    "            ))\n",
    "            self.linear = nn.Linear(hidden_channels, 1)\n",
    "\n",
    "\n",
    "\n",
    "        def forward(self, x, edge_index, pos_embeddings):\n",
    "            x = x.view(-1, x.size(2))\n",
    "            pos_embeddings = pos_embeddings.view(-1, pos_embeddings.size(2))\n",
    "           # x = torch.cat([x, pos_embeddings], dim=1)\n",
    "\n",
    "            #fh from the paper\n",
    "            x = self.conv1(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            pos_embeddings = self.conv1_pos(pos_embeddings, edge_index)\n",
    "            pos_embeddings = F.relu(pos_embeddings)\n",
    "\n",
    "            x = self.conv2_var2(torch.cat([x,pos_embeddings],dim=1), edge_index)\n",
    "           # x = F.relu(x)\n",
    "            #x = self.conv3(x, edge_index)\n",
    "\n",
    "            movie_embed = x[edge_index[1]]\n",
    "            ratings = torch.sum(movie_embed, dim=1)\n",
    "\n",
    "            return ratings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "95e32818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Training Loss: 31.943771362304688, Validation Loss: 26.237390518188477\n",
      "Epoch 2/50, Training Loss: 26.10148811340332, Validation Loss: 20.97322654724121\n",
      "Epoch 3/50, Training Loss: 20.842777252197266, Validation Loss: 16.317869186401367\n",
      "Epoch 4/50, Training Loss: 16.195646286010742, Validation Loss: 12.294258117675781\n",
      "Epoch 5/50, Training Loss: 12.18300724029541, Validation Loss: 8.919139862060547\n",
      "Epoch 6/50, Training Loss: 8.82155704498291, Validation Loss: 6.201183795928955\n",
      "Epoch 7/50, Training Loss: 6.119847774505615, Validation Loss: 4.138229846954346\n",
      "Epoch 8/50, Training Loss: 4.07550573348999, Validation Loss: 2.7132980823516846\n",
      "Epoch 9/50, Training Loss: 2.671210765838623, Validation Loss: 1.8890321254730225\n",
      "Epoch 10/50, Training Loss: 1.8690845966339111, Validation Loss: 1.6006919145584106\n",
      "Epoch 11/50, Training Loss: 1.6036386489868164, Validation Loss: 1.7492884397506714\n",
      "Epoch 12/50, Training Loss: 1.7748627662658691, Validation Loss: 2.1996517181396484\n",
      "Epoch 13/50, Training Loss: 2.246326208114624, Validation Loss: 2.791559934616089\n",
      "Epoch 14/50, Training Loss: 2.856468439102173, Validation Loss: 3.367816209793091\n",
      "Epoch 15/50, Training Loss: 3.4469687938690186, Validation Loss: 3.8076047897338867\n",
      "Epoch 16/50, Training Loss: 3.8963663578033447, Validation Loss: 4.045534133911133\n",
      "Epoch 17/50, Training Loss: 4.139171600341797, Validation Loss: 4.069890975952148\n",
      "Epoch 18/50, Training Loss: 4.1640167236328125, Validation Loss: 3.9083049297332764\n",
      "Epoch 19/50, Training Loss: 3.9991555213928223, Validation Loss: 3.6106412410736084\n",
      "Epoch 20/50, Training Loss: 3.6952102184295654, Validation Loss: 3.2344517707824707\n",
      "Epoch 21/50, Training Loss: 3.3105151653289795, Validation Loss: 2.8345072269439697\n",
      "Epoch 22/50, Training Loss: 2.900588035583496, Validation Loss: 2.4562511444091797\n",
      "Epoch 23/50, Training Loss: 2.511535167694092, Validation Loss: 2.1324808597564697\n",
      "Epoch 24/50, Training Loss: 2.17672061920166, Validation Loss: 1.882533073425293\n",
      "Epoch 25/50, Training Loss: 1.9159376621246338, Validation Loss: 1.7132742404937744\n",
      "Epoch 26/50, Training Loss: 1.736405849456787, Validation Loss: 1.6212893724441528\n",
      "Epoch 27/50, Training Loss: 1.6349658966064453, Validation Loss: 1.5956910848617554\n",
      "Epoch 28/50, Training Loss: 1.600900650024414, Validation Loss: 1.6210672855377197\n",
      "Epoch 29/50, Training Loss: 1.6188980340957642, Validation Loss: 1.680187463760376\n",
      "Epoch 30/50, Training Loss: 1.6717725992202759, Validation Loss: 1.7562272548675537\n",
      "Epoch 31/50, Training Loss: 1.742702841758728, Validation Loss: 1.8343899250030518\n",
      "Epoch 32/50, Training Loss: 1.8168665170669556, Validation Loss: 1.9029269218444824\n",
      "Epoch 33/50, Training Loss: 1.8824681043624878, Validation Loss: 1.9536168575286865\n",
      "Epoch 34/50, Training Loss: 1.9312273263931274, Validation Loss: 1.9818079471588135\n",
      "Epoch 35/50, Training Loss: 1.9584264755249023, Validation Loss: 1.986136555671692\n",
      "Epoch 36/50, Training Loss: 1.9626301527023315, Validation Loss: 1.968021035194397\n",
      "Epoch 37/50, Training Loss: 1.9451810121536255, Validation Loss: 1.9310210943222046\n",
      "Epoch 38/50, Training Loss: 1.9095600843429565, Validation Loss: 1.8801376819610596\n",
      "Epoch 39/50, Training Loss: 1.8606849908828735, Validation Loss: 1.8211019039154053\n",
      "Epoch 40/50, Training Loss: 1.804198980331421, Validation Loss: 1.7597131729125977\n",
      "Epoch 41/50, Training Loss: 1.7458075284957886, Validation Loss: 1.701255202293396\n",
      "Epoch 42/50, Training Loss: 1.6906946897506714, Validation Loss: 1.6500344276428223\n",
      "Epoch 43/50, Training Loss: 1.6430617570877075, Validation Loss: 1.6090385913848877\n",
      "Epoch 44/50, Training Loss: 1.6057851314544678, Validation Loss: 1.5797863006591797\n",
      "Epoch 45/50, Training Loss: 1.5802698135375977, Validation Loss: 1.5623301267623901\n",
      "Epoch 46/50, Training Loss: 1.5664554834365845, Validation Loss: 1.5554208755493164\n",
      "Epoch 47/50, Training Loss: 1.5629833936691284, Validation Loss: 1.5568097829818726\n",
      "Epoch 48/50, Training Loss: 1.5675044059753418, Validation Loss: 1.5636467933654785\n",
      "Epoch 49/50, Training Loss: 1.577081322669983, Validation Loss: 1.5729169845581055\n",
      "Epoch 50/50, Training Loss: 1.5886279344558716, Validation Loss: 1.5818523168563843\n",
      "Batch Size: 32\n",
      "Epochs: 50\n",
      "MSE: 1.5906673669815063\n",
      "RMSE: 1.261216640472412\n",
      "Negative: \n",
      "6658\n",
      "Positive: \n",
      "2883\n",
      "Precision: 0.7036406495547407\n",
      "Recall: 0.806999098828477\n",
      "Precision@k: 0.718\n",
      "Recall@k: 0.1078401922499249\n",
      "0\n",
      "Epoch 1/50, Training Loss: 15.674240112304688, Validation Loss: 14.913631439208984\n",
      "Epoch 2/50, Training Loss: 14.760570526123047, Validation Loss: 13.937307357788086\n",
      "Epoch 3/50, Training Loss: 13.788071632385254, Validation Loss: 12.907240867614746\n",
      "Epoch 4/50, Training Loss: 12.762360572814941, Validation Loss: 11.827951431274414\n",
      "Epoch 5/50, Training Loss: 11.688079833984375, Validation Loss: 10.70428466796875\n",
      "Epoch 6/50, Training Loss: 10.570201873779297, Validation Loss: 9.545263290405273\n",
      "Epoch 7/50, Training Loss: 9.417852401733398, Validation Loss: 8.365523338317871\n",
      "Epoch 8/50, Training Loss: 8.245820999145508, Validation Loss: 7.185464382171631\n",
      "Epoch 9/50, Training Loss: 7.07465934753418, Validation Loss: 6.03019380569458\n",
      "Epoch 10/50, Training Loss: 5.929664134979248, Validation Loss: 4.93424129486084\n",
      "Epoch 11/50, Training Loss: 4.845544338226318, Validation Loss: 3.93900728225708\n",
      "Epoch 12/50, Training Loss: 3.863891363143921, Validation Loss: 3.0898449420928955\n",
      "Epoch 13/50, Training Loss: 3.0303447246551514, Validation Loss: 2.440575361251831\n",
      "Epoch 14/50, Training Loss: 2.398704767227173, Validation Loss: 2.0395596027374268\n",
      "Epoch 15/50, Training Loss: 2.017179012298584, Validation Loss: 1.916892170906067\n",
      "Epoch 16/50, Training Loss: 1.9153860807418823, Validation Loss: 2.0630688667297363\n",
      "Epoch 17/50, Training Loss: 2.0825159549713135, Validation Loss: 2.405789852142334\n",
      "Epoch 18/50, Training Loss: 2.444587230682373, Validation Loss: 2.806436061859131\n",
      "Epoch 19/50, Training Loss: 2.8605847358703613, Validation Loss: 3.1061081886291504\n",
      "Epoch 20/50, Training Loss: 3.16987681388855, Validation Loss: 3.2061173915863037\n",
      "Epoch 21/50, Training Loss: 3.2730376720428467, Validation Loss: 3.1159024238586426\n",
      "Epoch 22/50, Training Loss: 3.180612564086914, Validation Loss: 2.8904218673706055\n",
      "Epoch 23/50, Training Loss: 2.9486629962921143, Validation Loss: 2.6079447269439697\n",
      "Epoch 24/50, Training Loss: 2.6569221019744873, Validation Loss: 2.3277411460876465\n",
      "Epoch 25/50, Training Loss: 2.3658812046051025, Validation Loss: 2.09740948677063\n",
      "Epoch 26/50, Training Loss: 2.124105453491211, Validation Loss: 1.9390783309936523\n",
      "Epoch 27/50, Training Loss: 1.9545997381210327, Validation Loss: 1.8566087484359741\n",
      "Epoch 28/50, Training Loss: 1.8618212938308716, Validation Loss: 1.839662790298462\n",
      "Epoch 29/50, Training Loss: 1.8358221054077148, Validation Loss: 1.8695145845413208\n",
      "Epoch 30/50, Training Loss: 1.8581020832061768, Validation Loss: 1.9247654676437378\n",
      "Epoch 31/50, Training Loss: 1.9073619842529297, Validation Loss: 1.9855804443359375\n",
      "Epoch 32/50, Training Loss: 1.963776707649231, Validation Loss: 2.0358316898345947\n",
      "Epoch 33/50, Training Loss: 2.0111536979675293, Validation Loss: 2.060086965560913\n",
      "Epoch 34/50, Training Loss: 2.033815383911133, Validation Loss: 2.0560262203216553\n",
      "Epoch 35/50, Training Loss: 2.0294954776763916, Validation Loss: 2.0187225341796875\n",
      "Epoch 36/50, Training Loss: 1.993544578552246, Validation Loss: 1.9545732736587524\n",
      "Epoch 37/50, Training Loss: 1.9322025775909424, Validation Loss: 1.8748245239257812\n",
      "Epoch 38/50, Training Loss: 1.8566275835037231, Validation Loss: 1.7958420515060425\n",
      "Epoch 39/50, Training Loss: 1.7827280759811401, Validation Loss: 1.7289248704910278\n",
      "Epoch 40/50, Training Loss: 1.721616268157959, Validation Loss: 1.683708667755127\n",
      "Epoch 41/50, Training Loss: 1.6826343536376953, Validation Loss: 1.664656400680542\n",
      "Epoch 42/50, Training Loss: 1.6698722839355469, Validation Loss: 1.6693886518478394\n",
      "Epoch 43/50, Training Loss: 1.6804940700531006, Validation Loss: 1.6887056827545166\n",
      "Epoch 44/50, Training Loss: 1.7048020362854004, Validation Loss: 1.709458351135254\n",
      "Epoch 45/50, Training Loss: 1.7291978597640991, Validation Loss: 1.7197459936141968\n",
      "Epoch 46/50, Training Loss: 1.74149489402771, Validation Loss: 1.7135682106018066\n",
      "Epoch 47/50, Training Loss: 1.735636591911316, Validation Loss: 1.6922162771224976\n",
      "Epoch 48/50, Training Loss: 1.713076114654541, Validation Loss: 1.6622943878173828\n",
      "Epoch 49/50, Training Loss: 1.6807348728179932, Validation Loss: 1.6321991682052612\n",
      "Epoch 50/50, Training Loss: 1.6474061012268066, Validation Loss: 1.608870029449463\n",
      "Batch Size: 32\n",
      "Epochs: 50\n",
      "MSE: 1.6115297079086304\n",
      "RMSE: 1.2694604396820068\n",
      "Negative: \n",
      "6658\n",
      "Positive: \n",
      "2883\n",
      "Precision: 0.7043596730245232\n",
      "Recall: 0.6988585160708921\n",
      "Precision@k: 0.7\n",
      "Recall@k: 0.10513667768098528\n",
      "1\n",
      "Epoch 1/50, Training Loss: 219.21498107910156, Validation Loss: 49.91663360595703\n",
      "Epoch 2/50, Training Loss: 932.4071044921875, Validation Loss: 10.101325988769531\n",
      "Epoch 3/50, Training Loss: 189.464111328125, Validation Loss: 17.352031707763672\n",
      "Epoch 4/50, Training Loss: 89.34696960449219, Validation Loss: 47.77788162231445\n",
      "Epoch 5/50, Training Loss: 437.92584228515625, Validation Loss: 36.639976501464844\n",
      "Epoch 6/50, Training Loss: 309.7547302246094, Validation Loss: 11.42708683013916\n",
      "Epoch 7/50, Training Loss: 41.737327575683594, Validation Loss: 4.382481575012207\n",
      "Epoch 8/50, Training Loss: 47.19441604614258, Validation Loss: 11.27328872680664\n",
      "Epoch 9/50, Training Loss: 214.22637939453125, Validation Loss: 12.11080265045166\n",
      "Epoch 10/50, Training Loss: 230.46876525878906, Validation Loss: 5.759474754333496\n",
      "Epoch 11/50, Training Loss: 93.88874816894531, Validation Loss: 4.036118984222412\n",
      "Epoch 12/50, Training Loss: 4.616631507873535, Validation Loss: 11.328474998474121\n",
      "Epoch 13/50, Training Loss: 55.253841400146484, Validation Loss: 18.748985290527344\n",
      "Epoch 14/50, Training Loss: 137.32984924316406, Validation Loss: 17.305612564086914\n",
      "Epoch 15/50, Training Loss: 124.98835754394531, Validation Loss: 9.43371868133545\n",
      "Epoch 16/50, Training Loss: 44.73421096801758, Validation Loss: 3.521432638168335\n",
      "Epoch 17/50, Training Loss: 3.1102864742279053, Validation Loss: 3.2853739261627197\n",
      "Epoch 18/50, Training Loss: 37.52523422241211, Validation Loss: 5.106084823608398\n",
      "Epoch 19/50, Training Loss: 82.98208618164062, Validation Loss: 4.678308963775635\n",
      "Epoch 20/50, Training Loss: 73.68253326416016, Validation Loss: 2.7628893852233887\n",
      "Epoch 21/50, Training Loss: 26.36138916015625, Validation Loss: 2.870267152786255\n",
      "Epoch 22/50, Training Loss: 2.5339269638061523, Validation Loss: 5.746756553649902\n",
      "Epoch 23/50, Training Loss: 23.838655471801758, Validation Loss: 8.183311462402344\n",
      "Epoch 24/50, Training Loss: 50.89927291870117, Validation Loss: 7.34682559967041\n",
      "Epoch 25/50, Training Loss: 43.99616622924805, Validation Loss: 4.305547714233398\n",
      "Epoch 26/50, Training Loss: 14.886296272277832, Validation Loss: 2.1774752140045166\n",
      "Epoch 27/50, Training Loss: 2.243342876434326, Validation Loss: 2.1803364753723145\n",
      "Epoch 28/50, Training Loss: 16.9143009185791, Validation Loss: 2.793229818344116\n",
      "Epoch 29/50, Training Loss: 32.2857666015625, Validation Loss: 2.4953274726867676\n",
      "Epoch 30/50, Training Loss: 25.608985900878906, Validation Loss: 1.8253445625305176\n",
      "Epoch 31/50, Training Loss: 7.572251796722412, Validation Loss: 2.1365482807159424\n",
      "Epoch 32/50, Training Loss: 2.4331862926483154, Validation Loss: 3.343944787979126\n",
      "Epoch 33/50, Training Loss: 13.238242149353027, Validation Loss: 3.969736099243164\n",
      "Epoch 34/50, Training Loss: 20.864389419555664, Validation Loss: 3.2402524948120117\n",
      "Epoch 35/50, Training Loss: 13.87814998626709, Validation Loss: 2.0576348304748535\n",
      "Epoch 36/50, Training Loss: 3.2390763759613037, Validation Loss: 1.6112546920776367\n",
      "Epoch 37/50, Training Loss: 3.291264295578003, Validation Loss: 1.8625752925872803\n",
      "Epoch 38/50, Training Loss: 11.021800994873047, Validation Loss: 1.9653457403182983\n",
      "Epoch 39/50, Training Loss: 13.101078033447266, Validation Loss: 1.6789833307266235\n",
      "Epoch 40/50, Training Loss: 6.629230499267578, Validation Loss: 1.578458309173584\n",
      "Epoch 41/50, Training Loss: 1.618598461151123, Validation Loss: 1.9723212718963623\n",
      "Epoch 42/50, Training Loss: 4.388063430786133, Validation Loss: 2.3576574325561523\n",
      "Epoch 43/50, Training Loss: 8.834139823913574, Validation Loss: 2.203343629837036\n",
      "Epoch 44/50, Training Loss: 7.429733753204346, Validation Loss: 1.7333444356918335\n",
      "Epoch 45/50, Training Loss: 2.7291975021362305, Validation Loss: 1.516961693763733\n",
      "Epoch 46/50, Training Loss: 1.8885525465011597, Validation Loss: 1.631316065788269\n",
      "Epoch 47/50, Training Loss: 5.047433853149414, Validation Loss: 1.6966136693954468\n",
      "Epoch 48/50, Training Loss: 6.237862586975098, Validation Loss: 1.57029128074646\n",
      "Epoch 49/50, Training Loss: 3.5745034217834473, Validation Loss: 1.516919732093811\n",
      "Epoch 50/50, Training Loss: 1.5211671590805054, Validation Loss: 1.6817755699157715\n",
      "Batch Size: 32\n",
      "Epochs: 50\n",
      "MSE: 1.7639316320419312\n",
      "RMSE: 1.328130841255188\n",
      "Negative: \n",
      "6658\n",
      "Positive: \n",
      "2883\n",
      "Precision: 0.7023393854748603\n",
      "Recall: 0.6042355061580054\n",
      "Precision@k: 0.695\n",
      "Recall@k: 0.10438570141183538\n",
      "2\n",
      "Epoch 1/50, Training Loss: 32.050167083740234, Validation Loss: 26.03998374938965\n",
      "Epoch 2/50, Training Loss: 26.19076156616211, Validation Loss: 20.806743621826172\n",
      "Epoch 3/50, Training Loss: 20.91602325439453, Validation Loss: 16.181901931762695\n",
      "Epoch 4/50, Training Loss: 16.253992080688477, Validation Loss: 12.188260078430176\n",
      "Epoch 5/50, Training Loss: 12.227652549743652, Validation Loss: 8.84244155883789\n",
      "Epoch 6/50, Training Loss: 8.853759765625, Validation Loss: 6.152982711791992\n",
      "Epoch 7/50, Training Loss: 6.140929698944092, Validation Loss: 4.117580890655518\n",
      "Epoch 8/50, Training Loss: 4.086863040924072, Validation Loss: 2.7190959453582764\n",
      "Epoch 9/50, Training Loss: 2.6743292808532715, Validation Loss: 1.9199899435043335\n",
      "Epoch 10/50, Training Loss: 1.8655688762664795, Validation Loss: 1.655314564704895\n",
      "Epoch 11/50, Training Loss: 1.5952365398406982, Validation Loss: 1.8258286714553833\n",
      "Epoch 12/50, Training Loss: 1.7634772062301636, Validation Loss: 2.296039581298828\n",
      "Epoch 13/50, Training Loss: 2.233947277069092, Validation Loss: 2.9052889347076416\n",
      "Epoch 14/50, Training Loss: 2.8449649810791016, Validation Loss: 3.4958362579345703\n",
      "Epoch 15/50, Training Loss: 3.4377620220184326, Validation Loss: 3.9463372230529785\n",
      "Epoch 16/50, Training Loss: 3.8901727199554443, Validation Loss: 4.191052436828613\n",
      "Epoch 17/50, Training Loss: 4.135976314544678, Validation Loss: 4.218206882476807\n",
      "Epoch 18/50, Training Loss: 4.163253307342529, Validation Loss: 4.055633068084717\n",
      "Epoch 19/50, Training Loss: 3.999950408935547, Validation Loss: 3.7536041736602783\n",
      "Epoch 20/50, Training Loss: 3.696606159210205, Validation Loss: 3.3702166080474854\n",
      "Epoch 21/50, Training Loss: 3.3116347789764404, Validation Loss: 2.9608652591705322\n",
      "Epoch 22/50, Training Loss: 2.90073561668396, Validation Loss: 2.5716352462768555\n",
      "Epoch 23/50, Training Loss: 2.5102405548095703, Validation Loss: 2.2359564304351807\n",
      "Epoch 24/50, Training Loss: 2.17374587059021, Validation Loss: 1.9737510681152344\n",
      "Epoch 25/50, Training Loss: 1.9112567901611328, Validation Loss: 1.792404294013977\n",
      "Epoch 26/50, Training Loss: 1.7301599979400635, Validation Loss: 1.6889357566833496\n",
      "Epoch 27/50, Training Loss: 1.6274150609970093, Validation Loss: 1.6528010368347168\n",
      "Epoch 28/50, Training Loss: 1.592372179031372, Validation Loss: 1.6688382625579834\n",
      "Epoch 29/50, Training Loss: 1.6097395420074463, Validation Loss: 1.719980001449585\n",
      "Epoch 30/50, Training Loss: 1.6623132228851318, Validation Loss: 1.7894870042800903\n",
      "Epoch 31/50, Training Loss: 1.7332254648208618, Validation Loss: 1.8625829219818115\n",
      "Epoch 32/50, Training Loss: 1.8075908422470093, Validation Loss: 1.9274864196777344\n",
      "Epoch 33/50, Training Loss: 1.8735439777374268, Validation Loss: 1.9759031534194946\n",
      "Epoch 34/50, Training Loss: 1.9227341413497925, Validation Loss: 2.003079652786255\n",
      "Epoch 35/50, Training Loss: 1.9503796100616455, Validation Loss: 2.0075294971466064\n",
      "Epoch 36/50, Training Loss: 1.9549919366836548, Validation Loss: 1.9905363321304321\n",
      "Epoch 37/50, Training Loss: 1.9378726482391357, Validation Loss: 1.9555186033248901\n",
      "Epoch 38/50, Training Loss: 1.9024766683578491, Validation Loss: 1.9073327779769897\n",
      "Epoch 39/50, Training Loss: 1.8537079095840454, Validation Loss: 1.8515682220458984\n",
      "Epoch 40/50, Training Loss: 1.7972110509872437, Validation Loss: 1.7938859462738037\n",
      "Epoch 41/50, Training Loss: 1.7387022972106934, Validation Loss: 1.7394367456436157\n",
      "Epoch 42/50, Training Loss: 1.6833869218826294, Validation Loss: 1.6924015283584595\n",
      "Epoch 43/50, Training Loss: 1.635494589805603, Validation Loss: 1.6556473970413208\n",
      "Epoch 44/50, Training Loss: 1.597930908203125, Validation Loss: 1.6305817365646362\n",
      "Epoch 45/50, Training Loss: 1.5721333026885986, Validation Loss: 1.6171531677246094\n",
      "Epoch 46/50, Training Loss: 1.558068871498108, Validation Loss: 1.6140159368515015\n",
      "Epoch 47/50, Training Loss: 1.55440092086792, Validation Loss: 1.6188337802886963\n",
      "Epoch 48/50, Training Loss: 1.5587941408157349, Validation Loss: 1.6286793947219849\n",
      "Epoch 49/50, Training Loss: 1.5683155059814453, Validation Loss: 1.6404714584350586\n",
      "Epoch 50/50, Training Loss: 1.5798741579055786, Validation Loss: 1.6513921022415161\n",
      "Batch Size: 32\n",
      "Epochs: 50\n",
      "MSE: 1.5737366676330566\n",
      "RMSE: 1.2544865608215332\n",
      "Negative: \n",
      "6674\n",
      "Positive: \n",
      "2867\n",
      "Precision: 0.701037698673322\n",
      "Recall: 0.7996703626011388\n",
      "Precision@k: 0.721\n",
      "Recall@k: 0.10803116571771051\n",
      "3\n",
      "Epoch 1/50, Training Loss: 15.71978759765625, Validation Loss: 14.786775588989258\n",
      "Epoch 2/50, Training Loss: 14.803487777709961, Validation Loss: 13.817741394042969\n",
      "Epoch 3/50, Training Loss: 13.82813835144043, Validation Loss: 12.79565143585205\n",
      "Epoch 4/50, Training Loss: 12.79936408996582, Validation Loss: 11.725095748901367\n",
      "Epoch 5/50, Training Loss: 11.721806526184082, Validation Loss: 10.610992431640625\n",
      "Epoch 6/50, Training Loss: 10.600433349609375, Validation Loss: 9.462443351745605\n",
      "Epoch 7/50, Training Loss: 9.444372177124023, Validation Loss: 8.29415225982666\n",
      "Epoch 8/50, Training Loss: 8.26841926574707, Validation Loss: 7.1265692710876465\n",
      "Epoch 9/50, Training Loss: 7.093152046203613, Validation Loss: 5.984736442565918\n",
      "Epoch 10/50, Training Loss: 5.94389533996582, Validation Loss: 4.903301239013672\n",
      "Epoch 11/50, Training Loss: 4.855432510375977, Validation Loss: 3.923663854598999\n",
      "Epoch 12/50, Training Loss: 3.869407892227173, Validation Loss: 3.0912678241729736\n",
      "Epoch 13/50, Training Loss: 3.0315070152282715, Validation Loss: 2.4598469734191895\n",
      "Epoch 14/50, Training Loss: 2.395779609680176, Validation Loss: 2.07731556892395\n",
      "Epoch 15/50, Training Loss: 2.0105786323547363, Validation Loss: 1.973397135734558\n",
      "Epoch 16/50, Training Loss: 1.9059005975723267, Validation Loss: 2.1380722522735596\n",
      "Epoch 17/50, Training Loss: 2.0713717937469482, Validation Loss: 2.4982781410217285\n",
      "Epoch 18/50, Training Loss: 2.43349027633667, Validation Loss: 2.9138529300689697\n",
      "Epoch 19/50, Training Loss: 2.8512604236602783, Validation Loss: 3.2243993282318115\n",
      "Epoch 20/50, Training Loss: 3.1635398864746094, Validation Loss: 3.3291232585906982\n",
      "Epoch 21/50, Training Loss: 3.2689545154571533, Validation Loss: 3.238628625869751\n",
      "Epoch 22/50, Training Loss: 3.178138494491577, Validation Loss: 3.0079596042633057\n",
      "Epoch 23/50, Training Loss: 2.94640851020813, Validation Loss: 2.7170636653900146\n",
      "Epoch 24/50, Training Loss: 2.6540615558624268, Validation Loss: 2.4260995388031006\n",
      "Epoch 25/50, Training Loss: 2.3617658615112305, Validation Loss: 2.1838266849517822\n",
      "Epoch 26/50, Training Loss: 2.1184680461883545, Validation Loss: 2.0134336948394775\n",
      "Epoch 27/50, Training Loss: 1.9475511312484741, Validation Loss: 1.9194802045822144\n",
      "Epoch 28/50, Training Loss: 1.8536232709884644, Validation Loss: 1.8921973705291748\n",
      "Epoch 29/50, Training Loss: 1.8268535137176514, Validation Loss: 1.913222074508667\n",
      "Epoch 30/50, Training Loss: 1.8487637042999268, Validation Loss: 1.9613420963287354\n",
      "Epoch 31/50, Training Loss: 1.8980066776275635, Validation Loss: 2.016770362854004\n",
      "Epoch 32/50, Training Loss: 1.954668402671814, Validation Loss: 2.0632073879241943\n",
      "Epoch 33/50, Training Loss: 2.0023117065429688, Validation Loss: 2.0854549407958984\n",
      "Epoch 34/50, Training Loss: 2.0254366397857666, Validation Loss: 2.0808889865875244\n",
      "Epoch 35/50, Training Loss: 2.021559953689575, Validation Loss: 2.0447840690612793\n",
      "Epoch 36/50, Training Loss: 1.9856700897216797, Validation Loss: 1.9835752248764038\n",
      "Epoch 37/50, Training Loss: 1.9242898225784302, Validation Loss: 1.9084807634353638\n",
      "Epoch 38/50, Training Loss: 1.8486509323120117, Validation Loss: 1.8350852727890015\n",
      "Epoch 39/50, Training Loss: 1.7745859622955322, Validation Loss: 1.7744367122650146\n",
      "Epoch 40/50, Training Loss: 1.71324622631073, Validation Loss: 1.7358448505401611\n",
      "Epoch 41/50, Training Loss: 1.6740285158157349, Validation Loss: 1.7233920097351074\n",
      "Epoch 42/50, Training Loss: 1.66107976436615, Validation Loss: 1.7342596054077148\n",
      "Epoch 43/50, Training Loss: 1.671613097190857, Validation Loss: 1.7587722539901733\n",
      "Epoch 44/50, Training Loss: 1.695949673652649, Validation Loss: 1.7833377122879028\n",
      "Epoch 45/50, Training Loss: 1.7204657793045044, Validation Loss: 1.795745849609375\n",
      "Epoch 46/50, Training Loss: 1.7329140901565552, Validation Loss: 1.7899020910263062\n",
      "Epoch 47/50, Training Loss: 1.7271795272827148, Validation Loss: 1.7672213315963745\n",
      "Epoch 48/50, Training Loss: 1.704679250717163, Validation Loss: 1.734605073928833\n",
      "Epoch 49/50, Training Loss: 1.6723333597183228, Validation Loss: 1.7008483409881592\n",
      "Epoch 50/50, Training Loss: 1.6389561891555786, Validation Loss: 1.673330307006836\n",
      "Batch Size: 32\n",
      "Epochs: 50\n",
      "MSE: 1.5950968265533447\n",
      "RMSE: 1.2629714012145996\n",
      "Negative: \n",
      "6674\n",
      "Positive: \n",
      "2867\n",
      "Precision: 0.707623868691517\n",
      "Recall: 0.6911896913395266\n",
      "Precision@k: 0.706\n",
      "Recall@k: 0.10578363799820198\n",
      "4\n",
      "Epoch 1/50, Training Loss: 243.6751251220703, Validation Loss: 48.15764617919922\n",
      "Epoch 2/50, Training Loss: 1065.015380859375, Validation Loss: 9.433147430419922\n",
      "Epoch 3/50, Training Loss: 211.6806640625, Validation Loss: 17.761159896850586\n",
      "Epoch 4/50, Training Loss: 104.4581069946289, Validation Loss: 47.90799331665039\n",
      "Epoch 5/50, Training Loss: 501.4339904785156, Validation Loss: 36.59553527832031\n",
      "Epoch 6/50, Training Loss: 351.6949157714844, Validation Loss: 11.466341018676758\n",
      "Epoch 7/50, Training Loss: 46.338619232177734, Validation Loss: 4.225948333740234\n",
      "Epoch 8/50, Training Loss: 54.0149040222168, Validation Loss: 10.755791664123535\n",
      "Epoch 9/50, Training Loss: 244.6205596923828, Validation Loss: 11.549919128417969\n",
      "Epoch 10/50, Training Loss: 262.79241943359375, Validation Loss: 5.486478805541992\n",
      "Epoch 11/50, Training Loss: 106.92823028564453, Validation Loss: 4.037476062774658\n",
      "Epoch 12/50, Training Loss: 4.879058837890625, Validation Loss: 11.405806541442871\n",
      "Epoch 13/50, Training Loss: 62.24755859375, Validation Loss: 18.829265594482422\n",
      "Epoch 14/50, Training Loss: 156.08880615234375, Validation Loss: 17.445890426635742\n",
      "Epoch 15/50, Training Loss: 142.7930145263672, Validation Loss: 9.608220100402832\n",
      "Epoch 16/50, Training Loss: 51.46456527709961, Validation Loss: 3.583192825317383\n",
      "Epoch 17/50, Training Loss: 3.193288803100586, Validation Loss: 3.145984649658203\n",
      "Epoch 18/50, Training Loss: 41.68238067626953, Validation Loss: 4.863450527191162\n",
      "Epoch 19/50, Training Loss: 93.94185638427734, Validation Loss: 4.499932289123535\n",
      "Epoch 20/50, Training Loss: 84.37460327148438, Validation Loss: 2.70428466796875\n",
      "Epoch 21/50, Training Loss: 30.653820037841797, Validation Loss: 2.8668949604034424\n",
      "Epoch 22/50, Training Loss: 2.5783913135528564, Validation Loss: 5.75004768371582\n",
      "Epoch 23/50, Training Loss: 26.0780086517334, Validation Loss: 8.232019424438477\n",
      "Epoch 24/50, Training Loss: 57.3714485168457, Validation Loss: 7.47579288482666\n",
      "Epoch 25/50, Training Loss: 50.533565521240234, Validation Loss: 4.451227188110352\n",
      "Epoch 26/50, Training Loss: 17.50485610961914, Validation Loss: 2.2334156036376953\n",
      "Epoch 27/50, Training Loss: 2.2159054279327393, Validation Loss: 2.1235036849975586\n",
      "Epoch 28/50, Training Loss: 18.302831649780273, Validation Loss: 2.7069125175476074\n",
      "Epoch 29/50, Training Loss: 36.300872802734375, Validation Loss: 2.459453821182251\n",
      "Epoch 30/50, Training Loss: 29.54501724243164, Validation Loss: 1.8338414430618286\n",
      "Epoch 31/50, Training Loss: 8.98315715789795, Validation Loss: 2.1439614295959473\n",
      "Epoch 32/50, Training Loss: 2.3486826419830322, Validation Loss: 3.3532607555389404\n",
      "Epoch 33/50, Training Loss: 14.271379470825195, Validation Loss: 4.0253400802612305\n",
      "Epoch 34/50, Training Loss: 23.488462448120117, Validation Loss: 3.3439927101135254\n",
      "Epoch 35/50, Training Loss: 16.13250732421875, Validation Loss: 2.1472880840301514\n",
      "Epoch 36/50, Training Loss: 3.804072618484497, Validation Loss: 1.6373167037963867\n",
      "Epoch 37/50, Training Loss: 3.2278008460998535, Validation Loss: 1.8490196466445923\n",
      "Epoch 38/50, Training Loss: 11.941685676574707, Validation Loss: 1.9681141376495361\n",
      "Epoch 39/50, Training Loss: 14.844754219055176, Validation Loss: 1.7109037637710571\n",
      "Epoch 40/50, Training Loss: 7.77086067199707, Validation Loss: 1.610756516456604\n",
      "Epoch 41/50, Training Loss: 1.703655481338501, Validation Loss: 1.995930790901184\n",
      "Epoch 42/50, Training Loss: 4.451152324676514, Validation Loss: 2.4002602100372314\n",
      "Epoch 43/50, Training Loss: 9.682770729064941, Validation Loss: 2.2784318923950195\n",
      "Epoch 44/50, Training Loss: 8.500443458557129, Validation Loss: 1.810598611831665\n",
      "Epoch 45/50, Training Loss: 3.1407723426818848, Validation Loss: 1.5634570121765137\n",
      "Epoch 46/50, Training Loss: 1.8102571964263916, Validation Loss: 1.6566522121429443\n",
      "Epoch 47/50, Training Loss: 5.2920403480529785, Validation Loss: 1.7321183681488037\n",
      "Epoch 48/50, Training Loss: 6.938894271850586, Validation Loss: 1.6213388442993164\n",
      "Epoch 49/50, Training Loss: 4.092547416687012, Validation Loss: 1.564725637435913\n",
      "Epoch 50/50, Training Loss: 1.5484205484390259, Validation Loss: 1.7228801250457764\n",
      "Batch Size: 32\n",
      "Epochs: 50\n",
      "MSE: 1.6943069696426392\n",
      "RMSE: 1.3016555309295654\n",
      "Negative: \n",
      "6674\n",
      "Positive: \n",
      "2867\n",
      "Precision: 0.7025132501282271\n",
      "Recall: 0.6156727599640396\n",
      "Precision@k: 0.697\n",
      "Recall@k: 0.10443512136649685\n",
      "5\n",
      "Epoch 1/50, Training Loss: 31.9877986907959, Validation Loss: 26.079139709472656\n",
      "Epoch 2/50, Training Loss: 26.141895294189453, Validation Loss: 20.83308982849121\n",
      "Epoch 3/50, Training Loss: 20.879182815551758, Validation Loss: 16.197141647338867\n",
      "Epoch 4/50, Training Loss: 16.227638244628906, Validation Loss: 12.1941499710083\n",
      "Epoch 5/50, Training Loss: 12.21019172668457, Validation Loss: 8.84076976776123\n",
      "Epoch 6/50, Training Loss: 8.843554496765137, Validation Loss: 6.145552158355713\n",
      "Epoch 7/50, Training Loss: 6.136336326599121, Validation Loss: 4.106173038482666\n",
      "Epoch 8/50, Training Loss: 4.0862603187561035, Validation Loss: 2.7054364681243896\n",
      "Epoch 9/50, Training Loss: 2.676164150238037, Validation Loss: 1.9056941270828247\n",
      "Epoch 10/50, Training Loss: 1.868422269821167, Validation Loss: 1.6418176889419556\n",
      "Epoch 11/50, Training Loss: 1.5979069471359253, Validation Loss: 1.814299464225769\n",
      "Epoch 12/50, Training Loss: 1.7650794982910156, Validation Loss: 2.287278413772583\n",
      "Epoch 13/50, Training Loss: 2.23400616645813, Validation Loss: 2.8996379375457764\n",
      "Epoch 14/50, Training Loss: 2.8434455394744873, Validation Loss: 3.4931435585021973\n",
      "Epoch 15/50, Training Loss: 3.4349944591522217, Validation Loss: 3.9460246562957764\n",
      "Epoch 16/50, Training Loss: 3.8866991996765137, Validation Loss: 4.192264080047607\n",
      "Epoch 17/50, Training Loss: 4.132379055023193, Validation Loss: 4.219987392425537\n",
      "Epoch 18/50, Training Loss: 4.160041809082031, Validation Loss: 4.057076930999756\n",
      "Epoch 19/50, Training Loss: 3.9974889755249023, Validation Loss: 3.7539584636688232\n",
      "Epoch 20/50, Training Loss: 3.695100784301758, Validation Loss: 3.36893367767334\n",
      "Epoch 21/50, Training Loss: 3.311145544052124, Validation Loss: 2.9576070308685303\n",
      "Epoch 22/50, Training Loss: 2.9011964797973633, Validation Loss: 2.5662660598754883\n",
      "Epoch 23/50, Training Loss: 2.5115039348602295, Validation Loss: 2.2285072803497314\n",
      "Epoch 24/50, Training Loss: 2.175614833831787, Validation Loss: 1.9643765687942505\n",
      "Epoch 25/50, Training Loss: 1.9135165214538574, Validation Loss: 1.7813435792922974\n",
      "Epoch 26/50, Training Loss: 1.732608437538147, Validation Loss: 1.6764726638793945\n",
      "Epoch 27/50, Training Loss: 1.6298816204071045, Validation Loss: 1.6392282247543335\n",
      "Epoch 28/50, Training Loss: 1.5947270393371582, Validation Loss: 1.6544334888458252\n",
      "Epoch 29/50, Training Loss: 1.6118985414505005, Validation Loss: 1.7049891948699951\n",
      "Epoch 30/50, Training Loss: 1.6642369031906128, Validation Loss: 1.7741148471832275\n",
      "Epoch 31/50, Training Loss: 1.7349127531051636, Validation Loss: 1.846989631652832\n",
      "Epoch 32/50, Training Loss: 1.8090696334838867, Validation Loss: 1.9117909669876099\n",
      "Epoch 33/50, Training Loss: 1.8748623132705688, Validation Loss: 1.960188388824463\n",
      "Epoch 34/50, Training Loss: 1.9239509105682373, Validation Loss: 1.9873993396759033\n",
      "Epoch 35/50, Training Loss: 1.951555848121643, Validation Loss: 1.9919172525405884\n",
      "Epoch 36/50, Training Loss: 1.9561823606491089, Validation Loss: 1.975011944770813\n",
      "Epoch 37/50, Training Loss: 1.9391231536865234, Validation Loss: 1.9400960206985474\n",
      "Epoch 38/50, Training Loss: 1.9038187265396118, Validation Loss: 1.8920241594314575\n",
      "Epoch 39/50, Training Loss: 1.8551578521728516, Validation Loss: 1.8363877534866333\n",
      "Epoch 40/50, Training Loss: 1.7987693548202515, Validation Loss: 1.778851866722107\n",
      "Epoch 41/50, Training Loss: 1.7403573989868164, Validation Loss: 1.7245709896087646\n",
      "Epoch 42/50, Training Loss: 1.6851162910461426, Validation Loss: 1.6777291297912598\n",
      "Epoch 43/50, Training Loss: 1.6372698545455933, Validation Loss: 1.64119291305542\n",
      "Epoch 44/50, Training Loss: 1.5997202396392822, Validation Loss: 1.616368293762207\n",
      "Epoch 45/50, Training Loss: 1.5739059448242188, Validation Loss: 1.6031967401504517\n",
      "Epoch 46/50, Training Loss: 1.5597981214523315, Validation Loss: 1.600324273109436\n",
      "Epoch 47/50, Training Loss: 1.5560681819915771, Validation Loss: 1.6054027080535889\n",
      "Epoch 48/50, Training Loss: 1.5603879690170288, Validation Loss: 1.615492582321167\n",
      "Epoch 49/50, Training Loss: 1.5698333978652954, Validation Loss: 1.6274992227554321\n",
      "Epoch 50/50, Training Loss: 1.5813207626342773, Validation Loss: 1.6385931968688965\n",
      "Batch Size: 32\n",
      "Epochs: 50\n",
      "MSE: 1.5774824619293213\n",
      "RMSE: 1.2559787034988403\n",
      "Negative: \n",
      "6637\n",
      "Positive: \n",
      "2904\n",
      "Precision: 0.6984313725490197\n",
      "Recall: 0.8050323941539852\n",
      "Precision@k: 0.743\n",
      "Recall@k: 0.11194816935362363\n",
      "6\n",
      "Epoch 1/50, Training Loss: 15.70943832397461, Validation Loss: 14.784785270690918\n",
      "Epoch 2/50, Training Loss: 14.79466438293457, Validation Loss: 13.813993453979492\n",
      "Epoch 3/50, Training Loss: 13.820928573608398, Validation Loss: 12.790077209472656\n",
      "Epoch 4/50, Training Loss: 12.793837547302246, Validation Loss: 11.717658042907715\n",
      "Epoch 5/50, Training Loss: 11.718008041381836, Validation Loss: 10.601662635803223\n",
      "Epoch 6/50, Training Loss: 10.598374366760254, Validation Loss: 9.451223373413086\n",
      "Epoch 7/50, Training Loss: 9.444051742553711, Validation Loss: 8.281082153320312\n",
      "Epoch 8/50, Training Loss: 8.269795417785645, Validation Loss: 7.111741065979004\n",
      "Epoch 9/50, Training Loss: 7.096129417419434, Validation Loss: 5.968283176422119\n",
      "Epoch 10/50, Training Loss: 5.948232650756836, Validation Loss: 4.885522365570068\n",
      "Epoch 11/50, Training Loss: 4.860940456390381, Validation Loss: 3.904895305633545\n",
      "Epoch 12/50, Training Loss: 3.875753402709961, Validation Loss: 3.071688175201416\n",
      "Epoch 13/50, Training Loss: 3.0380873680114746, Validation Loss: 2.440152645111084\n",
      "Epoch 14/50, Training Loss: 2.402243137359619, Validation Loss: 2.0581343173980713\n",
      "Epoch 15/50, Training Loss: 2.016230583190918, Validation Loss: 1.9558446407318115\n",
      "Epoch 16/50, Training Loss: 1.9104535579681396, Validation Loss: 2.1229777336120605\n",
      "Epoch 17/50, Training Loss: 2.074650287628174, Validation Loss: 2.4865915775299072\n",
      "Epoch 18/50, Training Loss: 2.436215400695801, Validation Loss: 2.9053897857666016\n",
      "Epoch 19/50, Training Loss: 2.853732109069824, Validation Loss: 3.2182371616363525\n",
      "Epoch 20/50, Training Loss: 3.1657025814056396, Validation Loss: 3.3237030506134033\n",
      "Epoch 21/50, Training Loss: 3.2709639072418213, Validation Loss: 3.2333171367645264\n",
      "Epoch 22/50, Training Loss: 3.1806015968322754, Validation Loss: 3.0018787384033203\n",
      "Epoch 23/50, Training Loss: 2.9495186805725098, Validation Loss: 2.7095205783843994\n",
      "Epoch 24/50, Training Loss: 2.6578657627105713, Validation Loss: 2.4166555404663086\n",
      "Epoch 25/50, Training Loss: 2.366093158721924, Validation Loss: 2.1723196506500244\n",
      "Epoch 26/50, Training Loss: 2.123129367828369, Validation Loss: 1.9999337196350098\n",
      "Epoch 27/50, Training Loss: 1.9522838592529297, Validation Loss: 1.9042760133743286\n",
      "Epoch 28/50, Training Loss: 1.8582611083984375, Validation Loss: 1.8756624460220337\n",
      "Epoch 29/50, Training Loss: 1.8312948942184448, Validation Loss: 1.8957535028457642\n",
      "Epoch 30/50, Training Loss: 1.8529701232910156, Validation Loss: 1.943317174911499\n",
      "Epoch 31/50, Training Loss: 1.9019896984100342, Validation Loss: 1.9985215663909912\n",
      "Epoch 32/50, Training Loss: 1.958474040031433, Validation Loss: 2.045361042022705\n",
      "Epoch 33/50, Training Loss: 2.006375789642334, Validation Loss: 2.0679423809051514\n",
      "Epoch 34/50, Training Loss: 2.029526948928833, Validation Loss: 2.0638644695281982\n",
      "Epoch 35/50, Training Loss: 2.0257153511047363, Validation Loss: 2.028488874435425\n",
      "Epoch 36/50, Training Loss: 1.9904059171676636, Validation Loss: 1.9676400423049927\n",
      "Epoch 37/50, Training Loss: 1.9292494058609009, Validation Loss: 1.8925622701644897\n",
      "Epoch 38/50, Training Loss: 1.8532856702804565, Validation Loss: 1.8192356824874878\n",
      "Epoch 39/50, Training Loss: 1.778913140296936, Validation Loss: 1.7585664987564087\n",
      "Epoch 40/50, Training Loss: 1.7170699834823608, Validation Loss: 1.7199326753616333\n",
      "Epoch 41/50, Training Loss: 1.6772141456604004, Validation Loss: 1.7075051069259644\n",
      "Epoch 42/50, Training Loss: 1.6636015176773071, Validation Loss: 1.7185603380203247\n",
      "Epoch 43/50, Training Loss: 1.67359459400177, Validation Loss: 1.7434921264648438\n",
      "Epoch 44/50, Training Loss: 1.697666049003601, Validation Loss: 1.768688440322876\n",
      "Epoch 45/50, Training Loss: 1.7222660779953003, Validation Loss: 1.7817989587783813\n",
      "Epoch 46/50, Training Loss: 1.7350776195526123, Validation Loss: 1.7765110731124878\n",
      "Epoch 47/50, Training Loss: 1.729791283607483, Validation Loss: 1.7540514469146729\n",
      "Epoch 48/50, Training Loss: 1.7076095342636108, Validation Loss: 1.7212451696395874\n",
      "Epoch 49/50, Training Loss: 1.6753156185150146, Validation Loss: 1.6869401931762695\n",
      "Epoch 50/50, Training Loss: 1.6417008638381958, Validation Loss: 1.6586642265319824\n",
      "Batch Size: 32\n",
      "Epochs: 50\n",
      "MSE: 1.5980441570281982\n",
      "RMSE: 1.2641377449035645\n",
      "Negative: \n",
      "6637\n",
      "Positive: \n",
      "2904\n",
      "Precision: 0.7004103967168263\n",
      "Recall: 0.6942895886695797\n",
      "Precision@k: 0.727\n",
      "Recall@k: 0.10953744161518758\n",
      "7\n",
      "Epoch 1/50, Training Loss: 232.09783935546875, Validation Loss: 55.204429626464844\n",
      "Epoch 2/50, Training Loss: 1002.78955078125, Validation Loss: 10.775795936584473\n",
      "Epoch 3/50, Training Loss: 201.19317626953125, Validation Loss: 18.303285598754883\n",
      "Epoch 4/50, Training Loss: 97.43031311035156, Validation Loss: 51.31993103027344\n",
      "Epoch 5/50, Training Loss: 471.6454772949219, Validation Loss: 39.05599594116211\n",
      "Epoch 6/50, Training Loss: 331.97369384765625, Validation Loss: 11.737739562988281\n",
      "Epoch 7/50, Training Loss: 44.16545104980469, Validation Loss: 4.452465057373047\n",
      "Epoch 8/50, Training Loss: 50.8266716003418, Validation Loss: 12.274825096130371\n",
      "Epoch 9/50, Training Loss: 230.3505401611328, Validation Loss: 13.208894729614258\n",
      "Epoch 10/50, Training Loss: 247.60922241210938, Validation Loss: 6.087419509887695\n",
      "Epoch 11/50, Training Loss: 100.7978286743164, Validation Loss: 3.9645915031433105\n",
      "Epoch 12/50, Training Loss: 4.751186847686768, Validation Loss: 11.764134407043457\n",
      "Epoch 13/50, Training Loss: 58.9532470703125, Validation Loss: 19.83835220336914\n",
      "Epoch 14/50, Training Loss: 147.27418518066406, Validation Loss: 18.331130981445312\n",
      "Epoch 15/50, Training Loss: 134.43409729003906, Validation Loss: 9.839263916015625\n",
      "Epoch 16/50, Training Loss: 48.304351806640625, Validation Loss: 3.494731903076172\n",
      "Epoch 17/50, Training Loss: 3.1387970447540283, Validation Loss: 3.3530445098876953\n",
      "Epoch 18/50, Training Loss: 39.70875549316406, Validation Loss: 5.437693119049072\n",
      "Epoch 19/50, Training Loss: 88.7914047241211, Validation Loss: 4.9933671951293945\n",
      "Epoch 20/50, Training Loss: 79.35492706298828, Validation Loss: 2.825792074203491\n",
      "Epoch 21/50, Training Loss: 28.62949562072754, Validation Loss: 2.8181874752044678\n",
      "Epoch 22/50, Training Loss: 2.5372488498687744, Validation Loss: 5.879724502563477\n",
      "Epoch 23/50, Training Loss: 25.0037899017334, Validation Loss: 8.556498527526855\n",
      "Epoch 24/50, Training Loss: 54.31826400756836, Validation Loss: 7.716532230377197\n",
      "Epoch 25/50, Training Loss: 47.452667236328125, Validation Loss: 4.459033966064453\n",
      "Epoch 26/50, Training Loss: 16.263261795043945, Validation Loss: 2.171281099319458\n",
      "Epoch 27/50, Training Loss: 2.2072575092315674, Validation Loss: 2.21018385887146\n",
      "Epoch 28/50, Training Loss: 17.623960494995117, Validation Loss: 2.92380428314209\n",
      "Epoch 29/50, Training Loss: 34.401912689208984, Validation Loss: 2.6154186725616455\n",
      "Epoch 30/50, Training Loss: 27.692047119140625, Validation Loss: 1.8454430103302002\n",
      "Epoch 31/50, Training Loss: 8.312277793884277, Validation Loss: 2.121978998184204\n",
      "Epoch 32/50, Training Loss: 2.3688039779663086, Validation Loss: 3.413907289505005\n",
      "Epoch 33/50, Training Loss: 13.766327857971191, Validation Loss: 4.127310752868652\n",
      "Epoch 34/50, Training Loss: 22.245006561279297, Validation Loss: 3.377319574356079\n",
      "Epoch 35/50, Training Loss: 15.069884300231934, Validation Loss: 2.1084487438201904\n",
      "Epoch 36/50, Training Loss: 3.5297658443450928, Validation Loss: 1.6265536546707153\n",
      "Epoch 37/50, Training Loss: 3.241123676300049, Validation Loss: 1.9163756370544434\n",
      "Epoch 38/50, Training Loss: 11.496269226074219, Validation Loss: 2.049644708633423\n",
      "Epoch 39/50, Training Loss: 14.023314476013184, Validation Loss: 1.7347602844238281\n",
      "Epoch 40/50, Training Loss: 7.228062152862549, Validation Loss: 1.595676302909851\n",
      "Epoch 41/50, Training Loss: 1.6540642976760864, Validation Loss: 2.0027413368225098\n",
      "Epoch 42/50, Training Loss: 4.4064226150512695, Validation Loss: 2.4328882694244385\n",
      "Epoch 43/50, Training Loss: 9.275680541992188, Validation Loss: 2.290295362472534\n",
      "Epoch 44/50, Training Loss: 7.997163772583008, Validation Loss: 1.7898156642913818\n",
      "Epoch 45/50, Training Loss: 2.9448347091674805, Validation Loss: 1.5519936084747314\n",
      "Epoch 46/50, Training Loss: 1.8378605842590332, Validation Loss: 1.6808513402938843\n",
      "Epoch 47/50, Training Loss: 5.167616844177246, Validation Loss: 1.7640888690948486\n",
      "Epoch 48/50, Training Loss: 6.60689115524292, Validation Loss: 1.6274281740188599\n",
      "Epoch 49/50, Training Loss: 3.849895715713501, Validation Loss: 1.5543967485427856\n",
      "Epoch 50/50, Training Loss: 1.530438780784607, Validation Loss: 1.7230511903762817\n",
      "Batch Size: 32\n",
      "Epochs: 50\n",
      "MSE: 1.6881606578826904\n",
      "RMSE: 1.2992923259735107\n",
      "Negative: \n",
      "6637\n",
      "Positive: \n",
      "2904\n",
      "Precision: 0.6940726577437859\n",
      "Recall: 0.6016272412234444\n",
      "Precision@k: 0.719\n",
      "Recall@k: 0.10833207774596956\n",
      "8\n",
      "Epoch 1/50, Training Loss: 31.962499618530273, Validation Loss: 26.044822692871094\n",
      "Epoch 2/50, Training Loss: 26.123783111572266, Validation Loss: 20.791545867919922\n",
      "Epoch 3/50, Training Loss: 20.867229461669922, Validation Loss: 16.150468826293945\n",
      "Epoch 4/50, Training Loss: 16.220754623413086, Validation Loss: 12.144479751586914\n",
      "Epoch 5/50, Training Loss: 12.207262992858887, Validation Loss: 8.790246963500977\n",
      "Epoch 6/50, Training Loss: 8.843461036682129, Validation Loss: 6.0962934494018555\n",
      "Epoch 7/50, Training Loss: 6.137965679168701, Validation Loss: 4.060232162475586\n",
      "Epoch 8/50, Training Loss: 4.08854866027832, Validation Loss: 2.6647393703460693\n",
      "Epoch 9/50, Training Loss: 2.678144693374634, Validation Loss: 1.8719632625579834\n",
      "Epoch 10/50, Training Loss: 1.8692902326583862, Validation Loss: 1.6164655685424805\n",
      "Epoch 11/50, Training Loss: 1.5971041917800903, Validation Loss: 1.798291563987732\n",
      "Epoch 12/50, Training Loss: 1.762386679649353, Validation Loss: 2.280973434448242\n",
      "Epoch 13/50, Training Loss: 2.229602098464966, Validation Loss: 2.9026386737823486\n",
      "Epoch 14/50, Training Loss: 2.8378686904907227, Validation Loss: 3.5042436122894287\n",
      "Epoch 15/50, Training Loss: 3.428975820541382, Validation Loss: 3.963322639465332\n",
      "Epoch 16/50, Training Loss: 3.8809449672698975, Validation Loss: 4.213437557220459\n",
      "Epoch 17/50, Training Loss: 4.127419948577881, Validation Loss: 4.242607593536377\n",
      "Epoch 18/50, Training Loss: 4.15617561340332, Validation Loss: 4.0788750648498535\n",
      "Epoch 19/50, Training Loss: 3.9948012828826904, Validation Loss: 3.773003339767456\n",
      "Epoch 20/50, Training Loss: 3.693504810333252, Validation Loss: 3.3837246894836426\n",
      "Epoch 21/50, Training Loss: 3.3104445934295654, Validation Loss: 2.9671010971069336\n",
      "Epoch 22/50, Training Loss: 2.901132345199585, Validation Loss: 2.569859743118286\n",
      "Epoch 23/50, Training Loss: 2.5118067264556885, Validation Loss: 2.2259857654571533\n",
      "Epoch 24/50, Training Loss: 2.1760354042053223, Validation Loss: 1.9558488130569458\n",
      "Epoch 25/50, Training Loss: 1.9138516187667847, Validation Loss: 1.767166256904602\n",
      "Epoch 26/50, Training Loss: 1.7327144145965576, Validation Loss: 1.6571770906448364\n",
      "Epoch 27/50, Training Loss: 1.6296766996383667, Validation Loss: 1.6154547929763794\n",
      "Epoch 28/50, Training Loss: 1.594189167022705, Validation Loss: 1.626873254776001\n",
      "Epoch 29/50, Training Loss: 1.61105477809906, Validation Loss: 1.6743394136428833\n",
      "Epoch 30/50, Training Loss: 1.6631505489349365, Validation Loss: 1.7410457134246826\n",
      "Epoch 31/50, Training Loss: 1.7336695194244385, Validation Loss: 1.8121243715286255\n",
      "Epoch 32/50, Training Loss: 1.8077651262283325, Validation Loss: 1.8756929636001587\n",
      "Epoch 33/50, Training Loss: 1.8735902309417725, Validation Loss: 1.923356533050537\n",
      "Epoch 34/50, Training Loss: 1.9227945804595947, Validation Loss: 1.950269341468811\n",
      "Epoch 35/50, Training Loss: 1.9505805969238281, Validation Loss: 1.954864501953125\n",
      "Epoch 36/50, Training Loss: 1.9554328918457031, Validation Loss: 1.938356637954712\n",
      "Epoch 37/50, Training Loss: 1.9386203289031982, Validation Loss: 1.9041085243225098\n",
      "Epoch 38/50, Training Loss: 1.903562068939209, Validation Loss: 1.8569310903549194\n",
      "Epoch 39/50, Training Loss: 1.855126976966858, Validation Loss: 1.8023762702941895\n",
      "Epoch 40/50, Training Loss: 1.7989296913146973, Validation Loss: 1.746071696281433\n",
      "Epoch 41/50, Training Loss: 1.7406631708145142, Validation Loss: 1.6931370496749878\n",
      "Epoch 42/50, Training Loss: 1.6855182647705078, Validation Loss: 1.6477231979370117\n",
      "Epoch 43/50, Training Loss: 1.637719750404358, Validation Loss: 1.6126604080200195\n",
      "Epoch 44/50, Training Loss: 1.60017728805542, Validation Loss: 1.5893174409866333\n",
      "Epoch 45/50, Training Loss: 1.5743407011032104, Validation Loss: 1.577599048614502\n",
      "Epoch 46/50, Training Loss: 1.5601954460144043, Validation Loss: 1.5761109590530396\n",
      "Epoch 47/50, Training Loss: 1.5564264059066772, Validation Loss: 1.5824668407440186\n",
      "Epoch 48/50, Training Loss: 1.56071937084198, Validation Loss: 1.5936886072158813\n",
      "Epoch 49/50, Training Loss: 1.570159912109375, Validation Loss: 1.6066477298736572\n",
      "Epoch 50/50, Training Loss: 1.5816707611083984, Validation Loss: 1.6184866428375244\n",
      "Batch Size: 32\n",
      "Epochs: 50\n",
      "MSE: 1.6011877059936523\n",
      "RMSE: 1.2653805017471313\n",
      "Negative: \n",
      "6687\n",
      "Positive: \n",
      "2854\n",
      "Precision: 0.701599271876219\n",
      "Recall: 0.8069388365485269\n",
      "Precision@k: 0.714\n",
      "Recall@k: 0.10677433826828174\n",
      "9\n",
      "Epoch 1/50, Training Loss: 15.706992149353027, Validation Loss: 14.730043411254883\n",
      "Epoch 2/50, Training Loss: 14.792852401733398, Validation Loss: 13.759275436401367\n",
      "Epoch 3/50, Training Loss: 13.819767951965332, Validation Loss: 12.735519409179688\n",
      "Epoch 4/50, Training Loss: 12.793327331542969, Validation Loss: 11.663445472717285\n",
      "Epoch 5/50, Training Loss: 11.718119621276855, Validation Loss: 10.548060417175293\n",
      "Epoch 6/50, Training Loss: 10.599067687988281, Validation Loss: 9.39852523803711\n",
      "Epoch 7/50, Training Loss: 9.445255279541016, Validation Loss: 8.229667663574219\n",
      "Epoch 8/50, Training Loss: 8.27139949798584, Validation Loss: 7.062080383300781\n",
      "Epoch 9/50, Training Loss: 7.097986698150635, Validation Loss: 5.921096324920654\n",
      "Epoch 10/50, Training Loss: 5.9502177238464355, Validation Loss: 4.841446399688721\n",
      "Epoch 11/50, Training Loss: 4.86269998550415, Validation Loss: 3.8647820949554443\n",
      "Epoch 12/50, Training Loss: 3.876952648162842, Validation Loss: 3.036628484725952\n",
      "Epoch 13/50, Training Loss: 3.038541793823242, Validation Loss: 2.410942554473877\n",
      "Epoch 14/50, Training Loss: 2.40134334564209, Validation Loss: 2.0361087322235107\n",
      "Epoch 15/50, Training Loss: 2.013723850250244, Validation Loss: 1.942120909690857\n",
      "Epoch 16/50, Training Loss: 1.9059815406799316, Validation Loss: 2.118393898010254\n",
      "Epoch 17/50, Training Loss: 2.0684549808502197, Validation Loss: 2.491314649581909\n",
      "Epoch 18/50, Training Loss: 2.428797483444214, Validation Loss: 2.9188168048858643\n",
      "Epoch 19/50, Training Loss: 2.846391439437866, Validation Loss: 3.238365888595581\n",
      "Epoch 20/50, Training Loss: 3.1598265171051025, Validation Loss: 3.3479602336883545\n",
      "Epoch 21/50, Training Loss: 3.2673192024230957, Validation Loss: 3.257542848587036\n",
      "Epoch 22/50, Training Loss: 3.1785144805908203, Validation Loss: 3.0229766368865967\n",
      "Epoch 23/50, Training Loss: 2.9484128952026367, Validation Loss: 2.7251930236816406\n",
      "Epoch 24/50, Training Loss: 2.657015323638916, Validation Loss: 2.425628185272217\n",
      "Epoch 25/50, Training Loss: 2.364954710006714, Validation Loss: 2.174137592315674\n",
      "Epoch 26/50, Training Loss: 2.121443033218384, Validation Loss: 1.9947611093521118\n",
      "Epoch 27/50, Training Loss: 1.9499177932739258, Validation Loss: 1.8927910327911377\n",
      "Epoch 28/50, Training Loss: 1.8552680015563965, Validation Loss: 1.8588266372680664\n",
      "Epoch 29/50, Training Loss: 1.8278381824493408, Validation Loss: 1.874644160270691\n",
      "Epoch 30/50, Training Loss: 1.8492625951766968, Validation Loss: 1.9190025329589844\n",
      "Epoch 31/50, Training Loss: 1.8982453346252441, Validation Loss: 1.9719854593276978\n",
      "Epoch 32/50, Training Loss: 1.95487380027771, Validation Loss: 2.017606735229492\n",
      "Epoch 33/50, Training Loss: 2.0031986236572266, Validation Loss: 2.040678024291992\n",
      "Epoch 34/50, Training Loss: 2.0281074047088623, Validation Loss: 2.037271022796631\n",
      "Epoch 35/50, Training Loss: 2.025745153427124, Validation Loss: 2.003741502761841\n",
      "Epoch 36/50, Training Loss: 1.9927319288253784, Validation Loss: 1.944102168083191\n",
      "Epoch 37/50, Training Loss: 1.933060646057129, Validation Loss: 1.8692238330841064\n",
      "Epoch 38/50, Training Loss: 1.8568824529647827, Validation Loss: 1.796769380569458\n",
      "Epoch 39/50, Training Loss: 1.7815723419189453, Validation Loss: 1.7368237972259521\n",
      "Epoch 40/50, Training Loss: 1.718278408050537, Validation Loss: 1.698983907699585\n",
      "Epoch 41/50, Training Loss: 1.6767905950546265, Validation Loss: 1.6876497268676758\n",
      "Epoch 42/50, Training Loss: 1.6617571115493774, Validation Loss: 1.7002454996109009\n",
      "Epoch 43/50, Training Loss: 1.6709035634994507, Validation Loss: 1.7271062135696411\n",
      "Epoch 44/50, Training Loss: 1.6948977708816528, Validation Loss: 1.7543004751205444\n",
      "Epoch 45/50, Training Loss: 1.7201056480407715, Validation Loss: 1.7689892053604126\n",
      "Epoch 46/50, Training Loss: 1.7338787317276, Validation Loss: 1.764421820640564\n",
      "Epoch 47/50, Training Loss: 1.7295043468475342, Validation Loss: 1.7416114807128906\n",
      "Epoch 48/50, Training Loss: 1.7078914642333984, Validation Loss: 1.7074408531188965\n",
      "Epoch 49/50, Training Loss: 1.67572021484375, Validation Loss: 1.6710307598114014\n",
      "Epoch 50/50, Training Loss: 1.6418569087982178, Validation Loss: 1.64028799533844\n",
      "Batch Size: 32\n",
      "Epochs: 50\n",
      "MSE: 1.6213433742523193\n",
      "RMSE: 1.2733198404312134\n",
      "Negative: \n",
      "6687\n",
      "Positive: \n",
      "2854\n",
      "Precision: 0.702416918429003\n",
      "Recall: 0.6953790937640197\n",
      "Precision@k: 0.705\n",
      "Recall@k: 0.10542844324809332\n",
      "10\n",
      "Epoch 1/50, Training Loss: 224.79039001464844, Validation Loss: 55.717594146728516\n",
      "Epoch 2/50, Training Loss: 961.5164794921875, Validation Loss: 11.16683578491211\n",
      "Epoch 3/50, Training Loss: 194.47828674316406, Validation Loss: 17.79804039001465\n",
      "Epoch 4/50, Training Loss: 92.5750732421875, Validation Loss: 50.318275451660156\n",
      "Epoch 5/50, Training Loss: 451.81512451171875, Validation Loss: 38.3483772277832\n",
      "Epoch 6/50, Training Loss: 319.0248718261719, Validation Loss: 11.526922225952148\n",
      "Epoch 7/50, Training Loss: 42.777244567871094, Validation Loss: 4.588873386383057\n",
      "Epoch 8/50, Training Loss: 48.68587875366211, Validation Loss: 12.549272537231445\n",
      "Epoch 9/50, Training Loss: 220.9023895263672, Validation Loss: 13.486371040344238\n",
      "Epoch 10/50, Training Loss: 237.57333374023438, Validation Loss: 6.2712602615356445\n",
      "Epoch 11/50, Training Loss: 96.7491683959961, Validation Loss: 3.9665071964263916\n",
      "Epoch 12/50, Training Loss: 4.682578086853027, Validation Loss: 11.546472549438477\n",
      "Epoch 13/50, Training Loss: 56.82091522216797, Validation Loss: 19.471635818481445\n",
      "Epoch 14/50, Training Loss: 141.47824096679688, Validation Loss: 17.9708251953125\n",
      "Epoch 15/50, Training Loss: 128.8890838623047, Validation Loss: 9.624553680419922\n",
      "Epoch 16/50, Training Loss: 46.19340515136719, Validation Loss: 3.465979814529419\n",
      "Epoch 17/50, Training Loss: 3.137862205505371, Validation Loss: 3.4492545127868652\n",
      "Epoch 18/50, Training Loss: 38.48230743408203, Validation Loss: 5.5630879402160645\n",
      "Epoch 19/50, Training Loss: 85.41841888427734, Validation Loss: 5.0883941650390625\n",
      "Epoch 20/50, Training Loss: 76.01852416992188, Validation Loss: 2.8751137256622314\n",
      "Epoch 21/50, Training Loss: 27.2802677154541, Validation Loss: 2.814850330352783\n",
      "Epoch 22/50, Training Loss: 2.5508267879486084, Validation Loss: 5.809122085571289\n",
      "Epoch 23/50, Training Loss: 24.369037628173828, Validation Loss: 8.422287940979004\n",
      "Epoch 24/50, Training Loss: 52.340328216552734, Validation Loss: 7.565576076507568\n",
      "Epoch 25/50, Training Loss: 45.417259216308594, Validation Loss: 4.357665538787842\n",
      "Epoch 26/50, Training Loss: 15.441344261169434, Validation Loss: 2.149604320526123\n",
      "Epoch 27/50, Training Loss: 2.242830276489258, Validation Loss: 2.240950107574463\n",
      "Epoch 28/50, Training Loss: 17.248910903930664, Validation Loss: 2.957012176513672\n",
      "Epoch 29/50, Training Loss: 33.19000244140625, Validation Loss: 2.6270642280578613\n",
      "Epoch 30/50, Training Loss: 26.46199607849121, Validation Loss: 1.8422774076461792\n",
      "Epoch 31/50, Training Loss: 7.862724781036377, Validation Loss: 2.1092379093170166\n",
      "Epoch 32/50, Training Loss: 2.4211478233337402, Validation Loss: 3.379287004470825\n",
      "Epoch 33/50, Training Loss: 13.48788070678711, Validation Loss: 4.063568592071533\n",
      "Epoch 34/50, Training Loss: 21.444026947021484, Validation Loss: 3.3054850101470947\n",
      "Epoch 35/50, Training Loss: 14.35611629486084, Validation Loss: 2.060992479324341\n",
      "Epoch 36/50, Training Loss: 3.351125955581665, Validation Loss: 1.6116161346435547\n",
      "Epoch 37/50, Training Loss: 3.2845230102539062, Validation Loss: 1.9124937057495117\n",
      "Epoch 38/50, Training Loss: 11.236051559448242, Validation Loss: 2.03586483001709\n",
      "Epoch 39/50, Training Loss: 13.483362197875977, Validation Loss: 1.7123794555664062\n",
      "Epoch 40/50, Training Loss: 6.862827301025391, Validation Loss: 1.5746043920516968\n",
      "Epoch 41/50, Training Loss: 1.6298723220825195, Validation Loss: 1.9797651767730713\n",
      "Epoch 42/50, Training Loss: 4.407787799835205, Validation Loss: 2.3965823650360107\n",
      "Epoch 43/50, Training Loss: 9.024249076843262, Validation Loss: 2.242853879928589\n",
      "Epoch 44/50, Training Loss: 7.653034210205078, Validation Loss: 1.748382568359375\n",
      "Epoch 45/50, Training Loss: 2.806340456008911, Validation Loss: 1.5261976718902588\n",
      "Epoch 46/50, Training Loss: 1.8699617385864258, Validation Loss: 1.661605715751648\n",
      "Epoch 47/50, Training Loss: 5.102844715118408, Validation Loss: 1.7394165992736816\n",
      "Epoch 48/50, Training Loss: 6.384864807128906, Validation Loss: 1.5984289646148682\n",
      "Epoch 49/50, Training Loss: 3.676923990249634, Validation Loss: 1.5278961658477783\n",
      "Epoch 50/50, Training Loss: 1.5200350284576416, Validation Loss: 1.6969863176345825\n",
      "Batch Size: 32\n",
      "Epochs: 50\n",
      "MSE: 1.7392581701278687\n",
      "RMSE: 1.3188093900680542\n",
      "Negative: \n",
      "6687\n",
      "Positive: \n",
      "2854\n",
      "Precision: 0.6965922850717869\n",
      "Recall: 0.6022132495887543\n",
      "Precision@k: 0.703\n",
      "Recall@k: 0.10512935546582922\n",
      "11\n",
      "Epoch 1/50, Training Loss: 32.03974914550781, Validation Loss: 26.219633102416992\n",
      "Epoch 2/50, Training Loss: 26.187829971313477, Validation Loss: 20.939720153808594\n",
      "Epoch 3/50, Training Loss: 20.919458389282227, Validation Loss: 16.272098541259766\n",
      "Epoch 4/50, Training Loss: 16.262615203857422, Validation Loss: 12.239788055419922\n",
      "Epoch 5/50, Training Loss: 12.240240097045898, Validation Loss: 8.859573364257812\n",
      "Epoch 6/50, Training Loss: 8.869068145751953, Validation Loss: 6.1401286125183105\n",
      "Epoch 7/50, Training Loss: 6.1577253341674805, Validation Loss: 4.079244136810303\n",
      "Epoch 8/50, Training Loss: 4.103969097137451, Validation Loss: 2.6598222255706787\n",
      "Epoch 9/50, Training Loss: 2.6906750202178955, Validation Loss: 1.8443022966384888\n",
      "Epoch 10/50, Training Loss: 1.880268931388855, Validation Loss: 1.5676193237304688\n",
      "Epoch 11/50, Training Loss: 1.607698917388916, Validation Loss: 1.7302820682525635\n",
      "Epoch 12/50, Training Loss: 1.7735164165496826, Validation Loss: 2.196347951889038\n",
      "Epoch 13/50, Training Loss: 2.2418651580810547, Validation Loss: 2.804445743560791\n",
      "Epoch 14/50, Training Loss: 2.85150408744812, Validation Loss: 3.3958704471588135\n",
      "Epoch 15/50, Training Loss: 3.4438908100128174, Validation Loss: 3.8482377529144287\n",
      "Epoch 16/50, Training Loss: 3.8967983722686768, Validation Loss: 4.094939708709717\n",
      "Epoch 17/50, Training Loss: 4.143743991851807, Validation Loss: 4.1236572265625\n",
      "Epoch 18/50, Training Loss: 4.172485828399658, Validation Loss: 3.9620072841644287\n",
      "Epoch 19/50, Training Loss: 4.010676860809326, Validation Loss: 3.6603057384490967\n",
      "Epoch 20/50, Training Loss: 3.708641290664673, Validation Loss: 3.276837110519409\n",
      "Epoch 21/50, Training Loss: 3.324662685394287, Validation Loss: 2.8672425746917725\n",
      "Epoch 22/50, Training Loss: 2.914379596710205, Validation Loss: 2.4778547286987305\n",
      "Epoch 23/50, Training Loss: 2.524130344390869, Validation Loss: 2.142300605773926\n",
      "Epoch 24/50, Training Loss: 2.187558889389038, Validation Loss: 1.8806270360946655\n",
      "Epoch 25/50, Training Loss: 1.9247419834136963, Validation Loss: 1.7002713680267334\n",
      "Epoch 26/50, Training Loss: 1.7431560754776, Validation Loss: 1.5982328653335571\n",
      "Epoch 27/50, Training Loss: 1.639845371246338, Validation Loss: 1.5638893842697144\n",
      "Epoch 28/50, Training Loss: 1.6042358875274658, Validation Loss: 1.5819625854492188\n",
      "Epoch 29/50, Training Loss: 1.6210952997207642, Validation Loss: 1.6352479457855225\n",
      "Epoch 30/50, Training Loss: 1.673259973526001, Validation Loss: 1.7068653106689453\n",
      "Epoch 31/50, Training Loss: 1.7438839673995972, Validation Loss: 1.7819081544876099\n",
      "Epoch 32/50, Training Loss: 1.818086862564087, Validation Loss: 1.8484853506088257\n",
      "Epoch 33/50, Training Loss: 1.8839936256408691, Validation Loss: 1.898219108581543\n",
      "Epoch 34/50, Training Loss: 1.933234453201294, Validation Loss: 1.9263033866882324\n",
      "Epoch 35/50, Training Loss: 1.9610025882720947, Validation Loss: 1.9312279224395752\n",
      "Epoch 36/50, Training Loss: 1.965780258178711, Validation Loss: 1.914279818534851\n",
      "Epoch 37/50, Training Loss: 1.9488412141799927, Validation Loss: 1.87890625\n",
      "Epoch 38/50, Training Loss: 1.9136141538619995, Validation Loss: 1.8300107717514038\n",
      "Epoch 39/50, Training Loss: 1.8649811744689941, Validation Loss: 1.7732465267181396\n",
      "Epoch 40/50, Training Loss: 1.8085711002349854, Validation Loss: 1.7143455743789673\n",
      "Epoch 41/50, Training Loss: 1.7500916719436646, Validation Loss: 1.6585360765457153\n",
      "Epoch 42/50, Training Loss: 1.6947461366653442, Validation Loss: 1.610074520111084\n",
      "Epoch 43/50, Training Loss: 1.6467695236206055, Validation Loss: 1.5718992948532104\n",
      "Epoch 44/50, Training Loss: 1.609075665473938, Validation Loss: 1.5454814434051514\n",
      "Epoch 45/50, Training Loss: 1.583117127418518, Validation Loss: 1.5308213233947754\n",
      "Epoch 46/50, Training Loss: 1.5688773393630981, Validation Loss: 1.526611566543579\n",
      "Epoch 47/50, Training Loss: 1.565036416053772, Validation Loss: 1.530541181564331\n",
      "Epoch 48/50, Training Loss: 1.5692729949951172, Validation Loss: 1.5396945476531982\n",
      "Epoch 49/50, Training Loss: 1.5786645412445068, Validation Loss: 1.5509905815124512\n",
      "Epoch 50/50, Training Loss: 1.5901267528533936, Validation Loss: 1.5616014003753662\n",
      "Batch Size: 32\n",
      "Epochs: 50\n",
      "MSE: 1.6132724285125732\n",
      "RMSE: 1.2701466083526611\n",
      "Negative: \n",
      "6573\n",
      "Positive: \n",
      "2968\n",
      "Precision: 0.6900707361802463\n",
      "Recall: 0.8014605203103605\n",
      "Precision@k: 0.726\n",
      "Recall@k: 0.1104518484710178\n",
      "12\n",
      "Epoch 1/50, Training Loss: 15.742907524108887, Validation Loss: 14.809481620788574\n",
      "Epoch 2/50, Training Loss: 14.827166557312012, Validation Loss: 13.832557678222656\n",
      "Epoch 3/50, Training Loss: 13.852383613586426, Validation Loss: 12.802029609680176\n",
      "Epoch 4/50, Training Loss: 12.824165344238281, Validation Loss: 11.722550392150879\n",
      "Epoch 5/50, Training Loss: 11.747115135192871, Validation Loss: 10.599072456359863\n",
      "Epoch 6/50, Training Loss: 10.62617301940918, Validation Loss: 9.44067096710205\n",
      "Epoch 7/50, Training Loss: 9.470442771911621, Validation Loss: 8.26211166381836\n",
      "Epoch 8/50, Training Loss: 8.29467487335205, Validation Loss: 7.083946704864502\n",
      "Epoch 9/50, Training Loss: 7.119393825531006, Validation Loss: 5.931371212005615\n",
      "Epoch 10/50, Training Loss: 5.9698662757873535, Validation Loss: 4.839147090911865\n",
      "Epoch 11/50, Training Loss: 4.880772113800049, Validation Loss: 3.848975896835327\n",
      "Epoch 12/50, Training Loss: 3.8937058448791504, Validation Loss: 3.006707191467285\n",
      "Epoch 13/50, Training Loss: 3.0542664527893066, Validation Loss: 2.366363525390625\n",
      "Epoch 14/50, Training Loss: 2.4164936542510986, Validation Loss: 1.9763281345367432\n",
      "Epoch 15/50, Training Loss: 2.028799057006836, Validation Loss: 1.8670146465301514\n",
      "Epoch 16/50, Training Loss: 1.9214649200439453, Validation Loss: 2.028681755065918\n",
      "Epoch 17/50, Training Loss: 2.0845530033111572, Validation Loss: 2.3888678550720215\n",
      "Epoch 18/50, Training Loss: 2.4453608989715576, Validation Loss: 2.806985378265381\n",
      "Epoch 19/50, Training Loss: 2.86326265335083, Validation Loss: 3.121753692626953\n",
      "Epoch 20/50, Training Loss: 3.177654266357422, Validation Loss: 3.228217124938965\n",
      "Epoch 21/50, Training Loss: 3.2842509746551514, Validation Loss: 3.1389896869659424\n",
      "Epoch 22/50, Training Loss: 3.1950864791870117, Validation Loss: 2.907576084136963\n",
      "Epoch 23/50, Training Loss: 2.9635679721832275, Validation Loss: 2.6159842014312744\n",
      "Epoch 24/50, Training Loss: 2.67142915725708, Validation Loss: 2.3241050243377686\n",
      "Epoch 25/50, Training Loss: 2.378783702850342, Validation Loss: 2.0808842182159424\n",
      "Epoch 26/50, Training Loss: 2.1345858573913574, Validation Loss: 1.9102317094802856\n",
      "Epoch 27/50, Training Loss: 1.9627759456634521, Validation Loss: 1.816649079322815\n",
      "Epoch 28/50, Training Loss: 1.8679183721542358, Validation Loss: 1.7903650999069214\n",
      "Epoch 29/50, Training Loss: 1.8403023481369019, Validation Loss: 1.8129127025604248\n",
      "Epoch 30/50, Training Loss: 1.8615212440490723, Validation Loss: 1.8629183769226074\n",
      "Epoch 31/50, Training Loss: 1.9102545976638794, Validation Loss: 1.9204187393188477\n",
      "Epoch 32/50, Training Loss: 1.9665820598602295, Validation Loss: 1.9683510065078735\n",
      "Epoch 33/50, Training Loss: 2.013500690460205, Validation Loss: 1.9921839237213135\n",
      "Epoch 34/50, Training Loss: 2.036546468734741, Validation Loss: 1.9889423847198486\n",
      "Epoch 35/50, Training Loss: 2.0327017307281494, Validation Loss: 1.9531152248382568\n",
      "Epoch 36/50, Training Loss: 1.9964029788970947, Validation Loss: 1.892228364944458\n",
      "Epoch 37/50, Training Loss: 1.9349514245986938, Validation Loss: 1.8173234462738037\n",
      "Epoch 38/50, Training Loss: 1.8596746921539307, Validation Loss: 1.7435261011123657\n",
      "Epoch 39/50, Training Loss: 1.786007285118103, Validation Loss: 1.682361125946045\n",
      "Epoch 40/50, Training Loss: 1.7250006198883057, Validation Loss: 1.6431853771209717\n",
      "Epoch 41/50, Training Loss: 1.6859674453735352, Validation Loss: 1.630131483078003\n",
      "Epoch 42/50, Training Loss: 1.6730027198791504, Validation Loss: 1.640437364578247\n",
      "Epoch 43/50, Training Loss: 1.6833148002624512, Validation Loss: 1.664498209953308\n",
      "Epoch 44/50, Training Loss: 1.7072854042053223, Validation Loss: 1.6888049840927124\n",
      "Epoch 45/50, Training Loss: 1.731406569480896, Validation Loss: 1.7012183666229248\n",
      "Epoch 46/50, Training Loss: 1.743552327156067, Validation Loss: 1.6956696510314941\n",
      "Epoch 47/50, Training Loss: 1.7376705408096313, Validation Loss: 1.673547387123108\n",
      "Epoch 48/50, Training Loss: 1.7151598930358887, Validation Loss: 1.6416939496994019\n",
      "Epoch 49/50, Training Loss: 1.6828685998916626, Validation Loss: 1.608838677406311\n",
      "Epoch 50/50, Training Loss: 1.6495293378829956, Validation Loss: 1.5823042392730713\n",
      "Batch Size: 32\n",
      "Epochs: 50\n",
      "MSE: 1.6315988302230835\n",
      "RMSE: 1.2773405313491821\n",
      "Negative: \n",
      "6573\n",
      "Positive: \n",
      "2968\n",
      "Precision: 0.6934989267095982\n",
      "Recall: 0.6881180587250875\n",
      "Precision@k: 0.715\n",
      "Recall@k: 0.10877833561539632\n",
      "13\n",
      "Epoch 1/50, Training Loss: 215.77951049804688, Validation Loss: 62.423072814941406\n",
      "Epoch 2/50, Training Loss: 914.8936157226562, Validation Loss: 12.51742935180664\n",
      "Epoch 3/50, Training Loss: 186.41246032714844, Validation Loss: 18.371877670288086\n",
      "Epoch 4/50, Training Loss: 87.5238265991211, Validation Loss: 53.5824089050293\n",
      "Epoch 5/50, Training Loss: 429.62158203125, Validation Loss: 40.72334671020508\n",
      "Epoch 6/50, Training Loss: 304.1808776855469, Validation Loss: 11.840869903564453\n",
      "Epoch 7/50, Training Loss: 41.140316009521484, Validation Loss: 4.853210926055908\n",
      "Epoch 8/50, Training Loss: 46.35990524291992, Validation Loss: 14.02304744720459\n",
      "Epoch 9/50, Training Loss: 210.2405242919922, Validation Loss: 15.088583946228027\n",
      "Epoch 10/50, Training Loss: 226.2308349609375, Validation Loss: 6.878347396850586\n",
      "Epoch 11/50, Training Loss: 92.20648956298828, Validation Loss: 3.943105697631836\n",
      "Epoch 12/50, Training Loss: 4.6179585456848145, Validation Loss: 11.934425354003906\n",
      "Epoch 13/50, Training Loss: 54.346229553222656, Validation Loss: 20.47610855102539\n",
      "Epoch 14/50, Training Loss: 134.85720825195312, Validation Loss: 18.858781814575195\n",
      "Epoch 15/50, Training Loss: 122.66934204101562, Validation Loss: 9.89087200164795\n",
      "Epoch 16/50, Training Loss: 43.897674560546875, Validation Loss: 3.4203922748565674\n",
      "Epoch 17/50, Training Loss: 3.1224446296691895, Validation Loss: 3.6668262481689453\n",
      "Epoch 18/50, Training Loss: 36.967220306396484, Validation Loss: 6.112530708312988\n",
      "Epoch 19/50, Training Loss: 81.5324935913086, Validation Loss: 5.5556793212890625\n",
      "Epoch 20/50, Training Loss: 72.29640197753906, Validation Loss: 2.995924472808838\n",
      "Epoch 21/50, Training Loss: 25.83999252319336, Validation Loss: 2.7859268188476562\n",
      "Epoch 22/50, Training Loss: 2.5433149337768555, Validation Loss: 5.958853721618652\n",
      "Epoch 23/50, Training Loss: 23.522579193115234, Validation Loss: 8.760920524597168\n",
      "Epoch 24/50, Training Loss: 50.027992248535156, Validation Loss: 7.823427677154541\n",
      "Epoch 25/50, Training Loss: 43.161224365234375, Validation Loss: 4.387638092041016\n",
      "Epoch 26/50, Training Loss: 14.582849502563477, Validation Loss: 2.1005125045776367\n",
      "Epoch 27/50, Training Loss: 2.2539992332458496, Validation Loss: 2.316884756088257\n",
      "Epoch 28/50, Training Loss: 16.70755386352539, Validation Loss: 3.1434173583984375\n",
      "Epoch 29/50, Training Loss: 31.747547149658203, Validation Loss: 2.752289056777954\n",
      "Epoch 30/50, Training Loss: 25.12071990966797, Validation Loss: 1.8374035358428955\n",
      "Epoch 31/50, Training Loss: 7.414859294891357, Validation Loss: 2.0837976932525635\n",
      "Epoch 32/50, Training Loss: 2.4438164234161377, Validation Loss: 3.4406824111938477\n",
      "Epoch 33/50, Training Loss: 13.084741592407227, Validation Loss: 4.166452407836914\n",
      "Epoch 34/50, Training Loss: 20.517406463623047, Validation Loss: 3.3392438888549805\n",
      "Epoch 35/50, Training Loss: 13.605582237243652, Validation Loss: 2.017622470855713\n",
      "Epoch 36/50, Training Loss: 3.1826186180114746, Validation Loss: 1.5832157135009766\n",
      "Epoch 37/50, Training Loss: 3.293548822402954, Validation Loss: 1.9468168020248413\n",
      "Epoch 38/50, Training Loss: 10.889949798583984, Validation Loss: 2.0777082443237305\n",
      "Epoch 39/50, Training Loss: 12.881387710571289, Validation Loss: 1.7020405530929565\n",
      "Epoch 40/50, Training Loss: 6.497274398803711, Validation Loss: 1.5356011390686035\n",
      "Epoch 41/50, Training Loss: 1.614506721496582, Validation Loss: 1.9695829153060913\n",
      "Epoch 42/50, Training Loss: 4.371498107910156, Validation Loss: 2.4140117168426514\n",
      "Epoch 43/50, Training Loss: 8.718968391418457, Validation Loss: 2.2383840084075928\n",
      "Epoch 44/50, Training Loss: 7.297616004943848, Validation Loss: 1.7072468996047974\n",
      "Epoch 45/50, Training Loss: 2.68638277053833, Validation Loss: 1.4862397909164429\n",
      "Epoch 46/50, Training Loss: 1.8985888957977295, Validation Loss: 1.6479930877685547\n",
      "Epoch 47/50, Training Loss: 5.006568431854248, Validation Loss: 1.729345440864563\n",
      "Epoch 48/50, Training Loss: 6.146716594696045, Validation Loss: 1.5659444332122803\n",
      "Epoch 49/50, Training Loss: 3.5165600776672363, Validation Loss: 1.4857850074768066\n",
      "Epoch 50/50, Training Loss: 1.5228441953659058, Validation Loss: 1.670025110244751\n",
      "Batch Size: 32\n",
      "Epochs: 50\n",
      "MSE: 1.7567096948623657\n",
      "RMSE: 1.3254092931747437\n",
      "Negative: \n",
      "6573\n",
      "Positive: \n",
      "2968\n",
      "Precision: 0.687478077867415\n",
      "Recall: 0.5963791267305645\n",
      "Precision@k: 0.711\n",
      "Recall@k: 0.10816978548607942\n",
      "14\n",
      "Epoch 1/50, Training Loss: 31.975980758666992, Validation Loss: 26.271692276000977\n",
      "Epoch 2/50, Training Loss: 26.12700653076172, Validation Loss: 20.993806838989258\n",
      "Epoch 3/50, Training Loss: 20.862537384033203, Validation Loss: 16.32645034790039\n",
      "Epoch 4/50, Training Loss: 16.21057891845703, Validation Loss: 12.292634963989258\n",
      "Epoch 5/50, Training Loss: 12.194089889526367, Validation Loss: 8.90914249420166\n",
      "Epoch 6/50, Training Loss: 8.829774856567383, Validation Loss: 6.184662818908691\n",
      "Epoch 7/50, Training Loss: 6.126181125640869, Validation Loss: 4.117006301879883\n",
      "Epoch 8/50, Training Loss: 4.080899715423584, Validation Loss: 2.6891064643859863\n",
      "Epoch 9/50, Training Loss: 2.6765313148498535, Validation Loss: 1.8634443283081055\n",
      "Epoch 10/50, Training Loss: 1.8750708103179932, Validation Loss: 1.5750149488449097\n",
      "Epoch 11/50, Training Loss: 1.6108263731002808, Validation Loss: 1.7244449853897095\n",
      "Epoch 12/50, Training Loss: 1.7835110425949097, Validation Loss: 2.1760811805725098\n",
      "Epoch 13/50, Training Loss: 2.2563629150390625, Validation Loss: 2.769207000732422\n",
      "Epoch 14/50, Training Loss: 2.8675146102905273, Validation Loss: 3.3462514877319336\n",
      "Epoch 15/50, Training Loss: 3.458465814590454, Validation Loss: 3.7862398624420166\n",
      "Epoch 16/50, Training Loss: 3.907752752304077, Validation Loss: 4.023802280426025\n",
      "Epoch 17/50, Training Loss: 4.150003910064697, Validation Loss: 4.047375679016113\n",
      "Epoch 18/50, Training Loss: 4.174032688140869, Validation Loss: 3.8847756385803223\n",
      "Epoch 19/50, Training Loss: 4.0082597732543945, Validation Loss: 3.586050271987915\n",
      "Epoch 20/50, Training Loss: 3.7034456729888916, Validation Loss: 3.2089030742645264\n",
      "Epoch 21/50, Training Loss: 3.318020820617676, Validation Loss: 2.808212995529175\n",
      "Epoch 22/50, Training Loss: 2.907554864883423, Validation Loss: 2.4294838905334473\n",
      "Epoch 23/50, Training Loss: 2.518174648284912, Validation Loss: 2.105529546737671\n",
      "Epoch 24/50, Training Loss: 2.183232069015503, Validation Loss: 1.855665683746338\n",
      "Epoch 25/50, Training Loss: 1.9224883317947388, Validation Loss: 1.6867127418518066\n",
      "Epoch 26/50, Training Loss: 1.7431198358535767, Validation Loss: 1.5951941013336182\n",
      "Epoch 27/50, Training Loss: 1.641918420791626, Validation Loss: 1.5701545476913452\n",
      "Epoch 28/50, Training Loss: 1.6081218719482422, Validation Loss: 1.5961167812347412\n",
      "Epoch 29/50, Training Loss: 1.6263777017593384, Validation Loss: 1.6557954549789429\n",
      "Epoch 30/50, Training Loss: 1.6794706583023071, Validation Loss: 1.732321858406067\n",
      "Epoch 31/50, Training Loss: 1.7505594491958618, Validation Loss: 1.8108701705932617\n",
      "Epoch 32/50, Training Loss: 1.8248118162155151, Validation Loss: 1.879676342010498\n",
      "Epoch 33/50, Training Loss: 1.890432357788086, Validation Loss: 1.9305156469345093\n",
      "Epoch 34/50, Training Loss: 1.9391475915908813, Validation Loss: 1.9587440490722656\n",
      "Epoch 35/50, Training Loss: 1.9662516117095947, Validation Loss: 1.9630138874053955\n",
      "Epoch 36/50, Training Loss: 1.9703240394592285, Validation Loss: 1.9447640180587769\n",
      "Epoch 37/50, Training Loss: 1.9527252912521362, Validation Loss: 1.907578468322754\n",
      "Epoch 38/50, Training Loss: 1.916953206062317, Validation Loss: 1.8564823865890503\n",
      "Epoch 39/50, Training Loss: 1.8679393529891968, Validation Loss: 1.7972290515899658\n",
      "Epoch 40/50, Training Loss: 1.811339259147644, Validation Loss: 1.735636830329895\n",
      "Epoch 41/50, Training Loss: 1.7528666257858276, Validation Loss: 1.6770044565200806\n",
      "Epoch 42/50, Training Loss: 1.697709321975708, Validation Loss: 1.6256462335586548\n",
      "Epoch 43/50, Training Loss: 1.6500670909881592, Validation Loss: 1.5845537185668945\n",
      "Epoch 44/50, Training Loss: 1.6128125190734863, Validation Loss: 1.555243730545044\n",
      "Epoch 45/50, Training Loss: 1.5873429775238037, Validation Loss: 1.5377634763717651\n",
      "Epoch 46/50, Training Loss: 1.5735875368118286, Validation Loss: 1.5308551788330078\n",
      "Epoch 47/50, Training Loss: 1.5701769590377808, Validation Loss: 1.532261610031128\n",
      "Epoch 48/50, Training Loss: 1.5747531652450562, Validation Loss: 1.539124608039856\n",
      "Epoch 49/50, Training Loss: 1.5843702554702759, Validation Loss: 1.5484216213226318\n",
      "Epoch 50/50, Training Loss: 1.5959382057189941, Validation Loss: 1.5573827028274536\n",
      "Batch Size: 32\n",
      "Epochs: 50\n",
      "MSE: 1.5834674835205078\n",
      "RMSE: 1.2583590745925903\n",
      "Negative: \n",
      "6639\n",
      "Positive: \n",
      "2902\n",
      "Precision: 0.7003424657534246\n",
      "Recall: 0.8008736255460159\n",
      "Precision@k: 0.727\n",
      "Recall@k: 0.10950444344027715\n",
      "15\n",
      "Epoch 1/50, Training Loss: 15.68142032623291, Validation Loss: 14.891901016235352\n",
      "Epoch 2/50, Training Loss: 14.767162322998047, Validation Loss: 13.913952827453613\n",
      "Epoch 3/50, Training Loss: 13.794052124023438, Validation Loss: 12.88218879699707\n",
      "Epoch 4/50, Training Loss: 12.767728805541992, Validation Loss: 11.801154136657715\n",
      "Epoch 5/50, Training Loss: 11.692845344543457, Validation Loss: 10.675715446472168\n",
      "Epoch 6/50, Training Loss: 10.574390411376953, Validation Loss: 9.514936447143555\n",
      "Epoch 7/50, Training Loss: 9.421514511108398, Validation Loss: 8.333508491516113\n",
      "Epoch 8/50, Training Loss: 8.249029159545898, Validation Loss: 7.151890754699707\n",
      "Epoch 9/50, Training Loss: 7.077531814575195, Validation Loss: 5.995241165161133\n",
      "Epoch 10/50, Training Loss: 5.932353973388672, Validation Loss: 4.898235321044922\n",
      "Epoch 11/50, Training Loss: 4.848281383514404, Validation Loss: 3.902388334274292\n",
      "Epoch 12/50, Training Loss: 3.8669495582580566, Validation Loss: 3.0532565116882324\n",
      "Epoch 13/50, Training Loss: 3.034018039703369, Validation Loss: 2.4048011302948\n",
      "Epoch 14/50, Training Loss: 2.4034066200256348, Validation Loss: 2.0053536891937256\n",
      "Epoch 15/50, Training Loss: 2.0231897830963135, Validation Loss: 1.8850650787353516\n",
      "Epoch 16/50, Training Loss: 1.922888994216919, Validation Loss: 2.0337040424346924\n",
      "Epoch 17/50, Training Loss: 2.091377019882202, Validation Loss: 2.378532886505127\n",
      "Epoch 18/50, Training Loss: 2.45434832572937, Validation Loss: 2.7801177501678467\n",
      "Epoch 19/50, Training Loss: 2.8703184127807617, Validation Loss: 3.079641342163086\n",
      "Epoch 20/50, Training Loss: 3.178654670715332, Validation Loss: 3.1787750720977783\n",
      "Epoch 21/50, Training Loss: 3.280543088912964, Validation Loss: 3.087519884109497\n",
      "Epoch 22/50, Training Loss: 3.1869351863861084, Validation Loss: 2.861107110977173\n",
      "Epoch 23/50, Training Loss: 2.954291343688965, Validation Loss: 2.5778515338897705\n",
      "Epoch 24/50, Training Loss: 2.6623692512512207, Validation Loss: 2.297203302383423\n",
      "Epoch 25/50, Training Loss: 2.3714842796325684, Validation Loss: 2.0665643215179443\n",
      "Epoch 26/50, Training Loss: 2.130084276199341, Validation Loss: 1.9081276655197144\n",
      "Epoch 27/50, Training Loss: 1.9610251188278198, Validation Loss: 1.8257347345352173\n",
      "Epoch 28/50, Training Loss: 1.8686691522598267, Validation Loss: 1.8089791536331177\n",
      "Epoch 29/50, Training Loss: 1.8429925441741943, Validation Loss: 1.839086651802063\n",
      "Epoch 30/50, Training Loss: 1.865458607673645, Validation Loss: 1.89463472366333\n",
      "Epoch 31/50, Training Loss: 1.914763331413269, Validation Loss: 1.9557840824127197\n",
      "Epoch 32/50, Training Loss: 1.9710983037948608, Validation Loss: 2.0061604976654053\n",
      "Epoch 33/50, Training Loss: 2.018094539642334, Validation Loss: 2.0304996967315674\n",
      "Epoch 34/50, Training Loss: 2.0405452251434326, Validation Loss: 2.026562213897705\n",
      "Epoch 35/50, Training Loss: 2.036022424697876, Validation Loss: 1.989305019378662\n",
      "Epoch 36/50, Training Loss: 1.9996299743652344, Validation Loss: 1.9252643585205078\n",
      "Epoch 37/50, Training Loss: 1.9380425214767456, Validation Loss: 1.8456889390945435\n",
      "Epoch 38/50, Training Loss: 1.862494945526123, Validation Loss: 1.7670396566390991\n",
      "Epoch 39/50, Training Loss: 1.7887550592422485, Validation Loss: 1.7006207704544067\n",
      "Epoch 40/50, Training Loss: 1.7279417514801025, Validation Loss: 1.6560211181640625\n",
      "Epoch 41/50, Training Loss: 1.6893410682678223, Validation Loss: 1.637618064880371\n",
      "Epoch 42/50, Training Loss: 1.6769551038742065, Validation Loss: 1.6429197788238525\n",
      "Epoch 43/50, Training Loss: 1.6878446340560913, Validation Loss: 1.6626206636428833\n",
      "Epoch 44/50, Training Loss: 1.7122262716293335, Validation Loss: 1.683526635169983\n",
      "Epoch 45/50, Training Loss: 1.7364833354949951, Validation Loss: 1.693778395652771\n",
      "Epoch 46/50, Training Loss: 1.7484958171844482, Validation Loss: 1.6874855756759644\n",
      "Epoch 47/50, Training Loss: 1.7423290014266968, Validation Loss: 1.666050910949707\n",
      "Epoch 48/50, Training Loss: 1.719545841217041, Validation Loss: 1.636155128479004\n",
      "Epoch 49/50, Training Loss: 1.6871286630630493, Validation Loss: 1.6062124967575073\n",
      "Epoch 50/50, Training Loss: 1.6538679599761963, Validation Loss: 1.583134651184082\n",
      "Batch Size: 32\n",
      "Epochs: 50\n",
      "MSE: 1.6066150665283203\n",
      "RMSE: 1.267523169517517\n",
      "Negative: \n",
      "6639\n",
      "Positive: \n",
      "2902\n",
      "Precision: 0.7044757817290006\n",
      "Recall: 0.6922729326705829\n",
      "Precision@k: 0.714\n",
      "Recall@k: 0.10754631721644826\n",
      "16\n",
      "Epoch 1/50, Training Loss: 237.41940307617188, Validation Loss: 46.000919342041016\n",
      "Epoch 2/50, Training Loss: 1032.060546875, Validation Loss: 9.04918098449707\n",
      "Epoch 3/50, Training Loss: 206.032958984375, Validation Loss: 17.52931785583496\n",
      "Epoch 4/50, Training Loss: 100.77860260009766, Validation Loss: 46.90293884277344\n",
      "Epoch 5/50, Training Loss: 485.6815185546875, Validation Loss: 35.931129455566406\n",
      "Epoch 6/50, Training Loss: 341.2353210449219, Validation Loss: 11.403090476989746\n",
      "Epoch 7/50, Training Loss: 45.16415786743164, Validation Loss: 4.12178373336792\n",
      "Epoch 8/50, Training Loss: 52.338043212890625, Validation Loss: 10.260998725891113\n",
      "Epoch 9/50, Training Loss: 237.0796661376953, Validation Loss: 11.019319534301758\n",
      "Epoch 10/50, Training Loss: 254.7598876953125, Validation Loss: 5.2677154541015625\n",
      "Epoch 11/50, Training Loss: 103.67543029785156, Validation Loss: 4.024538516998291\n",
      "Epoch 12/50, Training Loss: 4.808783054351807, Validation Loss: 11.278366088867188\n",
      "Epoch 13/50, Training Loss: 60.49333572387695, Validation Loss: 18.504343032836914\n",
      "Epoch 14/50, Training Loss: 151.41392517089844, Validation Loss: 17.131412506103516\n",
      "Epoch 15/50, Training Loss: 138.3798065185547, Validation Loss: 9.469355583190918\n",
      "Epoch 16/50, Training Loss: 49.804656982421875, Validation Loss: 3.548778533935547\n",
      "Epoch 17/50, Training Loss: 3.166381359100342, Validation Loss: 3.045361280441284\n",
      "Epoch 18/50, Training Loss: 40.62506103515625, Validation Loss: 4.656981945037842\n",
      "Epoch 19/50, Training Loss: 91.20507049560547, Validation Loss: 4.30264949798584\n",
      "Epoch 20/50, Training Loss: 81.7237777709961, Validation Loss: 2.610426187515259\n",
      "Epoch 21/50, Training Loss: 29.606914520263672, Validation Loss: 2.834275484085083\n",
      "Epoch 22/50, Training Loss: 2.5612173080444336, Validation Loss: 5.671485424041748\n",
      "Epoch 23/50, Training Loss: 25.5023193359375, Validation Loss: 8.077210426330566\n",
      "Epoch 24/50, Training Loss: 55.75939178466797, Validation Loss: 7.316226482391357\n",
      "Epoch 25/50, Training Loss: 48.92021179199219, Validation Loss: 4.353304386138916\n",
      "Epoch 26/50, Training Loss: 16.862789154052734, Validation Loss: 2.177644968032837\n",
      "Epoch 27/50, Training Loss: 2.2181556224823, Validation Loss: 2.0467822551727295\n",
      "Epoch 28/50, Training Loss: 17.945478439331055, Validation Loss: 2.592350959777832\n",
      "Epoch 29/50, Training Loss: 35.29933166503906, Validation Loss: 2.350034475326538\n",
      "Epoch 30/50, Training Loss: 28.575510025024414, Validation Loss: 1.7637637853622437\n",
      "Epoch 31/50, Training Loss: 8.645694732666016, Validation Loss: 2.0918798446655273\n",
      "Epoch 32/50, Training Loss: 2.367483139038086, Validation Loss: 3.2763071060180664\n",
      "Epoch 33/50, Training Loss: 14.006904602050781, Validation Loss: 3.918532371520996\n",
      "Epoch 34/50, Training Loss: 22.837421417236328, Validation Loss: 3.2414681911468506\n",
      "Epoch 35/50, Training Loss: 15.587260246276855, Validation Loss: 2.070573091506958\n",
      "Epoch 36/50, Training Loss: 3.675973892211914, Validation Loss: 1.5699478387832642\n",
      "Epoch 37/50, Training Loss: 3.242114782333374, Validation Loss: 1.7671154737472534\n",
      "Epoch 38/50, Training Loss: 11.712997436523438, Validation Loss: 1.8754178285598755\n",
      "Epoch 39/50, Training Loss: 14.42525577545166, Validation Loss: 1.6285216808319092\n",
      "Epoch 40/50, Training Loss: 7.505509853363037, Validation Loss: 1.5429948568344116\n",
      "Epoch 41/50, Training Loss: 1.691689372062683, Validation Loss: 1.925188422203064\n",
      "Epoch 42/50, Training Loss: 4.437161922454834, Validation Loss: 2.314791440963745\n",
      "Epoch 43/50, Training Loss: 9.479279518127441, Validation Loss: 2.1882033348083496\n",
      "Epoch 44/50, Training Loss: 8.25268268585205, Validation Loss: 1.7290878295898438\n",
      "Epoch 45/50, Training Loss: 3.0544445514678955, Validation Loss: 1.4876056909561157\n",
      "Epoch 46/50, Training Loss: 1.8369818925857544, Validation Loss: 1.5746378898620605\n",
      "Epoch 47/50, Training Loss: 5.238442420959473, Validation Loss: 1.6433367729187012\n",
      "Epoch 48/50, Training Loss: 6.779106140136719, Validation Loss: 1.5361398458480835\n",
      "Epoch 49/50, Training Loss: 3.983142614364624, Validation Loss: 1.4871740341186523\n",
      "Epoch 50/50, Training Loss: 1.5540196895599365, Validation Loss: 1.6454530954360962\n",
      "Batch Size: 32\n",
      "Epochs: 50\n",
      "MSE: 1.7184171676635742\n",
      "RMSE: 1.3108841180801392\n",
      "Negative: \n",
      "6639\n",
      "Positive: \n",
      "2902\n",
      "Precision: 0.7037102473498233\n",
      "Recall: 0.5999397499623437\n",
      "Precision@k: 0.715\n",
      "Recall@k: 0.10769694231058895\n",
      "17\n",
      "Epoch 1/50, Training Loss: 32.0207405090332, Validation Loss: 26.237768173217773\n",
      "Epoch 2/50, Training Loss: 26.16883087158203, Validation Loss: 20.954015731811523\n",
      "Epoch 3/50, Training Loss: 20.90060806274414, Validation Loss: 16.283958435058594\n",
      "Epoch 4/50, Training Loss: 16.244064331054688, Validation Loss: 12.250606536865234\n",
      "Epoch 5/50, Training Loss: 12.222166061401367, Validation Loss: 8.870741844177246\n",
      "Epoch 6/50, Training Loss: 8.851641654968262, Validation Loss: 6.15300178527832\n",
      "Epoch 7/50, Training Loss: 6.141117572784424, Validation Loss: 4.095099449157715\n",
      "Epoch 8/50, Training Loss: 4.088329315185547, Validation Loss: 2.6798019409179688\n",
      "Epoch 9/50, Training Loss: 2.676119327545166, Validation Loss: 1.8693369626998901\n",
      "Epoch 10/50, Training Loss: 1.8668633699417114, Validation Loss: 1.5983232259750366\n",
      "Epoch 11/50, Training Loss: 1.595420002937317, Validation Loss: 1.7668341398239136\n",
      "Epoch 12/50, Training Loss: 1.7622182369232178, Validation Loss: 2.2383954524993896\n",
      "Epoch 13/50, Training Loss: 2.2312591075897217, Validation Loss: 2.851101875305176\n",
      "Epoch 14/50, Training Loss: 2.8411858081817627, Validation Loss: 3.445859670639038\n",
      "Epoch 15/50, Training Loss: 3.433433771133423, Validation Loss: 3.9001412391662598\n",
      "Epoch 16/50, Training Loss: 3.885866403579712, Validation Loss: 4.147418975830078\n",
      "Epoch 17/50, Training Loss: 4.132155895233154, Validation Loss: 4.175594806671143\n",
      "Epoch 18/50, Training Loss: 4.160220623016357, Validation Loss: 4.012556552886963\n",
      "Epoch 19/50, Training Loss: 3.997833251953125, Validation Loss: 3.708890914916992\n",
      "Epoch 20/50, Training Loss: 3.6953907012939453, Validation Loss: 3.3231213092803955\n",
      "Epoch 21/50, Training Loss: 3.3112056255340576, Validation Loss: 2.9110841751098633\n",
      "Epoch 22/50, Training Loss: 2.900909423828125, Validation Loss: 2.519259214401245\n",
      "Epoch 23/50, Training Loss: 2.5108108520507812, Validation Loss: 2.18137788772583\n",
      "Epoch 24/50, Training Loss: 2.174511432647705, Validation Loss: 1.9175528287887573\n",
      "Epoch 25/50, Training Loss: 1.9120399951934814, Validation Loss: 1.7352558374404907\n",
      "Epoch 26/50, Training Loss: 1.7308262586593628, Validation Loss: 1.631499171257019\n",
      "Epoch 27/50, Training Loss: 1.62787663936615, Validation Loss: 1.5956579446792603\n",
      "Epoch 28/50, Training Loss: 1.5925878286361694, Validation Loss: 1.6124427318572998\n",
      "Epoch 29/50, Training Loss: 1.6097086668014526, Validation Loss: 1.6646348237991333\n",
      "Epoch 30/50, Training Loss: 1.6620687246322632, Validation Loss: 1.7353390455245972\n",
      "Epoch 31/50, Training Loss: 1.732822299003601, Validation Loss: 1.8096369504928589\n",
      "Epoch 32/50, Training Loss: 1.8070961236953735, Validation Loss: 1.8756284713745117\n",
      "Epoch 33/50, Training Loss: 1.8730273246765137, Validation Loss: 1.924928903579712\n",
      "Epoch 34/50, Training Loss: 1.922261118888855, Validation Loss: 1.9527266025543213\n",
      "Epoch 35/50, Training Loss: 1.9500043392181396, Validation Loss: 1.9575092792510986\n",
      "Epoch 36/50, Training Loss: 1.9547535181045532, Validation Loss: 1.9405605792999268\n",
      "Epoch 37/50, Training Loss: 1.9377944469451904, Validation Loss: 1.9053226709365845\n",
      "Epoch 38/50, Training Loss: 1.9025646448135376, Validation Loss: 1.8566933870315552\n",
      "Epoch 39/50, Training Loss: 1.853952169418335, Validation Loss: 1.800315022468567\n",
      "Epoch 40/50, Training Loss: 1.7975887060165405, Validation Loss: 1.7419066429138184\n",
      "Epoch 41/50, Training Loss: 1.7391828298568726, Validation Loss: 1.6866778135299683\n",
      "Epoch 42/50, Training Loss: 1.683933973312378, Validation Loss: 1.6388647556304932\n",
      "Epoch 43/50, Training Loss: 1.6360715627670288, Validation Loss: 1.6013801097869873\n",
      "Epoch 44/50, Training Loss: 1.5985054969787598, Validation Loss: 1.575667381286621\n",
      "Epoch 45/50, Training Loss: 1.5726803541183472, Validation Loss: 1.5616979598999023\n",
      "Epoch 46/50, Training Loss: 1.5585731267929077, Validation Loss: 1.5581377744674683\n",
      "Epoch 47/50, Training Loss: 1.5548577308654785, Validation Loss: 1.5626503229141235\n",
      "Epoch 48/50, Training Loss: 1.559208631515503, Validation Loss: 1.572298526763916\n",
      "Epoch 49/50, Training Loss: 1.5687007904052734, Validation Loss: 1.5839866399765015\n",
      "Epoch 50/50, Training Loss: 1.580249309539795, Validation Loss: 1.59487783908844\n",
      "Batch Size: 32\n",
      "Epochs: 50\n",
      "MSE: 1.6220813989639282\n",
      "RMSE: 1.2736096382141113\n",
      "Negative: \n",
      "6633\n",
      "Positive: \n",
      "2908\n",
      "Precision: 0.6929536806193413\n",
      "Recall: 0.7961706618423037\n",
      "Precision@k: 0.714\n",
      "Recall@k: 0.10764360018091361\n",
      "18\n",
      "Epoch 1/50, Training Loss: 15.716845512390137, Validation Loss: 14.817339897155762\n",
      "Epoch 2/50, Training Loss: 14.801281929016113, Validation Loss: 13.840865135192871\n",
      "Epoch 3/50, Training Loss: 13.826698303222656, Validation Loss: 12.810909271240234\n",
      "Epoch 4/50, Training Loss: 12.798704147338867, Validation Loss: 11.73211669921875\n",
      "Epoch 5/50, Training Loss: 11.72192668914795, Validation Loss: 10.609464645385742\n",
      "Epoch 6/50, Training Loss: 10.601314544677734, Validation Loss: 9.452096939086914\n",
      "Epoch 7/50, Training Loss: 9.445971488952637, Validation Loss: 8.274832725524902\n",
      "Epoch 8/50, Training Loss: 8.270665168762207, Validation Loss: 7.098283290863037\n",
      "Epoch 9/50, Training Loss: 7.095933437347412, Validation Loss: 5.947772026062012\n",
      "Epoch 10/50, Training Loss: 5.947041988372803, Validation Loss: 4.858147144317627\n",
      "Epoch 11/50, Training Loss: 4.858712196350098, Validation Loss: 3.871140241622925\n",
      "Epoch 12/50, Training Loss: 3.8725385665893555, Validation Loss: 3.0326690673828125\n",
      "Epoch 13/50, Training Loss: 3.0341899394989014, Validation Loss: 2.3967483043670654\n",
      "Epoch 14/50, Training Loss: 2.397592544555664, Validation Loss: 2.0119166374206543\n",
      "Epoch 15/50, Training Loss: 2.011178970336914, Validation Loss: 1.9082204103469849\n",
      "Epoch 16/50, Training Loss: 1.9050220251083374, Validation Loss: 2.0750732421875\n",
      "Epoch 17/50, Training Loss: 2.069129228591919, Validation Loss: 2.439255952835083\n",
      "Epoch 18/50, Training Loss: 2.4303159713745117, Validation Loss: 2.8600897789001465\n",
      "Epoch 19/50, Training Loss: 2.848055362701416, Validation Loss: 3.1749894618988037\n",
      "Epoch 20/50, Training Loss: 3.161179304122925, Validation Loss: 3.282113552093506\n",
      "Epoch 21/50, Training Loss: 3.267789363861084, Validation Loss: 3.191559314727783\n",
      "Epoch 22/50, Training Loss: 3.1781105995178223, Validation Loss: 2.9590020179748535\n",
      "Epoch 23/50, Training Loss: 2.9471001625061035, Validation Loss: 2.665552854537964\n",
      "Epoch 24/50, Training Loss: 2.6550698280334473, Validation Loss: 2.371556282043457\n",
      "Epoch 25/50, Training Loss: 2.362699031829834, Validation Loss: 2.126089572906494\n",
      "Epoch 26/50, Training Loss: 2.1190528869628906, Validation Loss: 1.9530267715454102\n",
      "Epoch 27/50, Training Loss: 1.9476975202560425, Validation Loss: 1.8572266101837158\n",
      "Epoch 28/50, Training Loss: 1.8533178567886353, Validation Loss: 1.828983187675476\n",
      "Epoch 29/50, Training Loss: 1.8261590003967285, Validation Loss: 1.849863886833191\n",
      "Epoch 30/50, Training Loss: 1.8477818965911865, Validation Loss: 1.8985108137130737\n",
      "Epoch 31/50, Training Loss: 1.8968496322631836, Validation Loss: 1.9549623727798462\n",
      "Epoch 32/50, Training Loss: 1.9534368515014648, Validation Loss: 2.002654552459717\n",
      "Epoch 33/50, Training Loss: 2.0010578632354736, Validation Loss: 2.0259485244750977\n",
      "Epoch 34/50, Training Loss: 2.0243682861328125, Validation Loss: 2.022404909133911\n",
      "Epoch 35/50, Training Loss: 2.020747423171997, Validation Loss: 1.9871971607208252\n",
      "Epoch 36/50, Training Loss: 1.9852594137191772, Validation Loss: 1.9264105558395386\n",
      "Epoch 37/50, Training Loss: 1.9243178367614746, Validation Loss: 1.8510618209838867\n",
      "Epoch 38/50, Training Loss: 1.8489899635314941, Validation Loss: 1.7773784399032593\n",
      "Epoch 39/50, Training Loss: 1.7750358581542969, Validation Loss: 1.7163565158843994\n",
      "Epoch 40/50, Training Loss: 1.7136696577072144, Validation Loss: 1.677404761314392\n",
      "Epoch 41/50, Training Loss: 1.6743028163909912, Validation Loss: 1.6647013425827026\n",
      "Epoch 42/50, Training Loss: 1.6611320972442627, Validation Loss: 1.6754978895187378\n",
      "Epoch 43/50, Training Loss: 1.6714500188827515, Validation Loss: 1.7001442909240723\n",
      "Epoch 44/50, Training Loss: 1.6956641674041748, Validation Loss: 1.725009799003601\n",
      "Epoch 45/50, Training Loss: 1.7202045917510986, Validation Loss: 1.737793207168579\n",
      "Epoch 46/50, Training Loss: 1.732814908027649, Validation Loss: 1.732303261756897\n",
      "Epoch 47/50, Training Loss: 1.7273155450820923, Validation Loss: 1.7098991870880127\n",
      "Epoch 48/50, Training Loss: 1.705043911933899, Validation Loss: 1.6774824857711792\n",
      "Epoch 49/50, Training Loss: 1.6728590726852417, Validation Loss: 1.6438939571380615\n",
      "Epoch 50/50, Training Loss: 1.6395498514175415, Validation Loss: 1.6165754795074463\n",
      "Batch Size: 32\n",
      "Epochs: 50\n",
      "MSE: 1.643239974975586\n",
      "RMSE: 1.2818892002105713\n",
      "Negative: \n",
      "6633\n",
      "Positive: \n",
      "2908\n",
      "Precision: 0.6958857844642309\n",
      "Recall: 0.6834011759384894\n",
      "Precision@k: 0.705\n",
      "Recall@k: 0.10628674807779286\n",
      "19\n",
      "Epoch 1/50, Training Loss: 230.24029541015625, Validation Loss: 43.5626335144043\n",
      "Epoch 2/50, Training Loss: 990.3585815429688, Validation Loss: 8.709935188293457\n",
      "Epoch 3/50, Training Loss: 199.3965606689453, Validation Loss: 16.950048446655273\n",
      "Epoch 4/50, Training Loss: 95.78325653076172, Validation Loss: 45.10520935058594\n",
      "Epoch 5/50, Training Loss: 465.58062744140625, Validation Loss: 34.693878173828125\n",
      "Epoch 6/50, Training Loss: 328.1914367675781, Validation Loss: 11.179173469543457\n",
      "Epoch 7/50, Training Loss: 43.790618896484375, Validation Loss: 4.046477317810059\n",
      "Epoch 8/50, Training Loss: 50.14467239379883, Validation Loss: 9.78547191619873\n",
      "Epoch 9/50, Training Loss: 227.5068817138672, Validation Loss: 10.507275581359863\n",
      "Epoch 10/50, Training Loss: 244.60446166992188, Validation Loss: 5.096193313598633\n",
      "Epoch 11/50, Training Loss: 99.583740234375, Validation Loss: 4.011789798736572\n",
      "Epoch 12/50, Training Loss: 4.735010623931885, Validation Loss: 11.007420539855957\n",
      "Epoch 13/50, Training Loss: 58.344871520996094, Validation Loss: 17.91546630859375\n",
      "Epoch 14/50, Training Loss: 145.5718231201172, Validation Loss: 16.570119857788086\n",
      "Epoch 15/50, Training Loss: 132.75543212890625, Validation Loss: 9.212203025817871\n",
      "Epoch 16/50, Training Loss: 47.642879486083984, Validation Loss: 3.524838924407959\n",
      "Epoch 17/50, Training Loss: 3.1545941829681396, Validation Loss: 3.0117790699005127\n",
      "Epoch 18/50, Training Loss: 39.39686965942383, Validation Loss: 4.519310474395752\n",
      "Epoch 19/50, Training Loss: 87.81482696533203, Validation Loss: 4.171375274658203\n",
      "Epoch 20/50, Training Loss: 78.33843994140625, Validation Loss: 2.5840508937835693\n",
      "Epoch 21/50, Training Loss: 28.20334243774414, Validation Loss: 2.847383737564087\n",
      "Epoch 22/50, Training Loss: 2.5607798099517822, Validation Loss: 5.584768772125244\n",
      "Epoch 23/50, Training Loss: 24.87898826599121, Validation Loss: 7.865919589996338\n",
      "Epoch 24/50, Training Loss: 53.77389907836914, Validation Loss: 7.102246284484863\n",
      "Epoch 25/50, Training Loss: 46.83515930175781, Validation Loss: 4.252363681793213\n",
      "Epoch 26/50, Training Loss: 15.999246597290039, Validation Loss: 2.177706718444824\n",
      "Epoch 27/50, Training Loss: 2.239044666290283, Validation Loss: 2.0570600032806396\n",
      "Epoch 28/50, Training Loss: 17.576263427734375, Validation Loss: 2.569054126739502\n",
      "Epoch 29/50, Training Loss: 34.0727424621582, Validation Loss: 2.3290512561798096\n",
      "Epoch 30/50, Training Loss: 27.31238555908203, Validation Loss: 1.7784732580184937\n",
      "Epoch 31/50, Training Loss: 8.162257194519043, Validation Loss: 2.1102356910705566\n",
      "Epoch 32/50, Training Loss: 2.4070043563842773, Validation Loss: 3.243246078491211\n",
      "Epoch 33/50, Training Loss: 13.731128692626953, Validation Loss: 3.8359711170196533\n",
      "Epoch 34/50, Training Loss: 22.02605628967285, Validation Loss: 3.170060396194458\n",
      "Epoch 35/50, Training Loss: 14.844425201416016, Validation Loss: 2.054882526397705\n",
      "Epoch 36/50, Training Loss: 3.4672412872314453, Validation Loss: 1.593622088432312\n",
      "Epoch 37/50, Training Loss: 3.2787246704101562, Validation Loss: 1.7898857593536377\n",
      "Epoch 38/50, Training Loss: 11.449725151062012, Validation Loss: 1.8878036737442017\n",
      "Epoch 39/50, Training Loss: 13.861454010009766, Validation Loss: 1.6475050449371338\n",
      "Epoch 40/50, Training Loss: 7.105584621429443, Validation Loss: 1.569078803062439\n",
      "Epoch 41/50, Training Loss: 1.6468324661254883, Validation Loss: 1.933972954750061\n",
      "Epoch 42/50, Training Loss: 4.429083824157715, Validation Loss: 2.2945873737335205\n",
      "Epoch 43/50, Training Loss: 9.216510772705078, Validation Loss: 2.162303924560547\n",
      "Epoch 44/50, Training Loss: 7.886551380157471, Validation Loss: 1.7282060384750366\n",
      "Epoch 45/50, Training Loss: 2.891765594482422, Validation Loss: 1.5139739513397217\n",
      "Epoch 46/50, Training Loss: 1.854622721672058, Validation Loss: 1.6080894470214844\n",
      "Epoch 47/50, Training Loss: 5.164462089538574, Validation Loss: 1.6722214221954346\n",
      "Epoch 48/50, Training Loss: 6.543219566345215, Validation Loss: 1.5644646883010864\n",
      "Epoch 49/50, Training Loss: 3.7851336002349854, Validation Loss: 1.5151665210723877\n",
      "Epoch 50/50, Training Loss: 1.5245850086212158, Validation Loss: 1.6621904373168945\n",
      "Batch Size: 32\n",
      "Epochs: 50\n",
      "MSE: 1.7732465267181396\n",
      "RMSE: 1.3316329717636108\n",
      "Negative: \n",
      "6633\n",
      "Positive: \n",
      "2908\n",
      "Precision: 0.6902126164118784\n",
      "Recall: 0.592190562339816\n",
      "Precision@k: 0.689\n",
      "Recall@k: 0.10387456656113372\n",
      "20\n",
      "Epoch 1/50, Training Loss: 32.07390213012695, Validation Loss: 26.12594223022461\n",
      "Epoch 2/50, Training Loss: 26.211044311523438, Validation Loss: 20.87557601928711\n",
      "Epoch 3/50, Training Loss: 20.933252334594727, Validation Loss: 16.232948303222656\n",
      "Epoch 4/50, Training Loss: 16.268583297729492, Validation Loss: 12.220948219299316\n",
      "Epoch 5/50, Training Loss: 12.240021705627441, Validation Loss: 8.856280326843262\n",
      "Epoch 6/50, Training Loss: 8.86433219909668, Validation Loss: 6.147599697113037\n",
      "Epoch 7/50, Training Loss: 6.1501288414001465, Validation Loss: 4.09276008605957\n",
      "Epoch 8/50, Training Loss: 4.095105171203613, Validation Loss: 2.674842119216919\n",
      "Epoch 9/50, Training Loss: 2.6820075511932373, Validation Loss: 1.856611728668213\n",
      "Epoch 10/50, Training Loss: 1.8730391263961792, Validation Loss: 1.5735301971435547\n",
      "Epoch 11/50, Training Loss: 1.6027939319610596, Validation Loss: 1.7268846035003662\n",
      "Epoch 12/50, Training Loss: 1.7713313102722168, Validation Loss: 2.181809425354004\n",
      "Epoch 13/50, Training Loss: 2.2422034740448, Validation Loss: 2.77828049659729\n",
      "Epoch 14/50, Training Loss: 2.8536198139190674, Validation Loss: 3.3590493202209473\n",
      "Epoch 15/50, Training Loss: 3.446723461151123, Validation Loss: 3.8030004501342773\n",
      "Epoch 16/50, Training Loss: 3.8993027210235596, Validation Loss: 4.044339656829834\n",
      "Epoch 17/50, Training Loss: 4.145125865936279, Validation Loss: 4.071022033691406\n",
      "Epoch 18/50, Training Loss: 4.172296524047852, Validation Loss: 3.910478115081787\n",
      "Epoch 19/50, Training Loss: 4.008791923522949, Validation Loss: 3.6125285625457764\n",
      "Epoch 20/50, Training Loss: 3.705192804336548, Validation Loss: 3.23480224609375\n",
      "Epoch 21/50, Training Loss: 3.3199498653411865, Validation Loss: 2.832244873046875\n",
      "Epoch 22/50, Training Loss: 2.9087910652160645, Validation Loss: 2.450531244277954\n",
      "Epoch 23/50, Training Loss: 2.518073797225952, Validation Loss: 2.1227223873138428\n",
      "Epoch 24/50, Training Loss: 2.1814064979553223, Validation Loss: 1.8684238195419312\n",
      "Epoch 25/50, Training Loss: 1.9187986850738525, Validation Loss: 1.6947548389434814\n",
      "Epoch 26/50, Training Loss: 1.7376353740692139, Validation Loss: 1.5985221862792969\n",
      "Epoch 27/50, Training Loss: 1.6348687410354614, Validation Loss: 1.5690159797668457\n",
      "Epoch 28/50, Training Loss: 1.5998374223709106, Validation Loss: 1.5909556150436401\n",
      "Epoch 29/50, Training Loss: 1.61723792552948, Validation Loss: 1.6471959352493286\n",
      "Epoch 30/50, Training Loss: 1.6698546409606934, Validation Loss: 1.7209534645080566\n",
      "Epoch 31/50, Training Loss: 1.7408093214035034, Validation Loss: 1.7974375486373901\n",
      "Epoch 32/50, Training Loss: 1.8152083158493042, Validation Loss: 1.8648755550384521\n",
      "Epoch 33/50, Training Loss: 1.8811814785003662, Validation Loss: 1.9150004386901855\n",
      "Epoch 34/50, Training Loss: 1.930374264717102, Validation Loss: 1.9431018829345703\n",
      "Epoch 35/50, Training Loss: 1.9580048322677612, Validation Loss: 1.9477479457855225\n",
      "Epoch 36/50, Training Loss: 1.9625866413116455, Validation Loss: 1.9302856922149658\n",
      "Epoch 37/50, Training Loss: 1.9454240798950195, Validation Loss: 1.8942055702209473\n",
      "Epoch 38/50, Training Loss: 1.9099762439727783, Validation Loss: 1.8444411754608154\n",
      "Epoch 39/50, Training Loss: 1.8611509799957275, Validation Loss: 1.7866652011871338\n",
      "Epoch 40/50, Training Loss: 1.804598093032837, Validation Loss: 1.726624846458435\n",
      "Epoch 41/50, Training Loss: 1.7460377216339111, Validation Loss: 1.6695607900619507\n",
      "Epoch 42/50, Training Loss: 1.6906780004501343, Validation Loss: 1.619746208190918\n",
      "Epoch 43/50, Training Loss: 1.6427500247955322, Validation Loss: 1.5801388025283813\n",
      "Epoch 44/50, Training Loss: 1.6051607131958008, Validation Loss: 1.5522359609603882\n",
      "Epoch 45/50, Training Loss: 1.5793460607528687, Validation Loss: 1.5360726118087769\n",
      "Epoch 46/50, Training Loss: 1.5652720928192139, Validation Loss: 1.5303823947906494\n",
      "Epoch 47/50, Training Loss: 1.561599850654602, Validation Loss: 1.532901406288147\n",
      "Epoch 48/50, Training Loss: 1.565990686416626, Validation Loss: 1.5407623052597046\n",
      "Epoch 49/50, Training Loss: 1.5755094289779663, Validation Loss: 1.5509302616119385\n",
      "Epoch 50/50, Training Loss: 1.5870630741119385, Validation Loss: 1.560619592666626\n",
      "Batch Size: 32\n",
      "Epochs: 50\n",
      "MSE: 1.6217188835144043\n",
      "RMSE: 1.2734673023223877\n",
      "Negative: \n",
      "6575\n",
      "Positive: \n",
      "2966\n",
      "Precision: 0.6919931407466033\n",
      "Recall: 0.79787072243346\n",
      "Precision@k: 0.716\n",
      "Recall@k: 0.10889733840304182\n",
      "21\n",
      "Epoch 1/50, Training Loss: 15.731731414794922, Validation Loss: 14.828239440917969\n",
      "Epoch 2/50, Training Loss: 14.815084457397461, Validation Loss: 13.854432106018066\n",
      "Epoch 3/50, Training Loss: 13.839371681213379, Validation Loss: 12.827054023742676\n",
      "Epoch 4/50, Training Loss: 12.810219764709473, Validation Loss: 11.750654220581055\n",
      "Epoch 5/50, Training Loss: 11.732269287109375, Validation Loss: 10.630101203918457\n",
      "Epoch 6/50, Training Loss: 10.610494613647461, Validation Loss: 9.47439956665039\n",
      "Epoch 7/50, Training Loss: 9.454033851623535, Validation Loss: 8.298175811767578\n",
      "Epoch 8/50, Training Loss: 8.277688980102539, Validation Loss: 7.121814250946045\n",
      "Epoch 9/50, Training Loss: 7.102057456970215, Validation Loss: 5.970398902893066\n",
      "Epoch 10/50, Training Loss: 5.952525615692139, Validation Loss: 4.878378391265869\n",
      "Epoch 11/50, Training Loss: 4.86381196975708, Validation Loss: 3.887146234512329\n",
      "Epoch 12/50, Training Loss: 3.877626419067383, Validation Loss: 3.0421295166015625\n",
      "Epoch 13/50, Training Loss: 3.039757490158081, Validation Loss: 2.397031784057617\n",
      "Epoch 14/50, Training Loss: 2.4041316509246826, Validation Loss: 2.0001704692840576\n",
      "Epoch 15/50, Training Loss: 2.019174337387085, Validation Loss: 1.8816959857940674\n",
      "Epoch 16/50, Training Loss: 1.9147804975509644, Validation Loss: 2.0320885181427\n",
      "Epoch 17/50, Training Loss: 2.080503463745117, Validation Loss: 2.3795101642608643\n",
      "Epoch 18/50, Training Loss: 2.442734718322754, Validation Loss: 2.7852518558502197\n",
      "Epoch 19/50, Training Loss: 2.8605196475982666, Validation Loss: 3.089601755142212\n",
      "Epoch 20/50, Training Loss: 3.172624349594116, Validation Loss: 3.1920480728149414\n",
      "Epoch 21/50, Training Loss: 3.2777867317199707, Validation Loss: 3.1027119159698486\n",
      "Epoch 22/50, Training Loss: 3.186690092086792, Validation Loss: 2.8760154247283936\n",
      "Epoch 23/50, Training Loss: 2.9546868801116943, Validation Loss: 2.591352939605713\n",
      "Epoch 24/50, Training Loss: 2.6622426509857178, Validation Loss: 2.30789852142334\n",
      "Epoch 25/50, Training Loss: 2.369879961013794, Validation Loss: 2.0735833644866943\n",
      "Epoch 26/50, Training Loss: 2.126469373703003, Validation Loss: 1.911172866821289\n",
      "Epoch 27/50, Training Loss: 1.9554792642593384, Validation Loss: 1.8248100280761719\n",
      "Epoch 28/50, Training Loss: 1.861472725868225, Validation Loss: 1.8044230937957764\n",
      "Epoch 29/50, Training Loss: 1.8346055746078491, Validation Loss: 1.8314567804336548\n",
      "Epoch 30/50, Training Loss: 1.8563934564590454, Validation Loss: 1.8845956325531006\n",
      "Epoch 31/50, Training Loss: 1.905487060546875, Validation Loss: 1.9440199136734009\n",
      "Epoch 32/50, Training Loss: 1.9619749784469604, Validation Loss: 1.9930140972137451\n",
      "Epoch 33/50, Training Loss: 2.0090200901031494, Validation Loss: 2.0169832706451416\n",
      "Epoch 34/50, Training Loss: 2.031853199005127, Validation Loss: 2.0132083892822266\n",
      "Epoch 35/50, Training Loss: 2.0276942253112793, Validation Loss: 1.9762037992477417\n",
      "Epoch 36/50, Training Loss: 1.9912875890731812, Validation Loss: 1.9131572246551514\n",
      "Epoch 37/50, Training Loss: 1.929796814918518, Validation Loss: 1.8355157375335693\n",
      "Epoch 38/50, Training Loss: 1.8544647693634033, Validation Loss: 1.7590606212615967\n",
      "Epoch 39/50, Training Loss: 1.7807533740997314, Validation Loss: 1.6949788331985474\n",
      "Epoch 40/50, Training Loss: 1.7198913097381592, Validation Loss: 1.6527204513549805\n",
      "Epoch 41/50, Training Loss: 1.6811847686767578, Validation Loss: 1.63653564453125\n",
      "Epoch 42/50, Training Loss: 1.6686694622039795, Validation Loss: 1.6438153982162476\n",
      "Epoch 43/50, Training Loss: 1.679443597793579, Validation Loss: 1.6651501655578613\n",
      "Epoch 44/50, Training Loss: 1.7037538290023804, Validation Loss: 1.6872559785842896\n",
      "Epoch 45/50, Training Loss: 1.7279943227767944, Validation Loss: 1.6982141733169556\n",
      "Epoch 46/50, Training Loss: 1.7400343418121338, Validation Loss: 1.6921088695526123\n",
      "Epoch 47/50, Training Loss: 1.733917236328125, Validation Loss: 1.6703613996505737\n",
      "Epoch 48/50, Training Loss: 1.7111841440200806, Validation Loss: 1.6397054195404053\n",
      "Epoch 49/50, Training Loss: 1.678800106048584, Validation Loss: 1.6086455583572388\n",
      "Epoch 50/50, Training Loss: 1.6455503702163696, Validation Loss: 1.5842115879058838\n",
      "Batch Size: 32\n",
      "Epochs: 50\n",
      "MSE: 1.6397897005081177\n",
      "RMSE: 1.2805427312850952\n",
      "Negative: \n",
      "6575\n",
      "Positive: \n",
      "2966\n",
      "Precision: 0.6953366275478691\n",
      "Recall: 0.6848669201520913\n",
      "Precision@k: 0.705\n",
      "Recall@k: 0.10722433460076046\n",
      "22\n",
      "Epoch 1/50, Training Loss: 217.6787872314453, Validation Loss: 61.13047409057617\n",
      "Epoch 2/50, Training Loss: 923.7454223632812, Validation Loss: 12.207019805908203\n",
      "Epoch 3/50, Training Loss: 188.05470275878906, Validation Loss: 18.36582374572754\n",
      "Epoch 4/50, Training Loss: 88.36600494384766, Validation Loss: 53.259071350097656\n",
      "Epoch 5/50, Training Loss: 433.7685241699219, Validation Loss: 40.51499938964844\n",
      "Epoch 6/50, Training Loss: 307.03302001953125, Validation Loss: 11.857577323913574\n",
      "Epoch 7/50, Training Loss: 41.458736419677734, Validation Loss: 4.773069381713867\n",
      "Epoch 8/50, Training Loss: 46.764617919921875, Validation Loss: 13.692597389221191\n",
      "Epoch 9/50, Training Loss: 212.24749755859375, Validation Loss: 14.736333847045898\n",
      "Epoch 10/50, Training Loss: 228.36622619628906, Validation Loss: 6.725311756134033\n",
      "Epoch 11/50, Training Loss: 93.04290008544922, Validation Loss: 3.956134080886841\n",
      "Epoch 12/50, Training Loss: 4.611344337463379, Validation Loss: 11.94602108001709\n",
      "Epoch 13/50, Training Loss: 54.81187438964844, Validation Loss: 20.42384147644043\n",
      "Epoch 14/50, Training Loss: 136.1184539794922, Validation Loss: 18.81775665283203\n",
      "Epoch 15/50, Training Loss: 123.82289123535156, Validation Loss: 9.91106128692627\n",
      "Epoch 16/50, Training Loss: 44.29236602783203, Validation Loss: 3.4430387020111084\n",
      "Epoch 17/50, Training Loss: 3.111905813217163, Validation Loss: 3.6091151237487793\n",
      "Epoch 18/50, Training Loss: 37.268409729003906, Validation Loss: 5.982423305511475\n",
      "Epoch 19/50, Training Loss: 82.27091217041016, Validation Loss: 5.441555023193359\n",
      "Epoch 20/50, Training Loss: 72.96556854248047, Validation Loss: 2.960110902786255\n",
      "Epoch 21/50, Training Loss: 26.071779251098633, Validation Loss: 2.80826473236084\n",
      "Epoch 22/50, Training Loss: 2.5337188243865967, Validation Loss: 5.983292102813721\n",
      "Epoch 23/50, Training Loss: 23.700380325317383, Validation Loss: 8.768830299377441\n",
      "Epoch 24/50, Training Loss: 50.47008514404297, Validation Loss: 7.837453365325928\n",
      "Epoch 25/50, Training Loss: 43.558197021484375, Validation Loss: 4.417291641235352\n",
      "Epoch 26/50, Training Loss: 14.707371711730957, Validation Loss: 2.118665933609009\n",
      "Epoch 27/50, Training Loss: 2.245610475540161, Validation Loss: 2.2954156398773193\n",
      "Epoch 28/50, Training Loss: 16.827882766723633, Validation Loss: 3.0942609310150146\n",
      "Epoch 29/50, Training Loss: 32.01879119873047, Validation Loss: 2.7154619693756104\n",
      "Epoch 30/50, Training Loss: 25.343791961669922, Validation Loss: 1.8351852893829346\n",
      "Epoch 31/50, Training Loss: 7.472949981689453, Validation Loss: 2.103688955307007\n",
      "Epoch 32/50, Training Loss: 2.439517021179199, Validation Loss: 3.46100115776062\n",
      "Epoch 33/50, Training Loss: 13.172332763671875, Validation Loss: 4.183445453643799\n",
      "Epoch 34/50, Training Loss: 20.685190200805664, Validation Loss: 3.3598127365112305\n",
      "Epoch 35/50, Training Loss: 13.718344688415527, Validation Loss: 2.037381410598755\n",
      "Epoch 36/50, Training Loss: 3.1968348026275635, Validation Loss: 1.5870604515075684\n",
      "Epoch 37/50, Training Loss: 3.2946813106536865, Validation Loss: 1.930902361869812\n",
      "Epoch 38/50, Training Loss: 10.959107398986816, Validation Loss: 2.0577802658081055\n",
      "Epoch 39/50, Training Loss: 12.98182487487793, Validation Loss: 1.6955305337905884\n",
      "Epoch 40/50, Training Loss: 6.547954559326172, Validation Loss: 1.5438458919525146\n",
      "Epoch 41/50, Training Loss: 1.6100900173187256, Validation Loss: 1.9829305410385132\n",
      "Epoch 42/50, Training Loss: 4.381991386413574, Validation Loss: 2.4270317554473877\n",
      "Epoch 43/50, Training Loss: 8.772378921508789, Validation Loss: 2.2521238327026367\n",
      "Epoch 44/50, Training Loss: 7.355230331420898, Validation Loss: 1.7190417051315308\n",
      "Epoch 45/50, Training Loss: 2.6982929706573486, Validation Loss: 1.4889286756515503\n",
      "Epoch 46/50, Training Loss: 1.892670750617981, Validation Loss: 1.6398043632507324\n",
      "Epoch 47/50, Training Loss: 5.029294013977051, Validation Loss: 1.718323826789856\n",
      "Epoch 48/50, Training Loss: 6.186822891235352, Validation Loss: 1.5614235401153564\n",
      "Epoch 49/50, Training Loss: 3.5355992317199707, Validation Loss: 1.4892550706863403\n",
      "Epoch 50/50, Training Loss: 1.51753568649292, Validation Loss: 1.677245020866394\n",
      "Batch Size: 32\n",
      "Epochs: 50\n",
      "MSE: 1.7559279203414917\n",
      "RMSE: 1.3251142501831055\n",
      "Negative: \n",
      "6575\n",
      "Positive: \n",
      "2966\n",
      "Precision: 0.6917199715707179\n",
      "Recall: 0.5920912547528517\n",
      "Precision@k: 0.701\n",
      "Recall@k: 0.10661596958174904\n",
      "23\n",
      "Epoch 1/50, Training Loss: 32.026004791259766, Validation Loss: 26.133625030517578\n",
      "Epoch 2/50, Training Loss: 26.171354293823242, Validation Loss: 20.883359909057617\n",
      "Epoch 3/50, Training Loss: 20.901050567626953, Validation Loss: 16.242244720458984\n",
      "Epoch 4/50, Training Loss: 16.243135452270508, Validation Loss: 12.233141899108887\n",
      "Epoch 5/50, Training Loss: 12.220577239990234, Validation Loss: 8.872736930847168\n",
      "Epoch 6/50, Training Loss: 8.850113868713379, Validation Loss: 6.169631004333496\n",
      "Epoch 7/50, Training Loss: 6.140344142913818, Validation Loss: 4.121575355529785\n",
      "Epoch 8/50, Training Loss: 4.088949203491211, Validation Loss: 2.7114827632904053\n",
      "Epoch 9/50, Training Loss: 2.678670883178711, Validation Loss: 1.9018628597259521\n",
      "Epoch 10/50, Training Loss: 1.8717200756072998, Validation Loss: 1.6278060674667358\n",
      "Epoch 11/50, Training Loss: 1.6027134656906128, Validation Loss: 1.7901006937026978\n",
      "Epoch 12/50, Training Loss: 1.7717554569244385, Validation Loss: 2.2532758712768555\n",
      "Epoch 13/50, Training Loss: 2.242482900619507, Validation Loss: 2.8566973209381104\n",
      "Epoch 14/50, Training Loss: 2.853250026702881, Validation Loss: 3.442660093307495\n",
      "Epoch 15/50, Training Loss: 3.445413827896118, Validation Loss: 3.889852523803711\n",
      "Epoch 16/50, Training Loss: 3.897000789642334, Validation Loss: 4.132541179656982\n",
      "Epoch 17/50, Training Loss: 4.141986846923828, Validation Loss: 4.158911228179932\n",
      "Epoch 18/50, Training Loss: 4.168603420257568, Validation Loss: 3.996711254119873\n",
      "Epoch 19/50, Training Loss: 4.00487756729126, Validation Loss: 3.6960973739624023\n",
      "Epoch 20/50, Training Loss: 3.7013707160949707, Validation Loss: 3.315016746520996\n",
      "Epoch 21/50, Training Loss: 3.3164727687835693, Validation Loss: 2.9086966514587402\n",
      "Epoch 22/50, Training Loss: 2.905832052230835, Validation Loss: 2.523045063018799\n",
      "Epoch 23/50, Training Loss: 2.5157222747802734, Validation Loss: 2.1913082599639893\n",
      "Epoch 24/50, Training Loss: 2.179675817489624, Validation Loss: 1.9332290887832642\n",
      "Epoch 25/50, Training Loss: 1.9176379442214966, Validation Loss: 1.756024718284607\n",
      "Epoch 26/50, Training Loss: 1.7369533777236938, Validation Loss: 1.6565648317337036\n",
      "Epoch 27/50, Training Loss: 1.6345490217208862, Validation Loss: 1.6241772174835205\n",
      "Epoch 28/50, Training Loss: 1.5997563600540161, Validation Loss: 1.64359712600708\n",
      "Epoch 29/50, Training Loss: 1.6172786951065063, Validation Loss: 1.6976827383041382\n",
      "Epoch 30/50, Training Loss: 1.6699177026748657, Validation Loss: 1.7696447372436523\n",
      "Epoch 31/50, Training Loss: 1.7408183813095093, Validation Loss: 1.844682216644287\n",
      "Epoch 32/50, Training Loss: 1.8151127099990845, Validation Loss: 1.911008596420288\n",
      "Epoch 33/50, Training Loss: 1.8809536695480347, Validation Loss: 1.9603427648544312\n",
      "Epoch 34/50, Training Loss: 1.9300103187561035, Validation Loss: 1.9879587888717651\n",
      "Epoch 35/50, Training Loss: 1.9575183391571045, Validation Loss: 1.9924086332321167\n",
      "Epoch 36/50, Training Loss: 1.9620028734207153, Validation Loss: 1.9750219583511353\n",
      "Epoch 37/50, Training Loss: 1.944777250289917, Validation Loss: 1.9392709732055664\n",
      "Epoch 38/50, Training Loss: 1.9093042612075806, Validation Loss: 1.8900659084320068\n",
      "Epoch 39/50, Training Loss: 1.8604905605316162, Validation Loss: 1.8330539464950562\n",
      "Epoch 40/50, Training Loss: 1.8039805889129639, Validation Loss: 1.7739523649215698\n",
      "Epoch 41/50, Training Loss: 1.745487928390503, Validation Loss: 1.7179667949676514\n",
      "Epoch 42/50, Training Loss: 1.690212368965149, Validation Loss: 1.6693313121795654\n",
      "Epoch 43/50, Training Loss: 1.6423747539520264, Validation Loss: 1.630961298942566\n",
      "Epoch 44/50, Training Loss: 1.604873538017273, Validation Loss: 1.6043092012405396\n",
      "Epoch 45/50, Training Loss: 1.5791370868682861, Validation Loss: 1.5893644094467163\n",
      "Epoch 46/50, Training Loss: 1.5651253461837769, Validation Loss: 1.5848171710968018\n",
      "Epoch 47/50, Training Loss: 1.5614949464797974, Validation Loss: 1.5883632898330688\n",
      "Epoch 48/50, Training Loss: 1.5659072399139404, Validation Loss: 1.5971019268035889\n",
      "Epoch 49/50, Training Loss: 1.5754276514053345, Validation Loss: 1.6079732179641724\n",
      "Epoch 50/50, Training Loss: 1.5869672298431396, Validation Loss: 1.6181739568710327\n",
      "Batch Size: 32\n",
      "Epochs: 50\n",
      "MSE: 1.5733610391616821\n",
      "RMSE: 1.2543368339538574\n",
      "Negative: \n",
      "6660\n",
      "Positive: \n",
      "2881\n",
      "Precision: 0.7018025078369906\n",
      "Recall: 0.8067567567567567\n",
      "Precision@k: 0.723\n",
      "Recall@k: 0.10855855855855856\n",
      "24\n",
      "Epoch 1/50, Training Loss: 15.71254825592041, Validation Loss: 14.838321685791016\n",
      "Epoch 2/50, Training Loss: 14.797045707702637, Validation Loss: 13.86566162109375\n",
      "Epoch 3/50, Training Loss: 13.822558403015137, Validation Loss: 12.839638710021973\n",
      "Epoch 4/50, Training Loss: 12.794711112976074, Validation Loss: 11.764859199523926\n",
      "Epoch 5/50, Training Loss: 11.718130111694336, Validation Loss: 10.646267890930176\n",
      "Epoch 6/50, Training Loss: 10.59778881072998, Validation Loss: 9.492875099182129\n",
      "Epoch 7/50, Training Loss: 9.442813873291016, Validation Loss: 8.319378852844238\n",
      "Epoch 8/50, Training Loss: 8.267999649047852, Validation Loss: 7.1462202072143555\n",
      "Epoch 9/50, Training Loss: 7.093916893005371, Validation Loss: 5.99857234954834\n",
      "Epoch 10/50, Training Loss: 5.94597864151001, Validation Loss: 4.910861968994141\n",
      "Epoch 11/50, Training Loss: 4.858765602111816, Validation Loss: 3.9246327877044678\n",
      "Epoch 12/50, Training Loss: 3.874004364013672, Validation Loss: 3.0853707790374756\n",
      "Epoch 13/50, Training Loss: 3.037536859512329, Validation Loss: 2.4466097354888916\n",
      "Epoch 14/50, Training Loss: 2.4030065536499023, Validation Loss: 2.0568132400512695\n",
      "Epoch 15/50, Training Loss: 2.018977642059326, Validation Loss: 1.9456932544708252\n",
      "Epoch 16/50, Training Loss: 1.9151332378387451, Validation Loss: 2.1035268306732178\n",
      "Epoch 17/50, Training Loss: 2.0810091495513916, Validation Loss: 2.457181692123413\n",
      "Epoch 18/50, Training Loss: 2.44277024269104, Validation Loss: 2.8672401905059814\n",
      "Epoch 19/50, Training Loss: 2.8598456382751465, Validation Loss: 3.174180030822754\n",
      "Epoch 20/50, Training Loss: 3.171192169189453, Validation Loss: 3.2778263092041016\n",
      "Epoch 21/50, Training Loss: 3.276200532913208, Validation Loss: 3.187922239303589\n",
      "Epoch 22/50, Training Loss: 3.18497633934021, Validation Loss: 2.9592721462249756\n",
      "Epoch 23/50, Training Loss: 2.9531126022338867, Validation Loss: 2.671189546585083\n",
      "Epoch 24/50, Training Loss: 2.6609275341033936, Validation Loss: 2.3838050365448\n",
      "Epoch 25/50, Training Loss: 2.3688859939575195, Validation Loss: 2.145423650741577\n",
      "Epoch 26/50, Training Loss: 2.12575626373291, Validation Loss: 1.9790970087051392\n",
      "Epoch 27/50, Training Loss: 1.9549918174743652, Validation Loss: 1.889093279838562\n",
      "Epoch 28/50, Training Loss: 1.861140489578247, Validation Loss: 1.865433692932129\n",
      "Epoch 29/50, Training Loss: 1.8343465328216553, Validation Loss: 1.8896034955978394\n",
      "Epoch 30/50, Training Loss: 1.856130599975586, Validation Loss: 1.9402954578399658\n",
      "Epoch 31/50, Training Loss: 1.9051614999771118, Validation Loss: 1.9976778030395508\n",
      "Epoch 32/50, Training Loss: 1.9615501165390015, Validation Loss: 2.045071601867676\n",
      "Epoch 33/50, Training Loss: 2.0085039138793945, Validation Loss: 2.067932605743408\n",
      "Epoch 34/50, Training Loss: 2.031363010406494, Validation Loss: 2.0634331703186035\n",
      "Epoch 35/50, Training Loss: 2.027306079864502, Validation Loss: 2.026240348815918\n",
      "Epoch 36/50, Training Loss: 1.9908974170684814, Validation Loss: 1.9635474681854248\n",
      "Epoch 37/50, Training Loss: 1.9293534755706787, Validation Loss: 1.8868341445922852\n",
      "Epoch 38/50, Training Loss: 1.8539983034133911, Validation Loss: 1.8117306232452393\n",
      "Epoch 39/50, Training Loss: 1.7803276777267456, Validation Loss: 1.7492341995239258\n",
      "Epoch 40/50, Training Loss: 1.7195078134536743, Validation Loss: 1.708704948425293\n",
      "Epoch 41/50, Training Loss: 1.6808379888534546, Validation Loss: 1.6942708492279053\n",
      "Epoch 42/50, Training Loss: 1.6683473587036133, Validation Loss: 1.703168272972107\n",
      "Epoch 43/50, Training Loss: 1.6791267395019531, Validation Loss: 1.7258132696151733\n",
      "Epoch 44/50, Training Loss: 1.7034138441085815, Validation Loss: 1.7487740516662598\n",
      "Epoch 45/50, Training Loss: 1.727603554725647, Validation Loss: 1.7600511312484741\n",
      "Epoch 46/50, Training Loss: 1.7395787239074707, Validation Loss: 1.753736972808838\n",
      "Epoch 47/50, Training Loss: 1.7334040403366089, Validation Loss: 1.7313270568847656\n",
      "Epoch 48/50, Training Loss: 1.7106375694274902, Validation Loss: 1.699670672416687\n",
      "Epoch 49/50, Training Loss: 1.6782501935958862, Validation Loss: 1.667395830154419\n",
      "Epoch 50/50, Training Loss: 1.6450201272964478, Validation Loss: 1.6416521072387695\n",
      "Batch Size: 32\n",
      "Epochs: 50\n",
      "MSE: 1.5927600860595703\n",
      "RMSE: 1.262045979499817\n",
      "Negative: \n",
      "6660\n",
      "Positive: \n",
      "2881\n",
      "Precision: 0.7040242976461655\n",
      "Recall: 0.6960960960960961\n",
      "Precision@k: 0.72\n",
      "Recall@k: 0.10810810810810811\n",
      "25\n",
      "Epoch 1/50, Training Loss: 219.2690887451172, Validation Loss: 49.38322067260742\n",
      "Epoch 2/50, Training Loss: 933.1678466796875, Validation Loss: 9.914817810058594\n",
      "Epoch 3/50, Training Loss: 189.54449462890625, Validation Loss: 17.340023040771484\n",
      "Epoch 4/50, Training Loss: 89.51171112060547, Validation Loss: 47.70814895629883\n",
      "Epoch 5/50, Training Loss: 438.3216552734375, Validation Loss: 36.58945846557617\n",
      "Epoch 6/50, Training Loss: 309.99188232421875, Validation Loss: 11.40618896484375\n",
      "Epoch 7/50, Training Loss: 41.76807403564453, Validation Loss: 4.291897773742676\n",
      "Epoch 8/50, Training Loss: 47.26690673828125, Validation Loss: 11.08137321472168\n",
      "Epoch 9/50, Training Loss: 214.40621948242188, Validation Loss: 11.910433769226074\n",
      "Epoch 10/50, Training Loss: 230.6734619140625, Validation Loss: 5.63850736618042\n",
      "Epoch 11/50, Training Loss: 93.98273468017578, Validation Loss: 3.9949162006378174\n",
      "Epoch 12/50, Training Loss: 4.634777069091797, Validation Loss: 11.315653800964355\n",
      "Epoch 13/50, Training Loss: 55.30003356933594, Validation Loss: 18.73454475402832\n",
      "Epoch 14/50, Training Loss: 137.4427490234375, Validation Loss: 17.295095443725586\n",
      "Epoch 15/50, Training Loss: 125.11324310302734, Validation Loss: 9.427118301391602\n",
      "Epoch 16/50, Training Loss: 44.79823684692383, Validation Loss: 3.49289870262146\n",
      "Epoch 17/50, Training Loss: 3.120638608932495, Validation Loss: 3.2121200561523438\n",
      "Epoch 18/50, Training Loss: 37.54391860961914, Validation Loss: 5.0011887550354\n",
      "Epoch 19/50, Training Loss: 83.05006408691406, Validation Loss: 4.582150459289551\n",
      "Epoch 20/50, Training Loss: 73.7560806274414, Validation Loss: 2.704962968826294\n",
      "Epoch 21/50, Training Loss: 26.405574798583984, Validation Loss: 2.849407911300659\n",
      "Epoch 22/50, Training Loss: 2.5389766693115234, Validation Loss: 5.745097637176514\n",
      "Epoch 23/50, Training Loss: 23.84650993347168, Validation Loss: 8.188559532165527\n",
      "Epoch 24/50, Training Loss: 50.926422119140625, Validation Loss: 7.355555057525635\n",
      "Epoch 25/50, Training Loss: 44.04567337036133, Validation Loss: 4.310328483581543\n",
      "Epoch 26/50, Training Loss: 14.914780616760254, Validation Loss: 2.164442300796509\n",
      "Epoch 27/50, Training Loss: 2.2435247898101807, Validation Loss: 2.1429359912872314\n",
      "Epoch 28/50, Training Loss: 16.911230087280273, Validation Loss: 2.74306321144104\n",
      "Epoch 29/50, Training Loss: 32.30844497680664, Validation Loss: 2.4536662101745605\n",
      "Epoch 30/50, Training Loss: 25.638111114501953, Validation Loss: 1.8060203790664673\n",
      "Epoch 31/50, Training Loss: 7.590147018432617, Validation Loss: 2.138394594192505\n",
      "Epoch 32/50, Training Loss: 2.430194616317749, Validation Loss: 3.3586795330047607\n",
      "Epoch 33/50, Training Loss: 13.236166000366211, Validation Loss: 3.9905307292938232\n",
      "Epoch 34/50, Training Loss: 20.87537956237793, Validation Loss: 3.261267900466919\n",
      "Epoch 35/50, Training Loss: 13.896283149719238, Validation Loss: 2.0709433555603027\n",
      "Epoch 36/50, Training Loss: 3.24589467048645, Validation Loss: 1.6102714538574219\n",
      "Epoch 37/50, Training Loss: 3.2843236923217773, Validation Loss: 1.8496173620224\n",
      "Epoch 38/50, Training Loss: 11.017989158630371, Validation Loss: 1.9517548084259033\n",
      "Epoch 39/50, Training Loss: 13.112252235412598, Validation Loss: 1.6761698722839355\n",
      "Epoch 40/50, Training Loss: 6.641498565673828, Validation Loss: 1.5902386903762817\n",
      "Epoch 41/50, Training Loss: 1.61948561668396, Validation Loss: 1.995635986328125\n",
      "Epoch 42/50, Training Loss: 4.3813700675964355, Validation Loss: 2.387387990951538\n",
      "Epoch 43/50, Training Loss: 8.83609390258789, Validation Loss: 2.234036684036255\n",
      "Epoch 44/50, Training Loss: 7.4388508796691895, Validation Loss: 1.7589133977890015\n",
      "Epoch 45/50, Training Loss: 2.734448194503784, Validation Loss: 1.5330365896224976\n",
      "Epoch 46/50, Training Loss: 1.8855034112930298, Validation Loss: 1.6392713785171509\n",
      "Epoch 47/50, Training Loss: 5.042657375335693, Validation Loss: 1.7036163806915283\n",
      "Epoch 48/50, Training Loss: 6.240431308746338, Validation Loss: 1.5838019847869873\n",
      "Epoch 49/50, Training Loss: 3.5799076557159424, Validation Loss: 1.5398173332214355\n",
      "Epoch 50/50, Training Loss: 1.5211246013641357, Validation Loss: 1.7123949527740479\n",
      "Batch Size: 32\n",
      "Epochs: 50\n",
      "MSE: 1.738251805305481\n",
      "RMSE: 1.3184278011322021\n",
      "Negative: \n",
      "6660\n",
      "Positive: \n",
      "2881\n",
      "Precision: 0.6973891711932714\n",
      "Recall: 0.5975975975975976\n",
      "Precision@k: 0.709\n",
      "Recall@k: 0.10645645645645646\n",
      "26\n",
      "Epoch 1/50, Training Loss: 32.04418182373047, Validation Loss: 26.242650985717773\n",
      "Epoch 2/50, Training Loss: 26.190582275390625, Validation Loss: 20.9504337310791\n",
      "Epoch 3/50, Training Loss: 20.920690536499023, Validation Loss: 16.27358627319336\n",
      "Epoch 4/50, Training Loss: 16.262487411499023, Validation Loss: 12.235147476196289\n",
      "Epoch 5/50, Training Loss: 12.238930702209473, Validation Loss: 8.851921081542969\n",
      "Epoch 6/50, Training Loss: 8.86676025390625, Validation Loss: 6.132536888122559\n",
      "Epoch 7/50, Training Loss: 6.154603958129883, Validation Loss: 4.074668884277344\n",
      "Epoch 8/50, Training Loss: 4.100220680236816, Validation Loss: 2.661001682281494\n",
      "Epoch 9/50, Training Loss: 2.6864845752716064, Validation Loss: 1.8536158800125122\n",
      "Epoch 10/50, Training Loss: 1.8758150339126587, Validation Loss: 1.5869063138961792\n",
      "Epoch 11/50, Training Loss: 1.6031430959701538, Validation Loss: 1.7606233358383179\n",
      "Epoch 12/50, Training Loss: 1.7689944505691528, Validation Loss: 2.237858533859253\n",
      "Epoch 13/50, Training Loss: 2.2374744415283203, Validation Loss: 2.856173515319824\n",
      "Epoch 14/50, Training Loss: 2.847294330596924, Validation Loss: 3.455904245376587\n",
      "Epoch 15/50, Training Loss: 3.4398586750030518, Validation Loss: 3.914031505584717\n",
      "Epoch 16/50, Training Loss: 3.8929028511047363, Validation Loss: 4.163719654083252\n",
      "Epoch 17/50, Training Loss: 4.139922142028809, Validation Loss: 4.192775726318359\n",
      "Epoch 18/50, Training Loss: 4.168671607971191, Validation Loss: 4.0291829109191895\n",
      "Epoch 19/50, Training Loss: 4.006815433502197, Validation Loss: 3.7237424850463867\n",
      "Epoch 20/50, Training Loss: 3.7046902179718018, Validation Loss: 3.3352651596069336\n",
      "Epoch 21/50, Training Loss: 3.320599317550659, Validation Loss: 2.919888973236084\n",
      "Epoch 22/50, Training Loss: 2.91019868850708, Validation Loss: 2.5243852138519287\n",
      "Epoch 23/50, Training Loss: 2.51983904838562, Validation Loss: 2.1827356815338135\n",
      "Epoch 24/50, Training Loss: 2.183173656463623, Validation Loss: 1.9152592420578003\n",
      "Epoch 25/50, Training Loss: 1.9202853441238403, Validation Loss: 1.7295793294906616\n",
      "Epoch 26/50, Training Loss: 1.7386513948440552, Validation Loss: 1.6228070259094238\n",
      "Epoch 27/50, Training Loss: 1.6353144645690918, Validation Loss: 1.584373950958252\n",
      "Epoch 28/50, Training Loss: 1.5996980667114258, Validation Loss: 1.5990067720413208\n",
      "Epoch 29/50, Training Loss: 1.6165647506713867, Validation Loss: 1.6494758129119873\n",
      "Epoch 30/50, Training Loss: 1.6687465906143188, Validation Loss: 1.718855857849121\n",
      "Epoch 31/50, Training Loss: 1.7393922805786133, Validation Loss: 1.7921870946884155\n",
      "Epoch 32/50, Training Loss: 1.81361722946167, Validation Loss: 1.8575232028961182\n",
      "Epoch 33/50, Training Loss: 1.8795442581176758, Validation Loss: 1.9064353704452515\n",
      "Epoch 34/50, Training Loss: 1.9288002252578735, Validation Loss: 1.93407142162323\n",
      "Epoch 35/50, Training Loss: 1.9565775394439697, Validation Loss: 1.9388830661773682\n",
      "Epoch 36/50, Training Loss: 1.9613583087921143, Validation Loss: 1.9221243858337402\n",
      "Epoch 37/50, Training Loss: 1.9444162845611572, Validation Loss: 1.887214183807373\n",
      "Epoch 38/50, Training Loss: 1.9091811180114746, Validation Loss: 1.8390295505523682\n",
      "Epoch 39/50, Training Loss: 1.860535979270935, Validation Loss: 1.7831979990005493\n",
      "Epoch 40/50, Training Loss: 1.804111361503601, Validation Loss: 1.7254235744476318\n",
      "Epoch 41/50, Training Loss: 1.7456163167953491, Validation Loss: 1.670902132987976\n",
      "Epoch 42/50, Training Loss: 1.690255880355835, Validation Loss: 1.623854160308838\n",
      "Epoch 43/50, Training Loss: 1.6422656774520874, Validation Loss: 1.5871742963790894\n",
      "Epoch 44/50, Training Loss: 1.6045609712600708, Validation Loss: 1.5622872114181519\n",
      "Epoch 45/50, Training Loss: 1.5785945653915405, Validation Loss: 1.5491411685943604\n",
      "Epoch 46/50, Training Loss: 1.5643500089645386, Validation Loss: 1.5463770627975464\n",
      "Epoch 47/50, Training Loss: 1.5605072975158691, Validation Loss: 1.5516325235366821\n",
      "Epoch 48/50, Training Loss: 1.564743995666504, Validation Loss: 1.5619451999664307\n",
      "Epoch 49/50, Training Loss: 1.574137806892395, Validation Loss: 1.5741950273513794\n",
      "Epoch 50/50, Training Loss: 1.5856022834777832, Validation Loss: 1.5855257511138916\n",
      "Batch Size: 32\n",
      "Epochs: 50\n",
      "MSE: 1.6087545156478882\n",
      "RMSE: 1.268366813659668\n",
      "Negative: \n",
      "6611\n",
      "Positive: \n",
      "2930\n",
      "Precision: 0.6984924623115578\n",
      "Recall: 0.7989714112842232\n",
      "Precision@k: 0.715\n",
      "Recall@k: 0.10815307820299501\n",
      "27\n",
      "Epoch 1/50, Training Loss: 15.741642951965332, Validation Loss: 14.791930198669434\n",
      "Epoch 2/50, Training Loss: 14.825657844543457, Validation Loss: 13.814723014831543\n",
      "Epoch 3/50, Training Loss: 13.850614547729492, Validation Loss: 12.784067153930664\n",
      "Epoch 4/50, Training Loss: 12.82211971282959, Validation Loss: 11.70465087890625\n",
      "Epoch 5/50, Training Loss: 11.744783401489258, Validation Loss: 10.581499099731445\n",
      "Epoch 6/50, Training Loss: 10.62354564666748, Validation Loss: 9.423778533935547\n",
      "Epoch 7/50, Training Loss: 9.467510223388672, Validation Loss: 8.246362686157227\n",
      "Epoch 8/50, Training Loss: 8.291435241699219, Validation Loss: 7.069928169250488\n",
      "Epoch 9/50, Training Loss: 7.115854263305664, Validation Loss: 5.919996738433838\n",
      "Epoch 10/50, Training Loss: 5.966042995452881, Validation Loss: 4.831394672393799\n",
      "Epoch 11/50, Training Loss: 4.876662731170654, Validation Loss: 3.8459742069244385\n",
      "Epoch 12/50, Training Loss: 3.889338970184326, Validation Loss: 3.0096237659454346\n",
      "Epoch 13/50, Training Loss: 3.0498015880584717, Validation Loss: 2.3764374256134033\n",
      "Epoch 14/50, Training Loss: 2.411921977996826, Validation Loss: 1.995055079460144\n",
      "Epoch 15/50, Training Loss: 2.0242013931274414, Validation Loss: 1.895510196685791\n",
      "Epoch 16/50, Training Loss: 1.916864275932312, Validation Loss: 2.067331075668335\n",
      "Epoch 17/50, Training Loss: 2.080068826675415, Validation Loss: 2.4370415210723877\n",
      "Epoch 18/50, Training Loss: 2.4409375190734863, Validation Loss: 2.86299467086792\n",
      "Epoch 19/50, Training Loss: 2.8589155673980713, Validation Loss: 3.1825079917907715\n",
      "Epoch 20/50, Training Loss: 3.173302412033081, Validation Loss: 3.2908618450164795\n",
      "Epoch 21/50, Training Loss: 3.2800636291503906, Validation Loss: 3.200326681137085\n",
      "Epoch 22/50, Training Loss: 3.1909067630767822, Validation Loss: 2.965433359146118\n",
      "Epoch 23/50, Training Loss: 2.9595112800598145, Validation Loss: 2.6689560413360596\n",
      "Epoch 24/50, Training Loss: 2.6672189235687256, Validation Loss: 2.3713245391845703\n",
      "Epoch 25/50, Training Loss: 2.374406576156616, Validation Loss: 2.1220784187316895\n",
      "Epoch 26/50, Training Loss: 2.1301329135894775, Validation Loss: 1.9455281496047974\n",
      "Epoch 27/50, Training Loss: 1.958267092704773, Validation Loss: 1.8465654850006104\n",
      "Epoch 28/50, Training Loss: 1.8634060621261597, Validation Loss: 1.815596103668213\n",
      "Epoch 29/50, Training Loss: 1.8358300924301147, Validation Loss: 1.8342249393463135\n",
      "Epoch 30/50, Training Loss: 1.8571171760559082, Validation Loss: 1.881077766418457\n",
      "Epoch 31/50, Training Loss: 1.9059300422668457, Validation Loss: 1.936147928237915\n",
      "Epoch 32/50, Training Loss: 1.962332844734192, Validation Loss: 1.9825327396392822\n",
      "Epoch 33/50, Training Loss: 2.0095436573028564, Validation Loss: 2.0051350593566895\n",
      "Epoch 34/50, Training Loss: 2.032684564590454, Validation Loss: 2.001152515411377\n",
      "Epoch 35/50, Training Loss: 2.0289106369018555, Validation Loss: 1.965802550315857\n",
      "Epoch 36/50, Training Loss: 1.9929267168045044, Validation Loss: 1.9056264162063599\n",
      "Epoch 37/50, Training Loss: 1.9315922260284424, Validation Loss: 1.8316713571548462\n",
      "Epoch 38/50, Training Loss: 1.8561240434646606, Validation Loss: 1.7594313621520996\n",
      "Epoch 39/50, Training Loss: 1.7822092771530151, Validation Loss: 1.7001155614852905\n",
      "Epoch 40/50, Training Loss: 1.7209312915802002, Validation Loss: 1.6630213260650635\n",
      "Epoch 41/50, Training Loss: 1.6816564798355103, Validation Loss: 1.652174711227417\n",
      "Epoch 42/50, Training Loss: 1.6685267686843872, Validation Loss: 1.6646449565887451\n",
      "Epoch 43/50, Training Loss: 1.6787807941436768, Validation Loss: 1.6906046867370605\n",
      "Epoch 44/50, Training Loss: 1.7028000354766846, Validation Loss: 1.7163045406341553\n",
      "Epoch 45/50, Training Loss: 1.727039098739624, Validation Loss: 1.7294204235076904\n",
      "Epoch 46/50, Training Loss: 1.7393150329589844, Validation Loss: 1.7238057851791382\n",
      "Epoch 47/50, Training Loss: 1.7335180044174194, Validation Loss: 1.7008999586105347\n",
      "Epoch 48/50, Training Loss: 1.7110188007354736, Validation Loss: 1.6676959991455078\n",
      "Epoch 49/50, Training Loss: 1.6786693334579468, Validation Loss: 1.6331266164779663\n",
      "Epoch 50/50, Training Loss: 1.6452279090881348, Validation Loss: 1.6047290563583374\n",
      "Batch Size: 32\n",
      "Epochs: 50\n",
      "MSE: 1.62698495388031\n",
      "RMSE: 1.2755331993103027\n",
      "Negative: \n",
      "6611\n",
      "Positive: \n",
      "2930\n",
      "Precision: 0.70231124807396\n",
      "Recall: 0.6894569656632885\n",
      "Precision@k: 0.707\n",
      "Recall@k: 0.10694297383149297\n",
      "28\n",
      "Epoch 1/50, Training Loss: 214.39227294921875, Validation Loss: 54.187442779541016\n",
      "Epoch 2/50, Training Loss: 907.1912841796875, Validation Loss: 10.973729133605957\n",
      "Epoch 3/50, Training Loss: 185.14877319335938, Validation Loss: 17.4897518157959\n",
      "Epoch 4/50, Training Loss: 86.63082122802734, Validation Loss: 49.37508010864258\n",
      "Epoch 5/50, Training Loss: 425.9352111816406, Validation Loss: 37.74955368041992\n",
      "Epoch 6/50, Training Loss: 301.7494812011719, Validation Loss: 11.451093673706055\n",
      "Epoch 7/50, Training Loss: 40.87384033203125, Validation Loss: 4.527492523193359\n",
      "Epoch 8/50, Training Loss: 45.956993103027344, Validation Loss: 12.227386474609375\n",
      "Epoch 9/50, Training Loss: 208.46824645996094, Validation Loss: 13.145880699157715\n",
      "Epoch 10/50, Training Loss: 224.3557586669922, Validation Loss: 6.141434192657471\n",
      "Epoch 11/50, Training Loss: 91.44788360595703, Validation Loss: 3.9510953426361084\n",
      "Epoch 12/50, Training Loss: 4.600107669830322, Validation Loss: 11.404291152954102\n",
      "Epoch 13/50, Training Loss: 53.93302536010742, Validation Loss: 19.149600982666016\n",
      "Epoch 14/50, Training Loss: 133.77386474609375, Validation Loss: 17.64767837524414\n",
      "Epoch 15/50, Training Loss: 121.63726043701172, Validation Loss: 9.457117080688477\n",
      "Epoch 16/50, Training Loss: 43.50274658203125, Validation Loss: 3.425894021987915\n",
      "Epoch 17/50, Training Loss: 3.1167304515838623, Validation Loss: 3.4033870697021484\n",
      "Epoch 18/50, Training Loss: 36.72178268432617, Validation Loss: 5.4513630867004395\n",
      "Epoch 19/50, Training Loss: 80.88863372802734, Validation Loss: 4.971551418304443\n",
      "Epoch 20/50, Training Loss: 71.67337799072266, Validation Loss: 2.8201000690460205\n",
      "Epoch 21/50, Training Loss: 25.588966369628906, Validation Loss: 2.7955355644226074\n",
      "Epoch 22/50, Training Loss: 2.5396745204925537, Validation Loss: 5.741395473480225\n",
      "Epoch 23/50, Training Loss: 23.392011642456055, Validation Loss: 8.274598121643066\n",
      "Epoch 24/50, Training Loss: 49.657188415527344, Validation Loss: 7.399627208709717\n",
      "Epoch 25/50, Training Loss: 42.77782440185547, Validation Loss: 4.250734329223633\n",
      "Epoch 26/50, Training Loss: 14.425370216369629, Validation Loss: 2.1098055839538574\n",
      "Epoch 27/50, Training Loss: 2.254319190979004, Validation Loss: 2.2157838344573975\n",
      "Epoch 28/50, Training Loss: 16.62519645690918, Validation Loss: 2.90946102142334\n",
      "Epoch 29/50, Training Loss: 31.510711669921875, Validation Loss: 2.5732696056365967\n",
      "Epoch 30/50, Training Loss: 24.88541603088379, Validation Loss: 1.809985637664795\n",
      "Epoch 31/50, Training Loss: 7.330134391784668, Validation Loss: 2.087373971939087\n",
      "Epoch 32/50, Training Loss: 2.4470608234405518, Validation Loss: 3.330294609069824\n",
      "Epoch 33/50, Training Loss: 13.022088050842285, Validation Loss: 3.9775311946868896\n",
      "Epoch 34/50, Training Loss: 20.35855484008789, Validation Loss: 3.2173454761505127\n",
      "Epoch 35/50, Training Loss: 13.468680381774902, Validation Loss: 2.0071704387664795\n",
      "Epoch 36/50, Training Loss: 3.1463935375213623, Validation Loss: 1.5888702869415283\n",
      "Epoch 37/50, Training Loss: 3.2941911220550537, Validation Loss: 1.8915598392486572\n",
      "Epoch 38/50, Training Loss: 10.832294464111328, Validation Loss: 2.0036520957946777\n",
      "Epoch 39/50, Training Loss: 12.774492263793945, Validation Loss: 1.681261658668518\n",
      "Epoch 40/50, Training Loss: 6.429067611694336, Validation Loss: 1.5525004863739014\n",
      "Epoch 41/50, Training Loss: 1.6056333780288696, Validation Loss: 1.9528285264968872\n",
      "Epoch 42/50, Training Loss: 4.3642778396606445, Validation Loss: 2.350534439086914\n",
      "Epoch 43/50, Training Loss: 8.665733337402344, Validation Loss: 2.187654972076416\n",
      "Epoch 44/50, Training Loss: 7.2306599617004395, Validation Loss: 1.706485629081726\n",
      "Epoch 45/50, Training Loss: 2.6581919193267822, Validation Loss: 1.5030670166015625\n",
      "Epoch 46/50, Training Loss: 1.9000060558319092, Validation Loss: 1.6425676345825195\n",
      "Epoch 47/50, Training Loss: 4.987760066986084, Validation Loss: 1.7137218713760376\n",
      "Epoch 48/50, Training Loss: 6.099143981933594, Validation Loss: 1.5711557865142822\n",
      "Epoch 49/50, Training Loss: 3.481173038482666, Validation Loss: 1.5048205852508545\n",
      "Epoch 50/50, Training Loss: 1.5167874097824097, Validation Loss: 1.6717557907104492\n",
      "Batch Size: 32\n",
      "Epochs: 50\n",
      "MSE: 1.7825161218643188\n",
      "RMSE: 1.335108995437622\n",
      "Negative: \n",
      "6611\n",
      "Positive: \n",
      "2930\n",
      "Precision: 0.6938341601700921\n",
      "Recall: 0.5923460898502496\n",
      "Precision@k: 0.706\n",
      "Recall@k: 0.10679171078505521\n",
      "29\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "indices = list(range(data.edge_index.size(1)))\n",
    "import csv\n",
    "csv_filename = \"results.csv\"\n",
    "with open(csv_filename, mode='a', newline='') as csv_file:\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "    csv_writer.writerow([\"i\", \"Model\", \"Precision\", \"Recall\", \"Precision@k\", \"Recall@k\", \"MSE\"])  # Write header\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in range(30):\n",
    "    #das hier klein, damit der Speicher nicht Ã¼berdreht wird. Aber nicht zu klein, weil sonst kommt es zu problemen!\n",
    "    if (i % 3 == 0):\n",
    "        train_indices, test_indices = train_test_split(indices, train_size=0.8, test_size=0.2)\n",
    "        train_indices, val_indices = train_test_split(train_indices, train_size=0.8, test_size=0.2, random_state=42)\n",
    "        np.savez('indices.npz', train_indices=train_indices, test_indices=test_indices, val_indices=val_indices)\n",
    "    # Now, you can comment out the above code that generates the indices\n",
    "    # Read the indices from the file\n",
    "    loaded_indices = np.load('indices.npz')\n",
    "    #irgendeine syntax\n",
    "    train_data = data.__class__()\n",
    "    test_data = data.__class__()\n",
    "    val_data = data.__class__()\n",
    "\n",
    "\n",
    "    #setzt die Parameter von train_data und test_data\n",
    "\n",
    "    #soweit ich es verstehe, sind alle 2.500 nodes im training und testset vorhanden. gesplittet werden nur die edges, d.h. \n",
    "    #es ist nur ein subset der 100.000 edges im training set sowie im test set vorhanden\n",
    "    # also 10% der Bewertungen \n",
    "    train_data.edge_index = data.edge_index[:, train_indices]\n",
    "    train_data.y = data.y[train_indices]\n",
    "    train_data.num_nodes = data.num_nodes\n",
    "    train_data.positional_encodings = data.positional_encodings\n",
    "    train_data.x = data.x\n",
    "\n",
    "\n",
    "    test_data.edge_index = data.edge_index[:, test_indices]\n",
    "    test_data.y = data.y[test_indices]\n",
    "    test_data.num_nodes = data.num_nodes\n",
    "    test_data.positional_encodings = data.positional_encodings\n",
    "    test_data.x = data.x\n",
    "\n",
    "\n",
    "    val_data.edge_index = data.edge_index[:, val_indices]\n",
    "    val_data.y = data.y[val_indices]\n",
    "    val_data.num_nodes = data.num_nodes\n",
    "    val_data.positional_encodings = data.positional_encodings\n",
    "    val_data.x = data.x\n",
    "        \n",
    "\n",
    "\n",
    "    # Step 6: Train and evaluate the GCN model\n",
    "    # Set seed for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    # Set the device --> aktiviere GPU falls vorhanden\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    #------------------------------------------------------\n",
    "\n",
    "    #hidden channels und epochs tunen\n",
    "    hidden_channels=8 #8 und 16\n",
    "    lr = 0.01  #0.01 vs 0.001 \n",
    "    epochs = 50  #100 vs 200\n",
    "    batch_size = 128#512\n",
    "\n",
    "    #1, 16, 32 ,64, 128, 256, 512\n",
    "\n",
    "    #Early Stopping\n",
    "    patience = 40  # Number of epochs to wait for improvement\n",
    "    min_delta = 0.001  # Minimum improvement required to consider as improvement\n",
    "\n",
    "    best_val_loss = np.inf\n",
    "    best_epoch = 0\n",
    "    early_stop_counter = 0\n",
    "\n",
    "    # this is for evaluation!\n",
    "    if (i % 3 == 0):\n",
    "        model = GINModel_nopos(hidden_channels=hidden_channels)\n",
    "    elif (i % 3 == 1):\n",
    "        model = GINModel_var1(hidden_channels=hidden_channels)\n",
    "    elif (i%3 == 2):\n",
    "        model = GINModel_var2(hidden_channels=hidden_channels)\n",
    "    #------------------------------------------------------\n",
    "    #loss function, and optimizer, MSE = Metrik fÃ¼r Loss \n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Create data loaders for training and test sets\n",
    "    train_loader = DataLoader([train_data], batch_size=batch_size)\n",
    "    test_loader = DataLoader([test_data], batch_size=batch_size)\n",
    "    val_loader = DataLoader([val_data], batch_size=batch_size)\n",
    "\n",
    "    # Model training\n",
    "    model.train()\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    predictions =[]\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(device)\n",
    "            out = model( batch.x.unsqueeze(1), batch.edge_index, batch.positional_encodings.unsqueeze(1))\n",
    "            task_loss = criterion(out, batch.y)\n",
    "            loss = task_loss\n",
    "            #loss = calculateLoss(task_loss, batch, num_nodes, batch.positional_encodings)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * batch.num_graphs\n",
    "            predictions = out.detach().cpu().numpy()\n",
    "            #print(predictions)\n",
    "\n",
    "        # Calculate average training loss\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "    # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        for batch in val_loader:\n",
    "            batch = batch.to(device)\n",
    "            out= model(batch.x.unsqueeze(1),batch.edge_index, batch.positional_encodings.unsqueeze(1))\n",
    "            task_loss = criterion(out, batch.y)\n",
    "            loss = task_loss\n",
    "        # loss = calculateLoss(task_loss, batch, num_nodes, batch.positional_encodings)\n",
    "            \n",
    "            val_loss += loss.item() * batch.num_graphs\n",
    "\n",
    "        # Calculate average validation loss\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        # Print training and validation loss for monitoring\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {train_loss}, Validation Loss: {val_loss}\")\n",
    "\n",
    "        # Check for early stopping\n",
    "        if val_loss < best_val_loss - min_delta:\n",
    "            best_val_loss = val_loss\n",
    "            best_epoch = epoch\n",
    "            early_stop_counter = 0\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "            if early_stop_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "        # Set the model back to training mode\n",
    "        model.train()\n",
    "        '''\n",
    "        # Plotting training and validation curves\n",
    "        plt.plot(train_losses, label='Training Loss')\n",
    "        #plt.plot(val_losses, label='Validation Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "\n",
    "        # Save the plot as an image file\n",
    "        plt.savefig('loss_plot.png')\n",
    "        '''\n",
    "\n",
    "        # Show the plot\n",
    "\n",
    "    # Evaluation on test set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = []\n",
    "        targets = []\n",
    "\n",
    "        for batch in test_loader:\n",
    "            batch = batch.to(device)\n",
    "            out = model(batch.x.unsqueeze(1), batch.edge_index, batch.positional_encodings.unsqueeze(1))\n",
    "            task_loss = criterion(out, batch.y)\n",
    "            test_loss = task_loss\n",
    "            #test_loss = calculateLoss(task_loss, batch, num_nodes, batch.positional_encodings)\n",
    "            \n",
    "            #print(f'Test Loss: {test_loss.item()}')\n",
    "            predictions.extend(out.cpu().numpy().flatten())\n",
    "            targets.extend(batch.y.cpu().numpy().flatten())\n",
    "\n",
    "        predictions = np.array(predictions)\n",
    "        targets = np.array(targets)\n",
    "\n",
    "        rounded_predictions = np.round(predictions, decimals=0)  # Round the predicted ratings\n",
    "\n",
    "        # Plotting the distribution\n",
    "        plt.hist(rounded_predictions, bins=15, edgecolor='black')\n",
    "        plt.xlabel('Predicted Rating')\n",
    "        plt.ylabel('Count')\n",
    "        plt.title('Distribution of Predicted Ratings')\n",
    "        plt.xticks(range(1, 16))\n",
    "        #plt.show()\n",
    "        plt.savefig('predicted_rankings.png')\n",
    "\n",
    "\n",
    "        mse = np.mean(np.abs(predictions - targets) ** 2)\n",
    "        rmse = np.sqrt(mse)\n",
    "        k = 5  # Define the value of k\n",
    "\n",
    "        print(f\"Batch Size: {batch_size}\")\n",
    "        print(f\"Epochs: {epochs}\")\n",
    "        print   (f\"MSE: {mse}\")\n",
    "        print(f\"RMSE: {rmse}\")\n",
    "        \n",
    "        threshold = 3.5 # Define the threshold to convert predictions into binary values\n",
    "        np.set_printoptions(threshold=sys.maxsize)  # Set the threshold to print the whole array\n",
    "\n",
    "        #print(rounded_predictions)\n",
    "        #print(predictions)\n",
    "\n",
    "        GAT_results = open(\"predictions_GAT.txt\", \"a\")\n",
    "\n",
    "        GAT_results.write(str(predictions))\n",
    "        \n",
    "        GAT_results.close()\n",
    "\n",
    "        precision_value = precision(predictions, targets, threshold)\n",
    "        recall_value = recall(predictions, targets, threshold)\n",
    "        precission_k, recall_k, normalized_recall_k = precission_recall_at_k(predictions, targets, 4, 1000)\n",
    "\n",
    "        with open(\"predictions.txt\", 'w') as file:\n",
    "            for prediction in predictions:\n",
    "                file.write(str(prediction) + '\\n')\n",
    "\n",
    "        print(f\"Precision: {precision_value}\")\n",
    "        print(f\"Recall: {recall_value}\")\n",
    "\n",
    "        \n",
    "        print(f\"Precision@k: {precission_k}\")\n",
    "        print(f\"Recall@k: {recall_k}\")\n",
    "        #print(f\"Normalized Recall@k: {normalized_recall_k}\")\n",
    "\n",
    "        #Now write the file! \n",
    "        if (i % 3 == 0):\n",
    "            model_name = \"GCN_nopos\"\n",
    "        elif (i % 3 == 1):\n",
    "            model_name = \"GCN_var1\"\n",
    "        elif (i%3 == 2):\n",
    "            model_name = \"GCN_variant2\"\n",
    "        \n",
    "        print(i)\n",
    "        with open(\"results.csv\", mode='a', newline='') as csv_file:\n",
    "            csv_writer = csv.writer(csv_file)\n",
    "            csv_writer.writerow([i, model_name, precision_value, recall_value, precission_k, recall_k, mse])\n",
    "\n",
    "\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5047b8bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
