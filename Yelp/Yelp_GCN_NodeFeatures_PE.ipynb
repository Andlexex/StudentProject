{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43aec689",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.utils as utils\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "from torch_geometric.nn import GINConv\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch_geometric as tg\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy import sparse as sp\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8122e7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculatePosEncodings_rswe(edge_index, num_nodes):\n",
    "    edge_index = edge_index.t().tolist()\n",
    "    edges = [(src, dst) for src, dst in edge_index]\n",
    "    rows, cols = zip(*edges)\n",
    "    data = np.ones(len(rows))\n",
    "    A = csr_matrix((data, (rows, cols)), shape=(num_nodes, num_nodes))\n",
    "\n",
    "    in_degrees_dict = {node: 0 for node in range(num_nodes)}\n",
    "    # Calculate the in-degrees for each node\n",
    "    for edge in edges:\n",
    "        _, dst = edge\n",
    "        in_degrees_dict[dst] += 1\n",
    "\n",
    "    in_degrees = np.array([in_degrees_dict[i] for i in range(len(in_degrees_dict))], dtype=float)\n",
    "    in_degrees = in_degrees.clip(1)  # Clip to ensure no division by zero\n",
    "    in_degrees = np.power(in_degrees, -1)  # Take the element-wise inverse square root\n",
    "\n",
    "    Dinv = sp.diags(in_degrees, dtype=float)\n",
    "\n",
    "    RW = A * Dinv  \n",
    "    M = RW\n",
    "    \n",
    "    # das ist wieder ein Hyperparameter; sollte >1 sein weil eins immer 0 ist irgendwie!\n",
    "    pos_enc_dim = 5\n",
    "\n",
    "    nb_pos_enc = pos_enc_dim\n",
    "    PE = [torch.from_numpy(M.diagonal()).float()]\n",
    "    M_power = M\n",
    "    for _ in range(nb_pos_enc-1):\n",
    "        M_power = M_power * M\n",
    "        PE.append(torch.from_numpy(M_power.diagonal()).float())\n",
    "    PE = torch.stack(PE,dim=-1)\n",
    "\n",
    "    #ERGEBNIS\n",
    "    RESULT_POS_ENCODING = PE \n",
    "    return RESULT_POS_ENCODING\n",
    "\n",
    "def calculatePosEncodings(edge_index, num_nodes):\n",
    "    print(\"checkpoint1\")\n",
    "    edge_index = edge_index.t().tolist()\n",
    "    edges = [(src, dst) for src, dst in edge_index]\n",
    "\n",
    "    # Create the adjacency matrix in CSR format -> das wird dann für die encodings benutzt!\n",
    "    rows, cols = zip(*edges)\n",
    "    data = np.ones(len(rows))\n",
    "    A = csr_matrix((data, (rows, cols)), shape=(num_nodes, num_nodes))\n",
    "\n",
    "    ''' this code computes the in_degrees matrix from the edge list. it can later be adapted to compute the in-degrees matrix from the adjacency matrix (however, then, we should\n",
    "    do some tests with small sample graphs to ensure everytheing is correct\n",
    "    '''\n",
    "    in_degrees_dict = {node: 0 for node in range(num_nodes)}\n",
    "    # Calculate the in-degrees for each node\n",
    "    for edge in edges:\n",
    "        print(\"checkpoint2\")\n",
    "        _, dst = edge\n",
    "        in_degrees_dict[dst] += 1\n",
    "\n",
    "    in_degrees = np.array([in_degrees_dict[i] for i in range(len(in_degrees_dict))], dtype=float)\n",
    "    in_degrees = in_degrees.clip(1)  # Clip to ensure no division by zero\n",
    "    in_degrees = np.power(in_degrees, -0.5)  # Take the element-wise inverse square root\n",
    "    print(\"checkpoint3\")\n",
    "    # Create the sparse diagonal matrix N\n",
    "    N = sp.diags(in_degrees, dtype=float)\n",
    "\n",
    "    L = sp.eye(num_nodes) - N * A * N\n",
    "\n",
    "    #calc eigvals and eigVecs, equivalent to the original code\n",
    "    EigVal, EigVec = np.linalg.eig(L.toarray())\n",
    "    idx = EigVal.argsort() # increasing order\n",
    "    EigVal, EigVec = EigVal[idx], np.real(EigVec[:,idx])\n",
    "\n",
    "    #pos_enc_dim = hyperparameter!\n",
    "    pos_enc_dim = 5\n",
    "    RESULT_POS_ENCODING = torch.from_numpy(EigVec[:,1:pos_enc_dim+1]).float() \n",
    "    return RESULT_POS_ENCODING\n",
    "\n",
    "def calculateLoss(task_loss, batch, num_nodes, positional_encoding):\n",
    "    #HYPERPARAMETERS\n",
    "    device = \"cpu\"\n",
    "    pos_enc_dim = 1\n",
    "    alpha_loss: 1e-3\n",
    "    lambda_loss: 100  # ist auch 100\n",
    "\n",
    "    #edge_index im korrekten Format definieren\n",
    "    edge_index = batch.edge_index.t().tolist()\n",
    "    edge_index = [(src, dst) for src, dst in edge_index]\n",
    "\n",
    "    # Loss B: Laplacian Eigenvector Loss --------------------------------------------\n",
    "    n = num_nodes\n",
    "\n",
    "    # Laplacian \n",
    "    rows, cols = zip(*edge_index)\n",
    "    data = np.ones(len(rows))\n",
    "    A = csr_matrix((data, (rows, cols)), shape=(num_nodes, num_nodes))\n",
    "\n",
    "    ''' this code computes the in_degrees matrix from the edge list. it can later be adapted to compute the in-degrees matrix from the adjacency matrix (however, then, we should\n",
    "    do some tests with small sample graphs to ensure everything is correct'''\n",
    "\n",
    "    in_degrees_dict = {node: 0 for node in range(num_nodes)}\n",
    "    # Calculate the in-degrees for each node\n",
    "    for edge in edge_index:\n",
    "        _, dst = edge\n",
    "        in_degrees_dict[dst] += 1\n",
    "\n",
    "    in_degrees = np.array([in_degrees_dict[i] for i in range(len(in_degrees_dict))], dtype=float)\n",
    "    in_degrees = in_degrees.clip(1)  # Clip to ensure no division by zero\n",
    "    in_degrees = np.power(in_degrees, -0.5)  # Take the element-wise inverse square root\n",
    "\n",
    "    # Create the sparse diagonal matrix N\n",
    "    N = sp.diags(in_degrees, dtype=float)\n",
    "    L = sp.eye(num_nodes) - N * A * N\n",
    "\n",
    "    p = positional_encoding\n",
    "    pT = torch.transpose(p, 1, 0)\n",
    "    loss_b_1 = torch.trace(torch.mm(torch.mm(pT, torch.Tensor(L.todense()).to(device)), p))\n",
    "\n",
    "    '''  TODO: loss_b_2 \n",
    "    '''\n",
    "\n",
    "    loss_b = loss_b_1\n",
    "\n",
    "    #TODO: parameter tunen!\n",
    "    loss = task_loss + 1e-3* loss_b\n",
    "    return loss\n",
    "\n",
    "\n",
    "def precision(predictions, targets, threshold):\n",
    "    # Apply a threshold to the predictions\n",
    "    binary_predictions = (predictions >= threshold).astype(int)\n",
    "    binary_targets = (targets >= threshold).astype(int)\n",
    "\n",
    "    # Calculate the true positive (TP) and false positive (FP) counts\n",
    "    TP = np.sum((binary_predictions == 1) & (binary_targets == 1))\n",
    "    FP = np.sum((binary_predictions == 1) & (binary_targets == 0))\n",
    "\n",
    "    print(\"Negative: \")\n",
    "    print(np.sum(binary_targets == 1))\n",
    "    print(\"Positive: \")\n",
    "    print(np.sum(binary_targets == 0))\n",
    "    # Calculate precision\n",
    "    precision_value = TP / (TP + FP)\n",
    "    return precision_value\n",
    "\n",
    "def recall(predictions, targets, threshold):\n",
    "    # Apply a threshold to the predictions\n",
    "    binary_predictions = (predictions >= threshold).astype(int)\n",
    "    binary_targets = (targets >= threshold).astype(int)\n",
    "    # Calculate the true positive (TP) and false negative (FN) counts\n",
    "    TP = np.sum((binary_predictions == 1) & (binary_targets == 1))\n",
    "    FN = np.sum((binary_predictions == 0) & (binary_targets == 1))\n",
    "    # Calculate recall\n",
    "    recall_value = TP / (TP + FN)\n",
    "    return recall_value\n",
    "\n",
    "def precission_recall_at_k (predictions, targets, threshold, k):\n",
    "    # Combine ratings and predictions into tuples for sorting\n",
    "    combined = list(zip(targets, predictions))\n",
    "\n",
    "    # Sort the combined list in descending order of predictions\n",
    "    combined.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Extract top k sorted items and calculate precision and recall\n",
    "    top_k_items = combined[:k]\n",
    "    true_positives = sum(1 for rating, _ in top_k_items if rating >= threshold)\n",
    "    false_positives = k - true_positives\n",
    "    relevant_items = sum(1 for rating in targets if rating >= threshold)\n",
    "    false_negatives = relevant_items - true_positives\n",
    "\n",
    "    precision_at_k = true_positives / (true_positives + false_positives)\n",
    "    recall_at_k = true_positives / (true_positives + false_negatives)\n",
    "    normalized_recall_at_k = recall_at_k / (k / relevant_items)\n",
    "\n",
    "\n",
    "    return precision_at_k, recall_at_k, normalized_recall_at_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d4d17e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done business\n",
      "done reviews\n",
      "done users\n"
     ]
    }
   ],
   "source": [
    "# Reading in all the data\n",
    "\n",
    "business_ids=[]\n",
    "business_average_stars=[]\n",
    "business_review_count=[]\n",
    "\n",
    "try:\n",
    "    with open('yelp_dataset/yelp_academic_dataset_business.json', 'r', encoding='utf-8') as json_file:\n",
    "        for line in json_file:\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "                business_ids.append(data[\"business_id\"])\n",
    "                business_average_stars.append(data[\"stars\"])\n",
    "                business_review_count.append(data[\"review_count\"])\n",
    "            except json.decoder.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON: {e}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"File not found.\")\n",
    "print(\"done business\")\n",
    "\n",
    "review_ids=[]\n",
    "review_business_ids=[]\n",
    "review_user_ids=[]\n",
    "review_stars=[]\n",
    "\n",
    "try:\n",
    "    with open('yelp_dataset/yelp_academic_dataset_review.json', 'r', encoding='utf-8') as json_file:\n",
    "        for i,line in enumerate(json_file):\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "                review_ids.append(data[\"review_id\"])\n",
    "                review_business_ids.append(data[\"business_id\"])\n",
    "                review_user_ids.append(data[\"user_id\"])\n",
    "                review_stars.append(data[\"stars\"])\n",
    "            except json.decoder.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON: {e}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"File not found.\")\n",
    "print(\"done reviews\")\n",
    "\n",
    "user_ids=[]\n",
    "\n",
    "try:\n",
    "    with open('yelp_dataset/yelp_academic_dataset_user.json', 'r', encoding='utf-8') as json_file:\n",
    "        for line in json_file:\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "                user_ids.append(data[\"user_id\"])\n",
    "            except json.decoder.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON: {e}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"File not found.\")\n",
    "print(\"done users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "618b183e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1987897\n",
      "150346\n",
      "6990280\n",
      "---------------------------------------------\n",
      "20000\n",
      "10000\n",
      "6990280\n"
     ]
    }
   ],
   "source": [
    "# As there is too much data use a subset by manipulating the business and user sets\n",
    "\n",
    "print(len(user_ids))\n",
    "print(len(business_ids))\n",
    "print(len(review_stars))\n",
    "\n",
    "businesses = 10000\n",
    "users = 20000\n",
    "\n",
    "business_average_stars = business_average_stars[:businesses]\n",
    "business_review_count = business_review_count[:businesses]\n",
    "business_ids = business_ids[:businesses]\n",
    "\n",
    "user_ids = user_ids[:users]\n",
    "\n",
    "print(\"---------------------------------------------\")\n",
    "print(len(user_ids))\n",
    "print(len(business_ids))\n",
    "print(len(review_stars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eed791a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47703\n"
     ]
    }
   ],
   "source": [
    "# User and business to index mapping in order.\n",
    "# Review lists are adjusted to only include users and businesses from the subset lists.\n",
    "\n",
    "user_to_index = {user_id: index for index, user_id in enumerate(set(user_ids))}\n",
    "business_to_index = {business_id: index for index, business_id in enumerate(set(business_ids))}\n",
    "\n",
    "\n",
    "user_index_col = [user_to_index[user_id] for user_id in user_ids]\n",
    "business_index_col = [business_to_index[business_id] for business_id in business_ids]\n",
    "\n",
    "skip_indexes = []\n",
    "current_index = -1\n",
    "\n",
    "for user_id in review_user_ids:\n",
    "    try:\n",
    "        current_index = current_index + 1\n",
    "        tmp = user_to_index[user_id]\n",
    "    except KeyError:\n",
    "        skip_indexes.append(current_index)\n",
    "\n",
    "current_index = -1\n",
    "for business_id in review_business_ids:\n",
    "    try:\n",
    "        current_index = current_index + 1\n",
    "        tmp = business_to_index[business_id]\n",
    "    except KeyError:\n",
    "        skip_indexes.append(current_index)    \n",
    "\n",
    "skip_indexes = set(skip_indexes)\n",
    "print(6990280 - len(skip_indexes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75d50b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22403\n",
      "30000\n",
      "Number of Users in index col 16327\n",
      "Number of businesses in index col 6076\n",
      "Total reviews left 47703\n",
      "22403\n"
     ]
    }
   ],
   "source": [
    "current_index = -1\n",
    "\n",
    "review_user_index_col = []\n",
    "for user_id in review_user_ids:\n",
    "    try:\n",
    "        current_index = current_index + 1\n",
    "        if current_index in skip_indexes:\n",
    "            pass\n",
    "        else:\n",
    "            user_index = user_to_index[user_id]\n",
    "            review_user_index_col.append(businesses+user_index)\n",
    "    except KeyError:\n",
    "        print(\"Huh?\")        \n",
    "        \n",
    "current_index = -1        \n",
    "review_business_index_col = []\n",
    "for business_id in review_business_ids:\n",
    "    try:\n",
    "        current_index = current_index + 1\n",
    "        if current_index in skip_indexes:\n",
    "            pass\n",
    "        else:\n",
    "            business_index = business_to_index[business_id]\n",
    "            review_business_index_col.append(business_index)\n",
    "    except KeyError:\n",
    "        print(\"Huh?\")\n",
    "    \n",
    "current_index = -1\n",
    "adjusted_review_stars = []\n",
    "for star in review_stars:\n",
    "        current_index = current_index + 1\n",
    "        if current_index in skip_indexes:\n",
    "            pass\n",
    "        else:\n",
    "            adjusted_review_stars.append(star)\n",
    "\n",
    "num_nodes = len(set(review_user_index_col)) + len(set(review_business_index_col)) \n",
    "print(num_nodes)\n",
    "print(len(user_to_index)+len(business_to_index))\n",
    "\n",
    "\n",
    "\n",
    "print(\"Number of Users in index col \" + str(len(set(review_user_index_col))))\n",
    "print(\"Number of businesses in index col \" + str(len(set(review_business_index_col))))\n",
    "print(\"Total reviews left \" + str(len(adjusted_review_stars)))\n",
    "print(num_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1f84a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4554\n",
      "tensor([[5.0000e+00, 1.5371e-03],\n",
      "        [3.0000e+00, 3.2938e-03],\n",
      "        [3.5000e+00, 4.8309e-03],\n",
      "        ...,\n",
      "        [0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00]])\n"
     ]
    }
   ],
   "source": [
    "# Feature list including average stars and the review_count/max_review_count of a business.\n",
    "# Also a dummy padding is added for the users.\n",
    "\n",
    "adjusted_features = []\n",
    "\n",
    "for starts in business_average_stars:\n",
    "    adjusted_features.append([starts])\n",
    "    \n",
    "highest_review_count = 0\n",
    "for review in business_review_count:\n",
    "    if(review > highest_review_count):\n",
    "        highest_review_count = review\n",
    "print(highest_review_count)\n",
    "        \n",
    "for i in range(len(business_review_count)):\n",
    "    adjusted_features[i].append(business_review_count[i]/highest_review_count)\n",
    "\n",
    "adjusted_features_tensor = torch.tensor(adjusted_features)\n",
    "\n",
    "num_features = len(adjusted_features[0])\n",
    "\n",
    "num_users = len(set(user_ids))\n",
    "user_genre_features = torch.zeros(num_users,num_features)\n",
    "\n",
    "features = torch.cat((adjusted_features_tensor, user_genre_features), dim=0)\n",
    "\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a582bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[11407, 26534, 28280,  ..., 18836, 15628, 12563],\n",
      "        [ 9559,  5602,  9756,  ...,  1490,  3442,  2342]])\n"
     ]
    }
   ],
   "source": [
    "rating_tensor = torch.tensor(adjusted_review_stars, dtype=torch.float)\n",
    "\n",
    "edge_index = torch.tensor([review_user_index_col, review_business_index_col], dtype=torch.long)\n",
    "\n",
    "positional_encodings = calculatePosEncodings_rswe(edge_index, 30000)\n",
    "\n",
    "#TODO: num nodes 30.000 oder num_nodes?\n",
    "data = Data(edge_index=edge_index, x=features, y=rating_tensor, positional_encodings=positional_encodings)\n",
    "\n",
    "print(edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f393a925",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GCN_nopos(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(7, hidden_channels)\n",
    "        self.conv1_nopos = GCNConv(2, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv2_var2 = GCNConv(hidden_channels*2, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        #hidden channels funktioniert besser!\n",
    "        self.conv3_nopos = GCNConv(hidden_channels, hidden_channels)\n",
    "\n",
    "\n",
    "        #this is for learning of the positional encodings, which is seperate!!!\n",
    "        self.conv1_pos = GCNConv(5, hidden_channels)\n",
    "        self.conv2_pos = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3_pos = GCNConv(hidden_channels, hidden_channels)\n",
    "\n",
    "        self.linear = nn.Linear(hidden_channels, 1)\n",
    "        \n",
    "    def forward(self, x, edge_index, pos_embeddings):\n",
    "        x = x.view(-1, x.size(2))\n",
    "        #fh from the paper\n",
    "        x = self.conv1_nopos(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3_nopos(x, edge_index)\n",
    "\n",
    "        movie_embed = x[edge_index[1]]\n",
    "        \n",
    "        ratings = torch.sum(movie_embed, dim=1)\n",
    "\n",
    "        return ratings\n",
    "\n",
    "\n",
    "class GCN_var1(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        # number of in layers = number of node features + number of positional embedding dimensions\n",
    "        # TODO: ADAPT WHEN ADDING FEATURES OR EMBEDDING DIMENSIONS!!!!\n",
    "        self.conv1 = GCNConv(7, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "\n",
    "        #this is for learning of the positional encodings, which is seperate!!!\n",
    "        self.conv1_pos = GCNConv(5, hidden_channels)\n",
    "        self.conv2_pos = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3_pos = GCNConv(hidden_channels, hidden_channels)\n",
    "\n",
    "        self.linear = nn.Linear(hidden_channels, 1)\n",
    "\n",
    "    def forward(self, x, edge_index, pos_embeddings):\n",
    "        x = x.view(-1, x.size(2))\n",
    "        pos_embeddings = pos_embeddings.view(-1, pos_embeddings.size(2))\n",
    "        x = torch.cat([x, pos_embeddings], dim=1)\n",
    "\n",
    "        #fh from the paper\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3(x, edge_index)\n",
    "\n",
    "        #Now the learning of positional embeddings. So this is fp from the paper\n",
    "        pos_embeddings = self.conv1_pos(pos_embeddings, edge_index)\n",
    "        pos_embeddings = F.relu(pos_embeddings)\n",
    "        pos_embeddings = self.conv2_pos(pos_embeddings, edge_index)\n",
    "        pos_embeddings = F.relu(pos_embeddings)\n",
    "        pos_embeddings = self.conv3_pos(pos_embeddings, edge_index)\n",
    "\n",
    "        final_output = self.linear(torch.cat([x, pos_embeddings]))\n",
    "\n",
    "        movie_embed = final_output[edge_index[1]]            \n",
    "        ratings = torch.sum(movie_embed, dim=1)\n",
    "        return ratings\n",
    "\n",
    "\n",
    "class GCN_variant2(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        # number of in layers = number of node features + number of positional embedding dimensions\n",
    "        # TODO: ADAPT WHEN ADDING FEATURES OR EMBEDDING DIMENSIONS!!!!\n",
    "        self.conv1 = GCNConv(7, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv2_var2 = GCNConv(hidden_channels*2, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "\n",
    "        #this is for learning of the positional encodings, which is seperate!!!\n",
    "        self.conv1_pos = GCNConv(5, hidden_channels)\n",
    "        self.conv2_pos = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3_pos = GCNConv(hidden_channels, hidden_channels)\n",
    "\n",
    "        self.linear = nn.Linear(hidden_channels, 1)\n",
    "\n",
    "    def forward(self, x, edge_index, pos_embeddings):\n",
    "        x = x.view(-1, x.size(2))\n",
    "        pos_embeddings = pos_embeddings.view(-1, pos_embeddings.size(2))\n",
    "        x = torch.cat([x, pos_embeddings], dim=1)\n",
    "\n",
    "        #fh from the paper\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        pos_embeddings = self.conv1_pos(pos_embeddings, edge_index)\n",
    "        pos_embeddings = F.relu(pos_embeddings)\n",
    "        #x = self.conv2(torch.cat([x,pos_embeddings],dim=1), edge_index)\n",
    "        x = self.conv2_var2(torch.cat([x,pos_embeddings],dim=1), edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3(x, edge_index)\n",
    "\n",
    "        movie_embed = x[edge_index[1]]\n",
    "        ratings = torch.sum(movie_embed, dim=1)\n",
    "\n",
    "        return ratings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "95e32818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300, Training Loss: 16.32558822631836, Validation Loss: 15.493612289428711\n",
      "Epoch 2/300, Training Loss: 15.149624824523926, Validation Loss: 14.093819618225098\n",
      "Epoch 3/300, Training Loss: 13.366190910339355, Validation Loss: 12.715886116027832\n",
      "Epoch 4/300, Training Loss: 11.64103889465332, Validation Loss: 11.361570358276367\n",
      "Epoch 5/300, Training Loss: 9.977531433105469, Validation Loss: 9.995509147644043\n",
      "Epoch 6/300, Training Loss: 8.332480430603027, Validation Loss: 8.646432876586914\n",
      "Epoch 7/300, Training Loss: 6.837743282318115, Validation Loss: 7.335582256317139\n",
      "Epoch 8/300, Training Loss: 5.611186504364014, Validation Loss: 6.120619297027588\n",
      "Epoch 9/300, Training Loss: 4.78764009475708, Validation Loss: 5.067718029022217\n",
      "Epoch 10/300, Training Loss: 4.48012113571167, Validation Loss: 4.249450206756592\n",
      "Epoch 11/300, Training Loss: 4.69005012512207, Validation Loss: 3.709867238998413\n",
      "Epoch 12/300, Training Loss: 5.169742584228516, Validation Loss: 3.408831834793091\n",
      "Epoch 13/300, Training Loss: 5.522312641143799, Validation Loss: 3.2614917755126953\n",
      "Epoch 14/300, Training Loss: 5.545283317565918, Validation Loss: 3.2112066745758057\n",
      "Epoch 15/300, Training Loss: 5.274724960327148, Validation Loss: 3.2323033809661865\n",
      "Epoch 16/300, Training Loss: 4.843565940856934, Validation Loss: 3.3112637996673584\n",
      "Epoch 17/300, Training Loss: 4.384167671203613, Validation Loss: 3.4346606731414795\n",
      "Epoch 18/300, Training Loss: 3.9883415699005127, Validation Loss: 3.585259199142456\n"
     ]
    }
   ],
   "source": [
    "indices = list(range(data.edge_index.size(1)))\n",
    "import csv\n",
    "csv_filename = \"results.csv\"\n",
    "with open(csv_filename, mode='a', newline='') as csv_file:\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "    csv_writer.writerow([\"i\", \"Model\", \"Precision\", \"Recall\", \"Precision@k\", \"Recall@k\", \"MSE\"])  # Write header\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in range(30):\n",
    "    #das hier klein, damit der Speicher nicht überdreht wird. Aber nicht zu klein, weil sonst kommt es zu problemen!\n",
    "    if (i % 3 == 0):\n",
    "        train_indices, test_indices = train_test_split(indices, train_size=0.8, test_size=0.2)\n",
    "        train_indices, val_indices = train_test_split(train_indices, train_size=0.8, test_size=0.2, random_state=42)\n",
    "        np.savez('indices.npz', train_indices=train_indices, test_indices=test_indices, val_indices=val_indices)\n",
    "    # Now, you can comment out the above code that generates the indices\n",
    "    # Read the indices from the file\n",
    "    loaded_indices = np.load('indices.npz')\n",
    "    #irgendeine syntax\n",
    "    train_data = data.__class__()\n",
    "    test_data = data.__class__()\n",
    "    val_data = data.__class__()\n",
    "\n",
    "\n",
    "    #setzt die Parameter von train_data und test_data\n",
    "\n",
    "    #soweit ich es verstehe, sind alle 2.500 nodes im training und testset vorhanden. gesplittet werden nur die edges, d.h. \n",
    "    #es ist nur ein subset der 100.000 edges im training set sowie im test set vorhanden\n",
    "    # also 10% der Bewertungen \n",
    "    train_data.edge_index = data.edge_index[:, train_indices]\n",
    "    train_data.y = data.y[train_indices]\n",
    "    train_data.num_nodes = data.num_nodes\n",
    "    train_data.positional_encodings = data.positional_encodings\n",
    "    train_data.x = data.x\n",
    "\n",
    "\n",
    "    test_data.edge_index = data.edge_index[:, test_indices]\n",
    "    test_data.y = data.y[test_indices]\n",
    "    test_data.num_nodes = data.num_nodes\n",
    "    test_data.positional_encodings = data.positional_encodings\n",
    "    test_data.x = data.x\n",
    "\n",
    "\n",
    "    val_data.edge_index = data.edge_index[:, val_indices]\n",
    "    val_data.y = data.y[val_indices]\n",
    "    val_data.num_nodes = data.num_nodes\n",
    "    val_data.positional_encodings = data.positional_encodings\n",
    "    val_data.x = data.x\n",
    "        \n",
    "\n",
    "\n",
    "    # Step 6: Train and evaluate the GCN model\n",
    "    # Set seed for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    # Set the device --> aktiviere GPU falls vorhanden\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    #------------------------------------------------------\n",
    "\n",
    "    #hidden channels und epochs tunen\n",
    "    hidden_channels=8 #8 und 16\n",
    "    lr = 0.01  #0.01 vs 0.001 \n",
    "    epochs = 300  #100 vs 200\n",
    "    batch_size = 512#512\n",
    "\n",
    "    #1, 16, 32 ,64, 128, 256, 512\n",
    "\n",
    "    #Early Stopping\n",
    "    patience = 40  # Number of epochs to wait for improvement\n",
    "    min_delta = 0.001  # Minimum improvement required to consider as improvement\n",
    "\n",
    "    best_val_loss = np.inf\n",
    "    best_epoch = 0\n",
    "    early_stop_counter = 0\n",
    "\n",
    "    # this is for evaluation!\n",
    "    if (i % 3 == 0):\n",
    "        model = GCN_nopos(hidden_channels=hidden_channels)\n",
    "    elif (i % 3 == 1):\n",
    "        model = GCN_var1(hidden_channels=hidden_channels)\n",
    "    elif (i%3 == 2):\n",
    "        model = GCN_variant2(hidden_channels=hidden_channels)\n",
    "    #------------------------------------------------------\n",
    "    #loss function, and optimizer, MSE = Metrik für Loss \n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Create data loaders for training and test sets\n",
    "    train_loader = DataLoader([train_data], batch_size=batch_size)\n",
    "    test_loader = DataLoader([test_data], batch_size=batch_size)\n",
    "    val_loader = DataLoader([val_data], batch_size=batch_size)\n",
    "\n",
    "    # Model training\n",
    "    model.train()\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    predictions =[]\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(device)\n",
    "            out = model( batch.x.unsqueeze(1), batch.edge_index, batch.positional_encodings.unsqueeze(1))\n",
    "            task_loss = criterion(out, batch.y)\n",
    "            loss = task_loss\n",
    "            #loss = calculateLoss(task_loss, batch, num_nodes, batch.positional_encodings)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * batch.num_graphs\n",
    "            predictions = out.detach().cpu().numpy()\n",
    "            #print(predictions)\n",
    "\n",
    "        # Calculate average training loss\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "    # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        for batch in val_loader:\n",
    "            batch = batch.to(device)\n",
    "            out= model(batch.x.unsqueeze(1),batch.edge_index, batch.positional_encodings.unsqueeze(1))\n",
    "            task_loss = criterion(out, batch.y)\n",
    "            loss = task_loss\n",
    "        # loss = calculateLoss(task_loss, batch, num_nodes, batch.positional_encodings)\n",
    "            \n",
    "            val_loss += loss.item() * batch.num_graphs\n",
    "\n",
    "        # Calculate average validation loss\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        # Print training and validation loss for monitoring\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {train_loss}, Validation Loss: {val_loss}\")\n",
    "\n",
    "        # Check for early stopping\n",
    "        if val_loss < best_val_loss - min_delta:\n",
    "            best_val_loss = val_loss\n",
    "            best_epoch = epoch\n",
    "            early_stop_counter = 0\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "            if early_stop_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "        # Set the model back to training mode\n",
    "        model.train()\n",
    "        '''\n",
    "        # Plotting training and validation curves\n",
    "        plt.plot(train_losses, label='Training Loss')\n",
    "        #plt.plot(val_losses, label='Validation Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "\n",
    "        # Save the plot as an image file\n",
    "        plt.savefig('loss_plot.png')\n",
    "        '''\n",
    "\n",
    "        # Show the plot\n",
    "\n",
    "    # Evaluation on test set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = []\n",
    "        targets = []\n",
    "\n",
    "        for batch in test_loader:\n",
    "            batch = batch.to(device)\n",
    "            out = model(batch.x.unsqueeze(1), batch.edge_index, batch.positional_encodings.unsqueeze(1))\n",
    "            task_loss = criterion(out, batch.y)\n",
    "            test_loss = task_loss\n",
    "            #test_loss = calculateLoss(task_loss, batch, num_nodes, batch.positional_encodings)\n",
    "            \n",
    "            #print(f'Test Loss: {test_loss.item()}')\n",
    "            predictions.extend(out.cpu().numpy().flatten())\n",
    "            targets.extend(batch.y.cpu().numpy().flatten())\n",
    "\n",
    "        predictions = np.array(predictions)\n",
    "        targets = np.array(targets)\n",
    "\n",
    "        rounded_predictions = np.round(predictions, decimals=0)  # Round the predicted ratings\n",
    "\n",
    "        # Plotting the distribution\n",
    "        plt.hist(rounded_predictions, bins=15, edgecolor='black')\n",
    "        plt.xlabel('Predicted Rating')\n",
    "        plt.ylabel('Count')\n",
    "        plt.title('Distribution of Predicted Ratings')\n",
    "        plt.xticks(range(1, 16))\n",
    "        #plt.show()\n",
    "        plt.savefig('predicted_rankings.png')\n",
    "\n",
    "\n",
    "        mse = np.mean(np.abs(predictions - targets) ** 2)\n",
    "        rmse = np.sqrt(mse)\n",
    "        k = 5  # Define the value of k\n",
    "\n",
    "        print(f\"Batch Size: {batch_size}\")\n",
    "        print(f\"Epochs: {epochs}\")\n",
    "        print   (f\"MSE: {mse}\")\n",
    "        print(f\"RMSE: {rmse}\")\n",
    "        \n",
    "        threshold = 3.5 # Define the threshold to convert predictions into binary values\n",
    "        np.set_printoptions(threshold=sys.maxsize)  # Set the threshold to print the whole array\n",
    "\n",
    "        #print(rounded_predictions)\n",
    "        #print(predictions)\n",
    "\n",
    "        GAT_results = open(\"predictions_GAT.txt\", \"a\")\n",
    "\n",
    "        GAT_results.write(str(predictions))\n",
    "        \n",
    "        GAT_results.close()\n",
    "\n",
    "        precision_value = precision(predictions, targets, threshold)\n",
    "        recall_value = recall(predictions, targets, threshold)\n",
    "        precission_k, recall_k, normalized_recall_k = precission_recall_at_k(predictions, targets, 4, 1000)\n",
    "\n",
    "        with open(\"predictions.txt\", 'w') as file:\n",
    "            for prediction in predictions:\n",
    "                file.write(str(prediction) + '\\n')\n",
    "\n",
    "        print(f\"Precision: {precision_value}\")\n",
    "        print(f\"Recall: {recall_value}\")\n",
    "\n",
    "        \n",
    "        print(f\"Precision@k: {precission_k}\")\n",
    "        print(f\"Recall@k: {recall_k}\")\n",
    "        #print(f\"Normalized Recall@k: {normalized_recall_k}\")\n",
    "\n",
    "        #Now write the file! \n",
    "        if (i % 3 == 0):\n",
    "            model_name = \"GCN_nopos\"\n",
    "        elif (i % 3 == 1):\n",
    "            model_name = \"GCN_var1\"\n",
    "        elif (i%3 == 2):\n",
    "            model_name = \"GCN_variant2\"\n",
    "        \n",
    "        print(i)\n",
    "        with open(\"results.csv\", mode='a', newline='') as csv_file:\n",
    "            csv_writer = csv.writer(csv_file)\n",
    "            csv_writer.writerow([i, model_name, precision_value, recall_value, precission_k, recall_k, mse])\n",
    "\n",
    "\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5047b8bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
