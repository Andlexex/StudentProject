{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43aec689",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.utils as utils\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "from torch_geometric.nn import GINConv\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch_geometric as tg\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy import sparse as sp\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8122e7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculatePosEncodings(edge_index, num_nodes):\n",
    "    edge_index = edge_index.t().tolist()\n",
    "    edges = [(src, dst) for src, dst in edge_index]\n",
    "\n",
    "# Create the adjacency matrix in CSR format -> das wird dann fÃ¼r die encodings benutzt!\n",
    "    rows, cols = zip(*edges)\n",
    "    data = np.ones(len(rows))\n",
    "    A = csr_matrix((data, (rows, cols)), shape=(num_nodes, num_nodes))\n",
    "\n",
    "    ''' this code computes the in_degrees matrix from the edge list. it can later be adapted to compute the in-degrees matrix from the adjacency matrix (however, then, we should\n",
    "    do some tests with small sample graphs to ensure everything is correct\n",
    "    '''\n",
    "    in_degrees_dict = {node: 0 for node in range(num_nodes)}\n",
    "    # Calculate the in-degrees for each node\n",
    "    for edge in edges:\n",
    "        _, dst = edge\n",
    "        in_degrees_dict[dst] += 1\n",
    "\n",
    "    in_degrees = np.array([in_degrees_dict[i] for i in range(len(in_degrees_dict))], dtype=float)\n",
    "    in_degrees = in_degrees.clip(1)  # Clip to ensure no division by zero\n",
    "    in_degrees = np.power(in_degrees, -0.5)  # Take the element-wise inverse square root\n",
    "\n",
    "    # Create the sparse diagonal matrix N\n",
    "    N = sp.diags(in_degrees, dtype=float)\n",
    "\n",
    "    L = sp.eye(num_nodes) - N * A * N\n",
    "\n",
    "    #calc eigvals and eigVecs, equivalent to the original code\n",
    "    EigVal, EigVec = np.linalg.eig(L.toarray())\n",
    "    idx = EigVal.argsort() # increasing order\n",
    "    EigVal, EigVec = EigVal[idx], np.real(EigVec[:,idx])\n",
    "\n",
    "    #pos_enc_dim = hyperparameter!\n",
    "    pos_enc_dim = 5\n",
    "    RESULT_POS_ENCODING = torch.from_numpy(EigVec[:,1:pos_enc_dim+1]).float() \n",
    "    return RESULT_POS_ENCODING\n",
    "\n",
    "def calculateLoss(task_loss, batch, num_nodes):\n",
    "    #HYPERPARAMETERS\n",
    "    device = \"cpu\"\n",
    "    pos_enc_dim = 1\n",
    "    alpha_loss: 1e-3\n",
    "    lambda_loss: 100  # ist auch 100\n",
    "\n",
    "    #edge_index im korrekten Format definieren\n",
    "    edge_index = batch.edge_index.t().tolist()\n",
    "    edge_index = [(src, dst) for src, dst in edge_index]\n",
    "\n",
    "    # Loss B: Laplacian Eigenvector Loss --------------------------------------------\n",
    "    n = num_nodes\n",
    "\n",
    "    # Laplacian \n",
    "    rows, cols = zip(*edge_index)\n",
    "    data = np.ones(len(rows))\n",
    "    A = csr_matrix((data, (rows, cols)), shape=(num_nodes, num_nodes))\n",
    "\n",
    "    ''' this code computes the in_degrees matrix from the edge list. it can later be adapted to compute the in-degrees matrix from the adjacency matrix (however, then, we should\n",
    "    do some tests with small sample graphs to ensure everything is correct'''\n",
    "\n",
    "    in_degrees_dict = {node: 0 for node in range(num_nodes)}\n",
    "    # Calculate the in-degrees for each node\n",
    "    for edge in edge_index:\n",
    "        _, dst = edge\n",
    "        in_degrees_dict[dst] += 1\n",
    "\n",
    "    in_degrees = np.array([in_degrees_dict[i] for i in range(len(in_degrees_dict))], dtype=float)\n",
    "    in_degrees = in_degrees.clip(1)  # Clip to ensure no division by zero\n",
    "    in_degrees = np.power(in_degrees, -0.5)  # Take the element-wise inverse square root\n",
    "\n",
    "    # Create the sparse diagonal matrix N\n",
    "    N = sp.diags(in_degrees, dtype=float)\n",
    "    L = sp.eye(num_nodes) - N * A * N\n",
    "\n",
    "#    p = positional_encoding\n",
    "    pT = torch.transpose(p, 1, 0)\n",
    "    loss_b_1 = torch.trace(torch.mm(torch.mm(pT, torch.Tensor(L.todense()).to(device)))) #Da war ein ,p zwischen Klammer 2 u 3\n",
    "\n",
    "    '''  TODO: loss_b_2 \n",
    "    '''\n",
    "\n",
    "    loss_b = loss_b_1\n",
    "\n",
    "    #TODO: parameter tunen!\n",
    "    loss = task_loss + 1e-3* loss_b\n",
    "    return loss\n",
    "\n",
    "\n",
    "def precision(predictions, targets, threshold):\n",
    "    # Apply a threshold to the predictions\n",
    "    binary_predictions = (predictions >= threshold).astype(int)\n",
    "    binary_targets = (targets >= threshold).astype(int)\n",
    "\n",
    "    # Calculate the true positive (TP) and false positive (FP) counts\n",
    "    TP = np.sum((binary_predictions == 1) & (binary_targets == 1))\n",
    "    FP = np.sum((binary_predictions == 1) & (binary_targets == 0))\n",
    "\n",
    "    print(\"Negative: \")\n",
    "    print(np.sum(binary_targets == 1))\n",
    "    print(\"Positive: \")\n",
    "    print(np.sum(binary_targets == 0))\n",
    "    # Calculate precision\n",
    "    precision_value = TP / (TP + FP)\n",
    "    return precision_value\n",
    "\n",
    "def recall(predictions, targets, threshold):\n",
    "    # Apply a threshold to the predictions\n",
    "    binary_predictions = (predictions >= threshold).astype(int)\n",
    "    binary_targets = (targets >= threshold).astype(int)\n",
    "    # Calculate the true positive (TP) and false negative (FN) counts\n",
    "    TP = np.sum((binary_predictions == 1) & (binary_targets == 1))\n",
    "    FN = np.sum((binary_predictions == 0) & (binary_targets == 1))\n",
    "    # Calculate recall\n",
    "    recall_value = TP / (TP + FN)\n",
    "    return recall_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d4d17e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done business\n",
      "done reviews\n",
      "done users\n"
     ]
    }
   ],
   "source": [
    "business_ids=[]\n",
    "\n",
    "try:\n",
    "    with open('yelp_dataset/yelp_academic_dataset_business.json', 'r', encoding='utf-8') as json_file:\n",
    "        for line in json_file:\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "                business_ids.append(data[\"business_id\"])\n",
    "            except json.decoder.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON: {e}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"File not found.\")\n",
    "print(\"done business\")\n",
    "\n",
    "review_ids=[]\n",
    "review_business_ids=[]\n",
    "review_user_ids=[]\n",
    "review_stars=[]\n",
    "\n",
    "try:\n",
    "    with open('yelp_dataset/yelp_academic_dataset_review.json', 'r', encoding='utf-8') as json_file:\n",
    "        for i,line in enumerate(json_file):\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "                review_ids.append(data[\"review_id\"])\n",
    "                review_business_ids.append(data[\"business_id\"])\n",
    "                review_user_ids.append(data[\"user_id\"])\n",
    "                review_stars.append(data[\"stars\"])\n",
    "            except json.decoder.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON: {e}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"File not found.\")\n",
    "print(\"done reviews\")\n",
    "\n",
    "user_ids=[]\n",
    "\n",
    "try:\n",
    "    with open('yelp_dataset/yelp_academic_dataset_user.json', 'r', encoding='utf-8') as json_file:\n",
    "        for line in json_file:\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "                user_ids.append(data[\"user_id\"])\n",
    "            except json.decoder.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON: {e}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"File not found.\")\n",
    "print(\"done users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "618b183e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1987897\n",
      "150346\n",
      "6990280\n",
      "---------------------------------------------\n",
      "20000\n",
      "10000\n",
      "6990280\n"
     ]
    }
   ],
   "source": [
    "print(len(user_ids))\n",
    "print(len(business_ids))\n",
    "print(len(review_stars))\n",
    "\n",
    "max_businesses = 10000\n",
    "max_users = 20000\n",
    "\n",
    "business_ids = business_ids[:max_businesses]\n",
    "user_ids = user_ids[:max_users]\n",
    "\n",
    "print(\"---------------------------------------------\")\n",
    "print(len(user_ids))\n",
    "print(len(business_ids))\n",
    "print(len(review_stars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eed791a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47703\n"
     ]
    }
   ],
   "source": [
    "user_to_index = {user_id: index for index, user_id in enumerate(set(user_ids))}\n",
    "business_to_index = {business_id: index for index, business_id in enumerate(set(business_ids))}\n",
    "\n",
    "user_index_col = [user_to_index[user_id] for user_id in user_ids]\n",
    "business_index_col = [business_to_index[business_id] for business_id in business_ids]\n",
    "\n",
    "\n",
    "skip_indexes = []\n",
    "current_index = -1\n",
    "\n",
    "for user_id in review_user_ids:\n",
    "    try:\n",
    "        current_index = current_index + 1\n",
    "        tmp = user_to_index[user_id]\n",
    "    except KeyError:\n",
    "        skip_indexes.append(current_index)\n",
    "\n",
    "current_index = -1\n",
    "for business_id in review_business_ids:\n",
    "    try:\n",
    "        current_index = current_index + 1\n",
    "        tmp = business_to_index[business_id]\n",
    "    except KeyError:\n",
    "        skip_indexes.append(current_index)    \n",
    "\n",
    "skip_indexes = set(skip_indexes)\n",
    "print(6990280 - len(skip_indexes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75d50b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47703\n"
     ]
    }
   ],
   "source": [
    "current_index = -1\n",
    "\n",
    "review_user_index_col = []\n",
    "for user_id in review_user_ids:\n",
    "    try:\n",
    "        current_index = current_index + 1\n",
    "        if current_index in skip_indexes:\n",
    "            pass\n",
    "        else:\n",
    "            user_index = user_to_index[user_id]\n",
    "            review_user_index_col.append(user_index)\n",
    "    except KeyError:\n",
    "        print(\"Huh?\")        \n",
    "        \n",
    "current_index = -1        \n",
    "review_business_index_col = []\n",
    "for business_id in review_business_ids:\n",
    "    try:\n",
    "        current_index = current_index + 1\n",
    "        if current_index in skip_indexes:\n",
    "            pass\n",
    "        else:\n",
    "            business_index = business_to_index[business_id]\n",
    "            review_business_index_col.append(business_index)\n",
    "    except KeyError:\n",
    "        print(\"Huh?\")\n",
    "    \n",
    "current_index = -1\n",
    "adjusted_review_stars = []\n",
    "for star in review_stars:\n",
    "        current_index = current_index + 1\n",
    "        if current_index in skip_indexes:\n",
    "            pass\n",
    "        else:\n",
    "            adjusted_review_stars.append(star)\n",
    "            \n",
    "print(len(adjusted_review_stars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a582bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = len(set(review_user_index_col)) + len(set(review_business_index_col))\n",
    "\n",
    "\n",
    "rating_tensor = torch.tensor(adjusted_review_stars, dtype=torch.float)\n",
    "\n",
    "edge_index = torch.tensor([review_user_index_col, review_business_index_col], dtype=torch.long)\n",
    "\n",
    "data = Data(edge_index=edge_index, y=rating_tensor, num_nodes=num_nodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95e32818",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(edge_index=[2, 47703], y=[47703], num_nodes=22403)\n",
      "Data(edge_index=[2, 3816], y=[47703], num_nodes=22403)\n",
      "Data(edge_index=[2, 9541], y=[47703], num_nodes=22403)\n",
      "Data(edge_index=[2, 954], y=[47703], num_nodes=22403)\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Split the data into training and test sets\n",
    "indices = list(range(data.edge_index.size(1)))\n",
    "\n",
    "#das hier klein, damit der Speicher nicht Ã¼berdreht wird. Aber nicht zu klein, weil sonst kommt es zu problemen!\n",
    "train_indices, test_indices = train_test_split(indices, train_size=0.1, test_size=0.2)\n",
    "train_indices, val_indices = train_test_split(train_indices, train_size=0.8, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "train_data = data.__class__()\n",
    "test_data = data.__class__()\n",
    "val_data = data.__class__()\n",
    "\n",
    "\n",
    "train_data.edge_index = data.edge_index[:, train_indices]\n",
    "train_data.y = data.y\n",
    "train_data.num_nodes = data.num_nodes\n",
    "\n",
    "test_data.edge_index = data.edge_index[:, test_indices]\n",
    "test_data.y = data.y\n",
    "test_data.num_nodes = data.num_nodes\n",
    "\n",
    "val_data.edge_index = data.edge_index[:, val_indices]\n",
    "val_data.y = data.y\n",
    "val_data.num_nodes = data.num_nodes\n",
    "\n",
    "print(data)\n",
    "print(train_data)\n",
    "print(test_data)\n",
    "print(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5047b8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\anaconda3\\Lib\\site-packages\\torch_geometric\\deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.],\n",
      "        [5.],\n",
      "        [5.],\n",
      "        ...,\n",
      "        [5.],\n",
      "        [5.],\n",
      "        [4.]])\n",
      "tensor([[4.],\n",
      "        [5.],\n",
      "        [5.],\n",
      "        ...,\n",
      "        [5.],\n",
      "        [5.],\n",
      "        [4.]])\n",
      "Epoch 1/100, Training Loss: 33.47612762451172, Validation Loss: 30.20687484741211\n",
      "tensor([[4.],\n",
      "        [5.],\n",
      "        [5.],\n",
      "        ...,\n",
      "        [5.],\n",
      "        [5.],\n",
      "        [4.]])\n",
      "tensor([[4.],\n",
      "        [5.],\n",
      "        [5.],\n",
      "        ...,\n",
      "        [5.],\n",
      "        [5.],\n",
      "        [4.]])\n",
      "Epoch 2/100, Training Loss: 30.498720169067383, Validation Loss: 27.46653175354004\n",
      "tensor([[4.],\n",
      "        [5.],\n",
      "        [5.],\n",
      "        ...,\n",
      "        [5.],\n",
      "        [5.],\n",
      "        [4.]])\n",
      "tensor([[4.],\n",
      "        [5.],\n",
      "        [5.],\n",
      "        ...,\n",
      "        [5.],\n",
      "        [5.],\n",
      "        [4.]])\n",
      "Epoch 3/100, Training Loss: 27.696353912353516, Validation Loss: 24.893754959106445\n",
      "tensor([[4.],\n",
      "        [5.],\n",
      "        [5.],\n",
      "        ...,\n",
      "        [5.],\n",
      "        [5.],\n",
      "        [4.]])\n",
      "tensor([[4.],\n",
      "        [5.],\n",
      "        [5.],\n",
      "        ...,\n",
      "        [5.],\n",
      "        [5.],\n",
      "        [4.]])\n",
      "Epoch 4/100, Training Loss: 25.068111419677734, Validation Loss: 22.4869384765625\n",
      "tensor([[4.],\n",
      "        [5.],\n",
      "        [5.],\n",
      "        ...,\n",
      "        [5.],\n",
      "        [5.],\n",
      "        [4.]])\n",
      "tensor([[4.],\n",
      "        [5.],\n",
      "        [5.],\n",
      "        ...,\n",
      "        [5.],\n",
      "        [5.],\n",
      "        [4.]])\n",
      "Epoch 5/100, Training Loss: 22.612253189086914, Validation Loss: 20.24367332458496\n",
      "tensor([[4.],\n",
      "        [5.],\n",
      "        [5.],\n",
      "        ...,\n",
      "        [5.],\n",
      "        [5.],\n",
      "        [4.]])\n",
      "tensor([[4.],\n",
      "        [5.],\n",
      "        [5.],\n",
      "        ...,\n",
      "        [5.],\n",
      "        [5.],\n",
      "        [4.]])\n",
      "Epoch 6/100, Training Loss: 20.326202392578125, Validation Loss: 18.163209915161133\n",
      "tensor([[4.],\n",
      "        [5.],\n",
      "        [5.],\n",
      "        ...,\n",
      "        [5.],\n",
      "        [5.],\n",
      "        [4.]])\n",
      "tensor([[4.],\n",
      "        [5.],\n",
      "        [5.],\n",
      "        ...,\n",
      "        [5.],\n",
      "        [5.],\n",
      "        [4.]])\n",
      "Epoch 7/100, Training Loss: 18.20904541015625, Validation Loss: 16.242679595947266\n",
      "tensor([[4.],\n",
      "        [5.],\n",
      "        [5.],\n",
      "        ...,\n",
      "        [5.],\n",
      "        [5.],\n",
      "        [4.]])\n",
      "tensor([[4.],\n",
      "        [5.],\n",
      "        [5.],\n",
      "        ...,\n",
      "        [5.],\n",
      "        [5.],\n",
      "        [4.]])\n",
      "Epoch 8/100, Training Loss: 16.257699966430664, Validation Loss: 14.480796813964844\n",
      "tensor([[4.],\n",
      "        [5.],\n",
      "        [5.],\n",
      "        ...,\n",
      "        [5.],\n",
      "        [5.],\n",
      "        [4.]])\n",
      "tensor([[4.],\n",
      "        [5.],\n",
      "        [5.],\n",
      "        ...,\n",
      "        [5.],\n",
      "        [5.],\n",
      "        [4.]])\n",
      "Epoch 9/100, Training Loss: 14.470619201660156, Validation Loss: 12.89075756072998\n",
      "tensor([[4.],\n",
      "        [5.],\n",
      "        [5.],\n",
      "        ...,\n",
      "        [5.],\n",
      "        [5.],\n",
      "        [4.]])\n",
      "tensor([[4.],\n",
      "        [5.],\n",
      "        [5.],\n",
      "        ...,\n",
      "        [5.],\n",
      "        [5.],\n",
      "        [4.]])\n",
      "Epoch 10/100, Training Loss: 12.86079216003418, Validation Loss: 11.565892219543457\n",
      "tensor([[4.],\n",
      "        [5.],\n",
      "        [5.],\n",
      "        ...,\n",
      "        [5.],\n",
      "        [5.],\n",
      "        [4.]])\n",
      "tensor([[4.],\n",
      "        [5.],\n",
      "        [5.],\n",
      "        ...,\n",
      "        [5.],\n",
      "        [5.],\n",
      "        [4.]])\n",
      "Epoch 11/100, Training Loss: 11.521679878234863, Validation Loss: 10.393808364868164\n",
      "tensor([[4.],\n",
      "        [5.],\n",
      "        [5.],\n",
      "        ...,\n",
      "        [5.],\n",
      "        [5.],\n",
      "        [4.]])\n",
      "tensor([[4.],\n",
      "        [5.],\n",
      "        [5.],\n",
      "        ...,\n",
      "        [5.],\n",
      "        [5.],\n",
      "        [4.]])\n",
      "Epoch 12/100, Training Loss: 10.340295791625977, Validation Loss: 9.321794509887695\n",
      "tensor([[4.],\n",
      "        [5.],\n",
      "        [5.],\n",
      "        ...,\n",
      "        [5.],\n",
      "        [5.],\n",
      "        [4.]])\n",
      "tensor([[4.],\n",
      "        [5.],\n",
      "        [5.],\n",
      "        ...,\n",
      "        [5.],\n",
      "        [5.],\n",
      "        [4.]])\n",
      "Epoch 13/100, Training Loss: 9.262394905090332, Validation Loss: 8.355504035949707\n",
      "tensor([[4.],\n",
      "        [5.],\n",
      "        [5.],\n",
      "        ...,\n",
      "        [5.],\n",
      "        [5.],\n",
      "        [4.]])\n",
      "tensor([[4.],\n",
      "        [5.],\n",
      "        [5.],\n",
      "        ...,\n",
      "        [5.],\n",
      "        [5.],\n",
      "        [4.]])\n",
      "Epoch 14/100, Training Loss: 8.293312072753906, Validation Loss: 7.513625621795654\n",
      "tensor([[4.],\n",
      "        [5.],\n",
      "        [5.],\n",
      "        ...,\n",
      "        [5.],\n",
      "        [5.],\n",
      "        [4.]])\n",
      "tensor([[4.],\n",
      "        [5.],\n",
      "        [5.],\n",
      "        ...,\n",
      "        [5.],\n",
      "        [5.],\n",
      "        [4.]])\n",
      "Epoch 15/100, Training Loss: 7.4513630867004395, Validation Loss: 6.761341094970703\n",
      "tensor([[4.],\n",
      "        [5.],\n",
      "        [5.],\n",
      "        ...,\n",
      "        [5.],\n",
      "        [5.],\n",
      "        [4.]])\n",
      "tensor([[4.],\n",
      "        [5.],\n",
      "        [5.],\n",
      "        ...,\n",
      "        [5.],\n",
      "        [5.],\n",
      "        [4.]])\n",
      "Epoch 16/100, Training Loss: 6.701661109924316, Validation Loss: 6.0781145095825195\n",
      "tensor([[4.],\n",
      "        [5.],\n",
      "        [5.],\n",
      "        ...,\n",
      "        [5.],\n",
      "        [5.],\n",
      "        [4.]])\n",
      "tensor([[4.],\n",
      "        [5.],\n",
      "        [5.],\n",
      "        ...,\n",
      "        [5.],\n",
      "        [5.],\n",
      "        [4.]])\n",
      "Epoch 17/100, Training Loss: 6.023312091827393, Validation Loss: 5.4614057540893555\n",
      "tensor([[4.],\n",
      "        [5.],\n",
      "        [5.],\n",
      "        ...,\n",
      "        [5.],\n",
      "        [5.],\n",
      "        [4.]])\n",
      "tensor([[4.],\n",
      "        [5.],\n",
      "        [5.],\n",
      "        ...,\n",
      "        [5.],\n",
      "        [5.],\n",
      "        [4.]])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 123\u001b[0m\n\u001b[0;32m    121\u001b[0m     out\u001b[38;5;241m=\u001b[39m model(batch\u001b[38;5;241m.\u001b[39my\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m), batch\u001b[38;5;241m.\u001b[39medge_index)\n\u001b[0;32m    122\u001b[0m     task_loss \u001b[38;5;241m=\u001b[39m criterion(out, batch\u001b[38;5;241m.\u001b[39my)\n\u001b[1;32m--> 123\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(out,batch\u001b[38;5;241m.\u001b[39my)\n\u001b[0;32m    125\u001b[0m     val_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m batch\u001b[38;5;241m.\u001b[39mnum_graphs\n\u001b[0;32m    127\u001b[0m \u001b[38;5;66;03m# Calculate average validation loss\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:536\u001b[0m, in \u001b[0;36mMSELoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 536\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mmse_loss(\u001b[38;5;28minput\u001b[39m, target, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:3295\u001b[0m, in \u001b[0;36mmse_loss\u001b[1;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3292\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m   3294\u001b[0m expanded_input, expanded_target \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbroadcast_tensors(\u001b[38;5;28minput\u001b[39m, target)\n\u001b[1;32m-> 3295\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mmse_loss(expanded_input, expanded_target, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "Step 5: Define the Graph Convolutional Network (GCN) model. Hier kÃ¶nnte man dann die zusÃ¤tzlichen Dinge einbauen\n",
    "num_features = Anzahl input layer.  hidden_channels = Anzahl \"output\" layer, d.h. anzahl Parameter, die das Model versucht\n",
    "sich herzuleiten und zu lernen. dadurch kann komplexitÃ¤t des models kontrolliert werden. wenn zB hidden_channels zu groÃ ist, \n",
    "dann passiert overfitting.\n",
    "Soweit ich es verstehe, hat dieses Neural network nur 3 Layer: self.conv1, dann RELU, dann self.conv2. \n",
    "RELU wird hier genutzt, um non-linearity einzufÃ¼hren. es ginge auch ohne. \n",
    "Actually ist der Loss ohne RELU sogar kleiner!\n",
    "\n",
    "'''\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, num_features, hidden_channels, num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        #first graph convolution\n",
    "        self.conv1 = GCNConv(num_features, hidden_channels)\n",
    "        #second graph convolution\n",
    "        self.conv2 = GCNConv(hidden_channels, num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "class LSPEGCN(nn.Module):\n",
    "    def __init__(self, num_features, hidden_channels, num_classes):\n",
    "        super(LSPEGCN, self).__init__()\n",
    "        self.gcn = GCN(num_features, hidden_channels, num_classes)\n",
    "\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "\n",
    "        #edge features: concatenate node features and edge features (TODO when adding node features)\n",
    "        x = self.gcn.conv1(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "        x = self.gcn.conv2(x, edge_index)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Step 6: Train and evaluate the GCN model\n",
    "# Set seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Set the device --> aktiviere GPU falls vorhanden\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#------------------------------------------------------\n",
    "\n",
    "#hidden channels und epochs tunen\n",
    "hidden_channels=8 #8 und 16\n",
    "lr = 0.01  #0.01 vs 0.001 \n",
    "epochs = 100  #100 vs 200\n",
    "batch_size = 64\n",
    "\n",
    " #1, 16, 32 ,64, 128, 256, 512\n",
    "\n",
    "#Early Stopping\n",
    "patience = 15  # Number of epochs to wait for improvement\n",
    "min_delta = 0.001  # Minimum improvement required to consider as improvement\n",
    "\n",
    "best_val_loss = np.inf\n",
    "best_epoch = 0\n",
    "early_stop_counter = 0\n",
    "\n",
    "# Define the GCNModel\n",
    "#model = GATModel(num_features = 1, hidden_channels = hidden_channels, num_classes = 1, heads = 1).to(device)\n",
    "#model = GINModel(num_features = 1, num_classes = 1).to(device)\n",
    "\n",
    "\n",
    "model = GCN(num_features=1, hidden_channels=hidden_channels, num_classes=1)\n",
    "\n",
    "#------------------------------------------------------\n",
    "#loss function, and optimizer, MSE = Metrik fÃ¼r Loss \n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Create data loaders for training and test sets\n",
    "train_loader = DataLoader([train_data], batch_size=batch_size)\n",
    "test_loader = DataLoader([test_data], batch_size=batch_size)\n",
    "val_loader = DataLoader([val_data], batch_size=batch_size)\n",
    "\n",
    "# Model training\n",
    "model.train()\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "predictions =[]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Training\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)\n",
    "        \n",
    "        out = model(batch.y.unsqueeze(1), batch.edge_index)\n",
    "        task_loss = criterion(out, batch.y)\n",
    "        loss = criterion(out, batch.y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * batch.num_graphs\n",
    "        predictions = out.detach().cpu().numpy()\n",
    "        #print(predictions)\n",
    "\n",
    "    # Calculate average training loss\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    for batch in val_loader:\n",
    "        batch = batch.to(device)\n",
    "        \n",
    "        out= model(batch.y.unsqueeze(1), batch.edge_index)\n",
    "        task_loss = criterion(out, batch.y)\n",
    "        loss = criterion(out,batch.y)\n",
    "        \n",
    "        val_loss += loss.item() * batch.num_graphs\n",
    "\n",
    "    # Calculate average validation loss\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    # Print training and validation loss for monitoring\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {train_loss}, Validation Loss: {val_loss}\")\n",
    "\n",
    "    # Check for early stopping\n",
    "    if val_loss < best_val_loss - min_delta:\n",
    "        best_val_loss = val_loss\n",
    "        best_epoch = epoch\n",
    "        early_stop_counter = 0\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "        if early_stop_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    # Set the model back to training mode\n",
    "    model.train()\n",
    "'''\n",
    "# Plotting training and validation curves\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "#plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Save the plot as an image file\n",
    "plt.savefig('loss_plot.png')\n",
    "'''\n",
    "\n",
    "# Show the plot\n",
    "\n",
    "# Evaluation on test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = []\n",
    "    targets = []\n",
    "\n",
    "    for batch in test_loader:\n",
    "        batch = batch.to(device)\n",
    "        out = model(batch.y.unsqueeze(1), batch.edge_index)\n",
    "        task_loss = criterion(out, batch.y)\n",
    "#        test_loss = calculateLoss(task_loss, batch, num_nodes)\n",
    "        \n",
    "        print(f'Task Loss: {task_loss.item()}')\n",
    "        predictions.extend(out.cpu().numpy().flatten())\n",
    "        targets.extend(batch.y.cpu().numpy().flatten())\n",
    "\n",
    "    predictions = np.array(predictions)\n",
    "    targets = np.array(targets)\n",
    "\n",
    "    rounded_predictions = np.round(predictions, decimals=0)  # Round the predicted ratings\n",
    "\n",
    "    # Plotting the distribution\n",
    "    plt.hist(rounded_predictions, bins=15, edgecolor='black')\n",
    "    plt.xlabel('Predicted Rating')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Distribution of Predicted Ratings')\n",
    "    plt.xticks(range(1, 16))\n",
    "    #plt.show()\n",
    "    plt.savefig('predicted_rankings.png')\n",
    "\n",
    "\n",
    "    mse = np.mean(np.abs(predictions - targets) ** 2)\n",
    "    rmse = np.sqrt(mse)\n",
    "    k = 5  # Define the value of k\n",
    "\n",
    "    print(f\"Batch Size: {batch_size}\")\n",
    "    print(f\"Epochs: {epochs}\")\n",
    "    print   (f\"MSE: {mse}\")\n",
    "    print(f\"RMSE: {rmse}\")\n",
    "    \n",
    "    threshold = 3.5 # Define the threshold to convert predictions into binary values\n",
    "    np.set_printoptions(threshold=sys.maxsize)  # Set the threshold to print the whole array\n",
    "\n",
    "    #print(rounded_predictions)\n",
    "    #print(predictions)\n",
    "\n",
    "    GAT_results = open(\"predictions_GAT.txt\", \"a\")\n",
    "\n",
    "    GAT_results.write(str(predictions))\n",
    "    \n",
    "    GAT_results.close()\n",
    "\n",
    "    precision_value = precision(predictions, targets, threshold)\n",
    "    recall_value = recall(predictions, targets, threshold)\n",
    "\n",
    "    print(f\"Precision: {precision_value}\")\n",
    "    print(f\"Recall: {recall_value}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
